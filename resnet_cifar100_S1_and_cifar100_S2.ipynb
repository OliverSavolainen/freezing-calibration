{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b09f8b-c683-4b1e-be79-3b82149ddb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import Input, Model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import (add, Conv2D, GlobalAveragePooling2D)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.datasets import cifar100\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ee168-6429-44d6-9edd-e37b74dcefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training procedure for CIFAR-100 using ResNet 110.\n",
    "# ResNet model from https://github.com/BIGBALLON/cifar-10-cnn/blob/master/4_Residual_Network/ResNet_keras.py\n",
    "stack_n = 18\n",
    "num_classes = 100\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "iterations = 45000 // batch_size\n",
    "weight_decay = 0.0001\n",
    "seed = 333\n",
    "\n",
    "\n",
    "def residual_network(img_input, classes_num=10, stack_n=5):\n",
    "    def residual_block(intput, out_channel, increase=False):\n",
    "        if increase:\n",
    "            stride = (2, 2)\n",
    "        else:\n",
    "            stride = (1, 1)\n",
    "\n",
    "        pre_bn = BatchNormalization()(intput)\n",
    "        pre_relu = Activation('relu')(pre_bn)\n",
    "\n",
    "        conv_1 = Conv2D(out_channel, kernel_size=(3, 3), strides=stride, padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(pre_relu)\n",
    "        bn_1 = BatchNormalization()(conv_1)\n",
    "        relu1 = Activation('relu')(bn_1)\n",
    "        conv_2 = Conv2D(out_channel, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(relu1)\n",
    "        if increase:\n",
    "            projection = Conv2D(out_channel,\n",
    "                                kernel_size=(1, 1),\n",
    "                                strides=(2, 2),\n",
    "                                padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(weight_decay))(intput)\n",
    "            block = add([conv_2, projection])\n",
    "        else:\n",
    "            block = add([intput, conv_2])\n",
    "        return block\n",
    "\n",
    "    # build model\n",
    "    # total layers = stack_n * 3 * 2 + 2\n",
    "    # stack_n = 5 by default, total layers = 32\n",
    "    # input: 32x32x3 output: 32x32x16\n",
    "    x = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "\n",
    "    # input: 32x32x16 output: 32x32x16\n",
    "    for _ in range(stack_n):\n",
    "        x = residual_block(x, 16, False)\n",
    "\n",
    "    # input: 32x32x16 output: 16x16x32\n",
    "    x = residual_block(x, 32, True)\n",
    "    for _ in range(1, stack_n):\n",
    "        x = residual_block(x, 32, False)\n",
    "\n",
    "    # input: 16x16x32 output: 8x8x64\n",
    "    x = residual_block(x, 64, True)\n",
    "    for _ in range(1, stack_n):\n",
    "        x = residual_block(x, 64, False)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # input: 64 output: 10\n",
    "    x = Dense(classes_num, activation='softmax',\n",
    "              kernel_initializer=\"he_normal\",\n",
    "              kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1,\n",
    "                                                      random_state=seed)  # random_state = seed\n",
    "\n",
    "img_mean = x_train45.mean(axis=0)  # per-pixel mean, what about std?\n",
    "img_std = x_train45.std(axis=0)\n",
    "x_train45 = (x_train45 - img_mean) / img_std\n",
    "x_val = (x_val - img_mean) / img_std\n",
    "x_test = (x_test - img_mean) / img_std\n",
    "\n",
    "# build network\n",
    "img_input = Input(shape=(img_rows, img_cols, img_channels))\n",
    "output = residual_network(img_input, num_classes, stack_n)\n",
    "model = Model(img_input, output)\n",
    "print(model.summary())\n",
    "\n",
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant', cval=0.)\n",
    "\n",
    "datagen.fit(x_train45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e63caf-1499-43a5-900e-5c0b3403a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.8:\n",
    "    freezing_list.append(int(epochs*0.6))\n",
    "freezing_list.append(epochs)\n",
    "checkpointer = ModelCheckpoint('model_resnet_c100_best.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b037ac09-8587-47b6-a127-af3bf6837085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_100\n",
      ".........vars\n",
      "......activation_101\n",
      ".........vars\n",
      "......activation_102\n",
      ".........vars\n",
      "......activation_103\n",
      ".........vars\n",
      "......activation_104\n",
      ".........vars\n",
      "......activation_105\n",
      ".........vars\n",
      "......activation_106\n",
      ".........vars\n",
      "......activation_107\n",
      ".........vars\n",
      "......activation_108\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_55\n",
      ".........vars\n",
      "......activation_56\n",
      ".........vars\n",
      "......activation_57\n",
      ".........vars\n",
      "......activation_58\n",
      ".........vars\n",
      "......activation_59\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_60\n",
      ".........vars\n",
      "......activation_61\n",
      ".........vars\n",
      "......activation_62\n",
      ".........vars\n",
      "......activation_63\n",
      ".........vars\n",
      "......activation_64\n",
      ".........vars\n",
      "......activation_65\n",
      ".........vars\n",
      "......activation_66\n",
      ".........vars\n",
      "......activation_67\n",
      ".........vars\n",
      "......activation_68\n",
      ".........vars\n",
      "......activation_69\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_70\n",
      ".........vars\n",
      "......activation_71\n",
      ".........vars\n",
      "......activation_72\n",
      ".........vars\n",
      "......activation_73\n",
      ".........vars\n",
      "......activation_74\n",
      ".........vars\n",
      "......activation_75\n",
      ".........vars\n",
      "......activation_76\n",
      ".........vars\n",
      "......activation_77\n",
      ".........vars\n",
      "......activation_78\n",
      ".........vars\n",
      "......activation_79\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_80\n",
      ".........vars\n",
      "......activation_81\n",
      ".........vars\n",
      "......activation_82\n",
      ".........vars\n",
      "......activation_83\n",
      ".........vars\n",
      "......activation_84\n",
      ".........vars\n",
      "......activation_85\n",
      ".........vars\n",
      "......activation_86\n",
      ".........vars\n",
      "......activation_87\n",
      ".........vars\n",
      "......activation_88\n",
      ".........vars\n",
      "......activation_89\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......activation_90\n",
      ".........vars\n",
      "......activation_91\n",
      ".........vars\n",
      "......activation_92\n",
      ".........vars\n",
      "......activation_93\n",
      ".........vars\n",
      "......activation_94\n",
      ".........vars\n",
      "......activation_95\n",
      ".........vars\n",
      "......activation_96\n",
      ".........vars\n",
      "......activation_97\n",
      ".........vars\n",
      "......activation_98\n",
      ".........vars\n",
      "......activation_99\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_109\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_110\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-20 18:15:13           64\n",
      "config.json                                    2023-04-20 18:15:13       179539\n",
      "variables.h5                                   2023-04-20 18:15:14      7961936\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-20 18:15:12           64\n",
      "config.json                                    2023-04-20 18:15:12       179539\n",
      "variables.h5                                   2023-04-20 18:15:14      7961936\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_100\n",
      ".........vars\n",
      "......activation_101\n",
      ".........vars\n",
      "......activation_102\n",
      ".........vars\n",
      "......activation_103\n",
      ".........vars\n",
      "......activation_104\n",
      ".........vars\n",
      "......activation_105\n",
      ".........vars\n",
      "......activation_106\n",
      ".........vars\n",
      "......activation_107\n",
      ".........vars\n",
      "......activation_108\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_55\n",
      ".........vars\n",
      "......activation_56\n",
      ".........vars\n",
      "......activation_57\n",
      ".........vars\n",
      "......activation_58\n",
      ".........vars\n",
      "......activation_59\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_60\n",
      ".........vars\n",
      "......activation_61\n",
      ".........vars\n",
      "......activation_62\n",
      ".........vars\n",
      "......activation_63\n",
      ".........vars\n",
      "......activation_64\n",
      ".........vars\n",
      "......activation_65\n",
      ".........vars\n",
      "......activation_66\n",
      ".........vars\n",
      "......activation_67\n",
      ".........vars\n",
      "......activation_68\n",
      ".........vars\n",
      "......activation_69\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_70\n",
      ".........vars\n",
      "......activation_71\n",
      ".........vars\n",
      "......activation_72\n",
      ".........vars\n",
      "......activation_73\n",
      ".........vars\n",
      "......activation_74\n",
      ".........vars\n",
      "......activation_75\n",
      ".........vars\n",
      "......activation_76\n",
      ".........vars\n",
      "......activation_77\n",
      ".........vars\n",
      "......activation_78\n",
      ".........vars\n",
      "......activation_79\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_80\n",
      ".........vars\n",
      "......activation_81\n",
      ".........vars\n",
      "......activation_82\n",
      ".........vars\n",
      "......activation_83\n",
      ".........vars\n",
      "......activation_84\n",
      ".........vars\n",
      "......activation_85\n",
      ".........vars\n",
      "......activation_86\n",
      ".........vars\n",
      "......activation_87\n",
      ".........vars\n",
      "......activation_88\n",
      ".........vars\n",
      "......activation_89\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......activation_90\n",
      ".........vars\n",
      "......activation_91\n",
      ".........vars\n",
      "......activation_92\n",
      ".........vars\n",
      "......activation_93\n",
      ".........vars\n",
      "......activation_94\n",
      ".........vars\n",
      "......activation_95\n",
      ".........vars\n",
      "......activation_96\n",
      ".........vars\n",
      "......activation_97\n",
      ".........vars\n",
      "......activation_98\n",
      ".........vars\n",
      "......activation_99\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_109\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_110\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 18:15:38.216983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - ETA: 0s - loss: 4.8297 - accuracy: 0.0789\n",
      "Epoch 1: val_loss improved from inf to 5.08418, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 75s 147ms/step - loss: 4.8297 - accuracy: 0.0789 - val_loss: 5.0842 - val_accuracy: 0.0684 - lr: 0.1000\n",
      "Epoch 2/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1377 - accuracy: 0.1694\n",
      "Epoch 2: val_loss did not improve from 5.08418\n",
      "351/351 [==============================] - 52s 148ms/step - loss: 4.1377 - accuracy: 0.1694 - val_loss: 5.1143 - val_accuracy: 0.1062 - lr: 0.1000\n",
      "Epoch 3/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6062 - accuracy: 0.2510\n",
      "Epoch 3: val_loss improved from 5.08418 to 3.90685, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 52s 148ms/step - loss: 3.6062 - accuracy: 0.2510 - val_loss: 3.9069 - val_accuracy: 0.2190 - lr: 0.1000\n",
      "Epoch 4/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2392 - accuracy: 0.3156\n",
      "Epoch 4: val_loss improved from 3.90685 to 3.72604, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 3.2392 - accuracy: 0.3156 - val_loss: 3.7260 - val_accuracy: 0.2526 - lr: 0.1000\n",
      "Epoch 5/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9623 - accuracy: 0.3686\n",
      "Epoch 5: val_loss did not improve from 3.72604\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 2.9623 - accuracy: 0.3686 - val_loss: 5.2733 - val_accuracy: 0.1888 - lr: 0.1000\n",
      "Epoch 6/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.7441 - accuracy: 0.4098\n",
      "Epoch 6: val_loss improved from 3.72604 to 3.05118, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 2.7441 - accuracy: 0.4098 - val_loss: 3.0512 - val_accuracy: 0.3532 - lr: 0.1000\n",
      "Epoch 7/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.5801 - accuracy: 0.4428\n",
      "Epoch 7: val_loss did not improve from 3.05118\n",
      "351/351 [==============================] - 59s 167ms/step - loss: 2.5801 - accuracy: 0.4428 - val_loss: 3.8742 - val_accuracy: 0.2922 - lr: 0.1000\n",
      "Epoch 8/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4454 - accuracy: 0.4707\n",
      "Epoch 8: val_loss improved from 3.05118 to 2.85273, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 64s 181ms/step - loss: 2.4454 - accuracy: 0.4707 - val_loss: 2.8527 - val_accuracy: 0.4078 - lr: 0.1000\n",
      "Epoch 9/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3390 - accuracy: 0.4962\n",
      "Epoch 9: val_loss improved from 2.85273 to 2.73677, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 63s 179ms/step - loss: 2.3390 - accuracy: 0.4962 - val_loss: 2.7368 - val_accuracy: 0.4328 - lr: 0.1000\n",
      "Epoch 10/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.2513 - accuracy: 0.5161\n",
      "Epoch 10: val_loss improved from 2.73677 to 2.68406, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 57s 162ms/step - loss: 2.2513 - accuracy: 0.5161 - val_loss: 2.6841 - val_accuracy: 0.4440 - lr: 0.1000\n",
      "Epoch 11/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.1758 - accuracy: 0.5288\n",
      "Epoch 11: val_loss did not improve from 2.68406\n",
      "351/351 [==============================] - 58s 165ms/step - loss: 2.1758 - accuracy: 0.5288 - val_loss: 2.8171 - val_accuracy: 0.4256 - lr: 0.1000\n",
      "Epoch 12/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.1098 - accuracy: 0.5475\n",
      "Epoch 12: val_loss improved from 2.68406 to 2.55736, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 2.1098 - accuracy: 0.5475 - val_loss: 2.5574 - val_accuracy: 0.4590 - lr: 0.1000\n",
      "Epoch 13/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0501 - accuracy: 0.5602\n",
      "Epoch 13: val_loss improved from 2.55736 to 2.31205, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 57s 163ms/step - loss: 2.0501 - accuracy: 0.5602 - val_loss: 2.3121 - val_accuracy: 0.5126 - lr: 0.1000\n",
      "Epoch 14/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9960 - accuracy: 0.5731\n",
      "Epoch 14: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 1.9960 - accuracy: 0.5731 - val_loss: 2.9436 - val_accuracy: 0.4104 - lr: 0.1000\n",
      "Epoch 15/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9594 - accuracy: 0.5828\n",
      "Epoch 15: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 53s 150ms/step - loss: 1.9594 - accuracy: 0.5828 - val_loss: 3.0703 - val_accuracy: 0.4192 - lr: 0.1000\n",
      "Epoch 16/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9185 - accuracy: 0.5935\n",
      "Epoch 16: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 53s 150ms/step - loss: 1.9185 - accuracy: 0.5935 - val_loss: 2.9059 - val_accuracy: 0.4200 - lr: 0.1000\n",
      "Epoch 17/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8840 - accuracy: 0.6037\n",
      "Epoch 17: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 1.8840 - accuracy: 0.6037 - val_loss: 2.4986 - val_accuracy: 0.4924 - lr: 0.1000\n",
      "Epoch 18/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8541 - accuracy: 0.6112\n",
      "Epoch 18: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 61s 175ms/step - loss: 1.8541 - accuracy: 0.6112 - val_loss: 2.6855 - val_accuracy: 0.4900 - lr: 0.1000\n",
      "Epoch 19/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8278 - accuracy: 0.6193\n",
      "Epoch 19: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 1.8278 - accuracy: 0.6193 - val_loss: 2.5763 - val_accuracy: 0.4892 - lr: 0.1000\n",
      "Epoch 20/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8045 - accuracy: 0.6269\n",
      "Epoch 20: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 1.8045 - accuracy: 0.6269 - val_loss: 2.6657 - val_accuracy: 0.4540 - lr: 0.1000\n",
      "Epoch 21/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7745 - accuracy: 0.6337\n",
      "Epoch 21: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 1.7745 - accuracy: 0.6337 - val_loss: 2.6238 - val_accuracy: 0.4722 - lr: 0.1000\n",
      "Epoch 22/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7630 - accuracy: 0.6383\n",
      "Epoch 22: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 1.7630 - accuracy: 0.6383 - val_loss: 2.4167 - val_accuracy: 0.5184 - lr: 0.1000\n",
      "Epoch 23/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7363 - accuracy: 0.6495\n",
      "Epoch 23: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 60s 169ms/step - loss: 1.7363 - accuracy: 0.6495 - val_loss: 2.4334 - val_accuracy: 0.5196 - lr: 0.1000\n",
      "Epoch 24/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7320 - accuracy: 0.6518\n",
      "Epoch 24: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 1.7320 - accuracy: 0.6518 - val_loss: 2.5666 - val_accuracy: 0.5066 - lr: 0.1000\n",
      "Epoch 25/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7131 - accuracy: 0.6566\n",
      "Epoch 25: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 1.7131 - accuracy: 0.6566 - val_loss: 2.3715 - val_accuracy: 0.5258 - lr: 0.1000\n",
      "Epoch 26/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6918 - accuracy: 0.6647\n",
      "Epoch 26: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 1.6918 - accuracy: 0.6647 - val_loss: 2.6583 - val_accuracy: 0.5026 - lr: 0.1000\n",
      "Epoch 27/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6807 - accuracy: 0.6680\n",
      "Epoch 27: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 59s 167ms/step - loss: 1.6807 - accuracy: 0.6680 - val_loss: 2.4409 - val_accuracy: 0.5310 - lr: 0.1000\n",
      "Epoch 28/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6713 - accuracy: 0.6744\n",
      "Epoch 28: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 63s 180ms/step - loss: 1.6713 - accuracy: 0.6744 - val_loss: 2.4471 - val_accuracy: 0.5114 - lr: 0.1000\n",
      "Epoch 29/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6608 - accuracy: 0.6752\n",
      "Epoch 29: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 1.6608 - accuracy: 0.6752 - val_loss: 2.9844 - val_accuracy: 0.4730 - lr: 0.1000\n",
      "Epoch 30/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6582 - accuracy: 0.6797\n",
      "Epoch 30: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 1.6582 - accuracy: 0.6797 - val_loss: 2.7278 - val_accuracy: 0.4998 - lr: 0.1000\n",
      "Epoch 31/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6352 - accuracy: 0.6861\n",
      "Epoch 31: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 57s 162ms/step - loss: 1.6352 - accuracy: 0.6861 - val_loss: 2.6954 - val_accuracy: 0.5120 - lr: 0.1000\n",
      "Epoch 32/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6400 - accuracy: 0.6860\n",
      "Epoch 32: val_loss did not improve from 2.31205\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 1.6400 - accuracy: 0.6860 - val_loss: 2.9133 - val_accuracy: 0.4830 - lr: 0.1000\n",
      "Epoch 33/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6280 - accuracy: 0.6897\n",
      "Epoch 33: val_loss improved from 2.31205 to 2.21608, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 57s 162ms/step - loss: 1.6280 - accuracy: 0.6897 - val_loss: 2.2161 - val_accuracy: 0.5712 - lr: 0.1000\n",
      "Epoch 34/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6156 - accuracy: 0.6973\n",
      "Epoch 34: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 63s 179ms/step - loss: 1.6156 - accuracy: 0.6973 - val_loss: 2.6456 - val_accuracy: 0.5124 - lr: 0.1000\n",
      "Epoch 35/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6181 - accuracy: 0.6977\n",
      "Epoch 35: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 64s 182ms/step - loss: 1.6181 - accuracy: 0.6977 - val_loss: 2.6391 - val_accuracy: 0.5300 - lr: 0.1000\n",
      "Epoch 36/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6027 - accuracy: 0.7038\n",
      "Epoch 36: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 65s 185ms/step - loss: 1.6027 - accuracy: 0.7038 - val_loss: 2.7351 - val_accuracy: 0.5142 - lr: 0.1000\n",
      "Epoch 37/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5969 - accuracy: 0.7074\n",
      "Epoch 37: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 62s 176ms/step - loss: 1.5969 - accuracy: 0.7074 - val_loss: 2.6126 - val_accuracy: 0.5268 - lr: 0.1000\n",
      "Epoch 38/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5940 - accuracy: 0.7084\n",
      "Epoch 38: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 64s 183ms/step - loss: 1.5940 - accuracy: 0.7084 - val_loss: 2.8093 - val_accuracy: 0.5086 - lr: 0.1000\n",
      "Epoch 39/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5941 - accuracy: 0.7102\n",
      "Epoch 39: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 59s 167ms/step - loss: 1.5941 - accuracy: 0.7102 - val_loss: 2.7162 - val_accuracy: 0.5236 - lr: 0.1000\n",
      "Epoch 40/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5885 - accuracy: 0.7120\n",
      "Epoch 40: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 63s 179ms/step - loss: 1.5885 - accuracy: 0.7120 - val_loss: 2.5697 - val_accuracy: 0.5296 - lr: 0.1000\n",
      "Epoch 41/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5830 - accuracy: 0.7142\n",
      "Epoch 41: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 1.5830 - accuracy: 0.7142 - val_loss: 2.5547 - val_accuracy: 0.5492 - lr: 0.1000\n",
      "Epoch 42/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5779 - accuracy: 0.7196\n",
      "Epoch 42: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 62s 175ms/step - loss: 1.5779 - accuracy: 0.7196 - val_loss: 2.6494 - val_accuracy: 0.5512 - lr: 0.1000\n",
      "Epoch 43/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5735 - accuracy: 0.7218\n",
      "Epoch 43: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 58s 166ms/step - loss: 1.5735 - accuracy: 0.7218 - val_loss: 2.5029 - val_accuracy: 0.5384 - lr: 0.1000\n",
      "Epoch 44/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5676 - accuracy: 0.7248\n",
      "Epoch 44: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 62s 176ms/step - loss: 1.5676 - accuracy: 0.7248 - val_loss: 2.4123 - val_accuracy: 0.5582 - lr: 0.1000\n",
      "Epoch 45/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5692 - accuracy: 0.7242\n",
      "Epoch 45: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 175ms/step - loss: 1.5692 - accuracy: 0.7242 - val_loss: 2.3708 - val_accuracy: 0.5740 - lr: 0.1000\n",
      "Epoch 46/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5586 - accuracy: 0.7312\n",
      "Epoch 46: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 57s 161ms/step - loss: 1.5586 - accuracy: 0.7312 - val_loss: 2.7562 - val_accuracy: 0.5272 - lr: 0.1000\n",
      "Epoch 47/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5558 - accuracy: 0.7299\n",
      "Epoch 47: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 175ms/step - loss: 1.5558 - accuracy: 0.7299 - val_loss: 2.6744 - val_accuracy: 0.5388 - lr: 0.1000\n",
      "Epoch 48/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5588 - accuracy: 0.7292\n",
      "Epoch 48: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 63s 178ms/step - loss: 1.5588 - accuracy: 0.7292 - val_loss: 2.5783 - val_accuracy: 0.5424 - lr: 0.1000\n",
      "Epoch 49/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5570 - accuracy: 0.7331\n",
      "Epoch 49: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 56s 160ms/step - loss: 1.5570 - accuracy: 0.7331 - val_loss: 2.8183 - val_accuracy: 0.5328 - lr: 0.1000\n",
      "Epoch 50/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5473 - accuracy: 0.7353\n",
      "Epoch 50: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 1.5473 - accuracy: 0.7353 - val_loss: 2.3792 - val_accuracy: 0.5750 - lr: 0.1000\n",
      "Epoch 51/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5569 - accuracy: 0.7326\n",
      "Epoch 51: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 63s 179ms/step - loss: 1.5569 - accuracy: 0.7326 - val_loss: 2.8296 - val_accuracy: 0.5038 - lr: 0.1000\n",
      "Epoch 52/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5447 - accuracy: 0.7420\n",
      "Epoch 52: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 1.5447 - accuracy: 0.7420 - val_loss: 2.7215 - val_accuracy: 0.5214 - lr: 0.1000\n",
      "Epoch 53/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5453 - accuracy: 0.7383\n",
      "Epoch 53: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 1.5453 - accuracy: 0.7383 - val_loss: 2.5483 - val_accuracy: 0.5536 - lr: 0.1000\n",
      "Epoch 54/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5471 - accuracy: 0.7410\n",
      "Epoch 54: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 53s 151ms/step - loss: 1.5471 - accuracy: 0.7410 - val_loss: 2.4729 - val_accuracy: 0.5620 - lr: 0.1000\n",
      "Epoch 55/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.7470\n",
      "Epoch 55: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 1.5291 - accuracy: 0.7470 - val_loss: 2.3886 - val_accuracy: 0.5778 - lr: 0.1000\n",
      "Epoch 56/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5352 - accuracy: 0.7466\n",
      "Epoch 56: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 57s 162ms/step - loss: 1.5352 - accuracy: 0.7466 - val_loss: 2.7145 - val_accuracy: 0.5416 - lr: 0.1000\n",
      "Epoch 57/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5227 - accuracy: 0.7510\n",
      "Epoch 57: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 1.5227 - accuracy: 0.7510 - val_loss: 2.6880 - val_accuracy: 0.5542 - lr: 0.1000\n",
      "Epoch 58/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5327 - accuracy: 0.7476\n",
      "Epoch 58: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 175ms/step - loss: 1.5327 - accuracy: 0.7476 - val_loss: 2.6042 - val_accuracy: 0.5696 - lr: 0.1000\n",
      "Epoch 59/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5272 - accuracy: 0.7537\n",
      "Epoch 59: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 59s 169ms/step - loss: 1.5272 - accuracy: 0.7537 - val_loss: 3.0043 - val_accuracy: 0.5130 - lr: 0.1000\n",
      "Epoch 60/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5249 - accuracy: 0.7518\n",
      "Epoch 60: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 64s 183ms/step - loss: 1.5249 - accuracy: 0.7518 - val_loss: 2.8112 - val_accuracy: 0.5282 - lr: 0.1000\n",
      "Epoch 61/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5264 - accuracy: 0.7510\n",
      "Epoch 61: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 1.5264 - accuracy: 0.7510 - val_loss: 2.6951 - val_accuracy: 0.5508 - lr: 0.1000\n",
      "Epoch 62/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5262 - accuracy: 0.7523\n",
      "Epoch 62: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 62s 177ms/step - loss: 1.5262 - accuracy: 0.7523 - val_loss: 2.7864 - val_accuracy: 0.5492 - lr: 0.1000\n",
      "Epoch 63/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5255 - accuracy: 0.7516\n",
      "Epoch 63: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 56s 161ms/step - loss: 1.5255 - accuracy: 0.7516 - val_loss: 2.6281 - val_accuracy: 0.5526 - lr: 0.1000\n",
      "Epoch 64/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5240 - accuracy: 0.7556\n",
      "Epoch 64: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 1.5240 - accuracy: 0.7556 - val_loss: 2.4848 - val_accuracy: 0.5830 - lr: 0.1000\n",
      "Epoch 65/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5197 - accuracy: 0.7573\n",
      "Epoch 65: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 1.5197 - accuracy: 0.7573 - val_loss: 2.5102 - val_accuracy: 0.5662 - lr: 0.1000\n",
      "Epoch 66/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5156 - accuracy: 0.7591\n",
      "Epoch 66: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 1.5156 - accuracy: 0.7591 - val_loss: 2.6641 - val_accuracy: 0.5568 - lr: 0.1000\n",
      "Epoch 67/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5179 - accuracy: 0.7580\n",
      "Epoch 67: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 1.5179 - accuracy: 0.7580 - val_loss: 2.5805 - val_accuracy: 0.5654 - lr: 0.1000\n",
      "Epoch 68/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5156 - accuracy: 0.7604\n",
      "Epoch 68: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 172ms/step - loss: 1.5156 - accuracy: 0.7604 - val_loss: 3.0989 - val_accuracy: 0.4800 - lr: 0.1000\n",
      "Epoch 69/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5122 - accuracy: 0.7603\n",
      "Epoch 69: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 58s 164ms/step - loss: 1.5122 - accuracy: 0.7603 - val_loss: 2.7964 - val_accuracy: 0.5356 - lr: 0.1000\n",
      "Epoch 70/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5097 - accuracy: 0.7636\n",
      "Epoch 70: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 59s 168ms/step - loss: 1.5097 - accuracy: 0.7636 - val_loss: 2.5389 - val_accuracy: 0.5690 - lr: 0.1000\n",
      "Epoch 71/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5105 - accuracy: 0.7643\n",
      "Epoch 71: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 58s 164ms/step - loss: 1.5105 - accuracy: 0.7643 - val_loss: 2.7474 - val_accuracy: 0.5370 - lr: 0.1000\n",
      "Epoch 72/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5039 - accuracy: 0.7646\n",
      "Epoch 72: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 60s 172ms/step - loss: 1.5039 - accuracy: 0.7646 - val_loss: 2.6752 - val_accuracy: 0.5390 - lr: 0.1000\n",
      "Epoch 73/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5026 - accuracy: 0.7676\n",
      "Epoch 73: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 60s 172ms/step - loss: 1.5026 - accuracy: 0.7676 - val_loss: 2.6287 - val_accuracy: 0.5704 - lr: 0.1000\n",
      "Epoch 74/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5076 - accuracy: 0.7673\n",
      "Epoch 74: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 56s 160ms/step - loss: 1.5076 - accuracy: 0.7673 - val_loss: 2.6579 - val_accuracy: 0.5540 - lr: 0.1000\n",
      "Epoch 75/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4936 - accuracy: 0.7695\n",
      "Epoch 75: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 1.4936 - accuracy: 0.7695 - val_loss: 2.5422 - val_accuracy: 0.5744 - lr: 0.1000\n",
      "Epoch 76/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5114 - accuracy: 0.7660\n",
      "Epoch 76: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 1.5114 - accuracy: 0.7660 - val_loss: 2.5258 - val_accuracy: 0.5940 - lr: 0.1000\n",
      "Epoch 77/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5007 - accuracy: 0.7719\n",
      "Epoch 77: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 1.5007 - accuracy: 0.7719 - val_loss: 2.5836 - val_accuracy: 0.5732 - lr: 0.1000\n",
      "Epoch 78/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4979 - accuracy: 0.7707\n",
      "Epoch 78: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 1.4979 - accuracy: 0.7707 - val_loss: 3.0488 - val_accuracy: 0.5256 - lr: 0.1000\n",
      "Epoch 79/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4915 - accuracy: 0.7722\n",
      "Epoch 79: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 62s 176ms/step - loss: 1.4915 - accuracy: 0.7722 - val_loss: 2.6536 - val_accuracy: 0.5676 - lr: 0.1000\n",
      "Epoch 80/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5059 - accuracy: 0.7702\n",
      "Epoch 80: val_loss did not improve from 2.21608\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 1.5059 - accuracy: 0.7702 - val_loss: 2.4853 - val_accuracy: 0.5880 - lr: 0.1000\n",
      "Epoch 81/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2146 - accuracy: 0.8615\n",
      "Epoch 81: val_loss improved from 2.21608 to 1.91008, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 64s 182ms/step - loss: 1.2146 - accuracy: 0.8615 - val_loss: 1.9101 - val_accuracy: 0.7038 - lr: 0.0100\n",
      "Epoch 82/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0505 - accuracy: 0.9105\n",
      "Epoch 82: val_loss improved from 1.91008 to 1.89821, saving model to model_resnet_c100_best.hdf5\n",
      "351/351 [==============================] - 59s 168ms/step - loss: 1.0505 - accuracy: 0.9105 - val_loss: 1.8982 - val_accuracy: 0.7178 - lr: 0.0100\n",
      "Epoch 83/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9951 - accuracy: 0.9241\n",
      "Epoch 83: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 57s 162ms/step - loss: 0.9951 - accuracy: 0.9241 - val_loss: 1.9111 - val_accuracy: 0.7158 - lr: 0.0100\n",
      "Epoch 84/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9553 - accuracy: 0.9331\n",
      "Epoch 84: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 0.9553 - accuracy: 0.9331 - val_loss: 1.9348 - val_accuracy: 0.7128 - lr: 0.0100\n",
      "Epoch 85/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9246 - accuracy: 0.9405\n",
      "Epoch 85: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 0.9246 - accuracy: 0.9405 - val_loss: 1.9119 - val_accuracy: 0.7154 - lr: 0.0100\n",
      "Epoch 86/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.9448\n",
      "Epoch 86: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 0.9004 - accuracy: 0.9448 - val_loss: 1.9659 - val_accuracy: 0.7150 - lr: 0.0100\n",
      "Epoch 87/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8728 - accuracy: 0.9512\n",
      "Epoch 87: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 59s 168ms/step - loss: 0.8728 - accuracy: 0.9512 - val_loss: 1.9424 - val_accuracy: 0.7186 - lr: 0.0100\n",
      "Epoch 88/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8523 - accuracy: 0.9557\n",
      "Epoch 88: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 62s 176ms/step - loss: 0.8523 - accuracy: 0.9557 - val_loss: 1.9660 - val_accuracy: 0.7170 - lr: 0.0100\n",
      "Epoch 89/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8310 - accuracy: 0.9594\n",
      "Epoch 89: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 64s 183ms/step - loss: 0.8310 - accuracy: 0.9594 - val_loss: 1.9744 - val_accuracy: 0.7114 - lr: 0.0100\n",
      "Epoch 90/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8151 - accuracy: 0.9622\n",
      "Epoch 90: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 58s 165ms/step - loss: 0.8151 - accuracy: 0.9622 - val_loss: 1.9930 - val_accuracy: 0.7108 - lr: 0.0100\n",
      "Epoch 91/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7969 - accuracy: 0.9653\n",
      "Epoch 91: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 0.7969 - accuracy: 0.9653 - val_loss: 1.9958 - val_accuracy: 0.7142 - lr: 0.0100\n",
      "Epoch 92/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7801 - accuracy: 0.9674\n",
      "Epoch 92: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 62s 177ms/step - loss: 0.7801 - accuracy: 0.9674 - val_loss: 2.0015 - val_accuracy: 0.7130 - lr: 0.0100\n",
      "Epoch 93/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7653 - accuracy: 0.9703\n",
      "Epoch 93: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 0.7653 - accuracy: 0.9703 - val_loss: 2.0042 - val_accuracy: 0.7164 - lr: 0.0100\n",
      "Epoch 94/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7517 - accuracy: 0.9715\n",
      "Epoch 94: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 0.7517 - accuracy: 0.9715 - val_loss: 1.9925 - val_accuracy: 0.7178 - lr: 0.0100\n",
      "Epoch 95/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7358 - accuracy: 0.9750\n",
      "Epoch 95: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 0.7358 - accuracy: 0.9750 - val_loss: 2.0426 - val_accuracy: 0.7120 - lr: 0.0100\n",
      "Epoch 96/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7256 - accuracy: 0.9753\n",
      "Epoch 96: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 62s 176ms/step - loss: 0.7256 - accuracy: 0.9753 - val_loss: 2.0370 - val_accuracy: 0.7132 - lr: 0.0100\n",
      "Epoch 97/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7104 - accuracy: 0.9775\n",
      "Epoch 97: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 0.7104 - accuracy: 0.9775 - val_loss: 2.0681 - val_accuracy: 0.7090 - lr: 0.0100\n",
      "Epoch 98/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7033 - accuracy: 0.9769\n",
      "Epoch 98: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 60s 172ms/step - loss: 0.7033 - accuracy: 0.9769 - val_loss: 2.0671 - val_accuracy: 0.7098 - lr: 0.0100\n",
      "Epoch 99/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.9787\n",
      "Epoch 99: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 60s 169ms/step - loss: 0.6893 - accuracy: 0.9787 - val_loss: 2.0414 - val_accuracy: 0.7158 - lr: 0.0100\n",
      "Epoch 100/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6775 - accuracy: 0.9805\n",
      "Epoch 100: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 57s 163ms/step - loss: 0.6775 - accuracy: 0.9805 - val_loss: 2.0358 - val_accuracy: 0.7124 - lr: 0.0100\n",
      "Epoch 101/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6656 - accuracy: 0.9826\n",
      "Epoch 101: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 0.6656 - accuracy: 0.9826 - val_loss: 2.0636 - val_accuracy: 0.7130 - lr: 0.0100\n",
      "Epoch 102/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6572 - accuracy: 0.9824\n",
      "Epoch 102: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 56s 158ms/step - loss: 0.6572 - accuracy: 0.9824 - val_loss: 2.0960 - val_accuracy: 0.7094 - lr: 0.0100\n",
      "Epoch 103/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6483 - accuracy: 0.9829\n",
      "Epoch 103: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 0.6483 - accuracy: 0.9829 - val_loss: 2.0615 - val_accuracy: 0.7134 - lr: 0.0100\n",
      "Epoch 104/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6385 - accuracy: 0.9836\n",
      "Epoch 104: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 59s 168ms/step - loss: 0.6385 - accuracy: 0.9836 - val_loss: 2.0706 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "Epoch 105/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6285 - accuracy: 0.9845\n",
      "Epoch 105: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 57s 163ms/step - loss: 0.6285 - accuracy: 0.9845 - val_loss: 2.0856 - val_accuracy: 0.7152 - lr: 0.0100\n",
      "Epoch 106/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6187 - accuracy: 0.9852\n",
      "Epoch 106: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 0.6187 - accuracy: 0.9852 - val_loss: 2.0800 - val_accuracy: 0.7142 - lr: 0.0100\n",
      "Epoch 107/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6106 - accuracy: 0.9858\n",
      "Epoch 107: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 52s 148ms/step - loss: 0.6106 - accuracy: 0.9858 - val_loss: 2.1054 - val_accuracy: 0.7134 - lr: 0.0100\n",
      "Epoch 108/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6024 - accuracy: 0.9868\n",
      "Epoch 108: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 54s 152ms/step - loss: 0.6024 - accuracy: 0.9868 - val_loss: 2.1078 - val_accuracy: 0.7072 - lr: 0.0100\n",
      "Epoch 109/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5934 - accuracy: 0.9861\n",
      "Epoch 109: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 0.5934 - accuracy: 0.9861 - val_loss: 2.0882 - val_accuracy: 0.7120 - lr: 0.0100\n",
      "Epoch 110/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5869 - accuracy: 0.9875\n",
      "Epoch 110: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 0.5869 - accuracy: 0.9875 - val_loss: 2.1152 - val_accuracy: 0.7092 - lr: 0.0100\n",
      "Epoch 111/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.9870\n",
      "Epoch 111: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 0.5773 - accuracy: 0.9870 - val_loss: 2.1308 - val_accuracy: 0.7016 - lr: 0.0100\n",
      "Epoch 112/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5709 - accuracy: 0.9875\n",
      "Epoch 112: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 0.5709 - accuracy: 0.9875 - val_loss: 2.0990 - val_accuracy: 0.7056 - lr: 0.0100\n",
      "Epoch 113/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.9878\n",
      "Epoch 113: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 0.5637 - accuracy: 0.9878 - val_loss: 2.1248 - val_accuracy: 0.7152 - lr: 0.0100\n",
      "Epoch 114/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.9882\n",
      "Epoch 114: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 56s 160ms/step - loss: 0.5550 - accuracy: 0.9882 - val_loss: 2.1161 - val_accuracy: 0.7074 - lr: 0.0100\n",
      "Epoch 115/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5505 - accuracy: 0.9878\n",
      "Epoch 115: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 53s 150ms/step - loss: 0.5505 - accuracy: 0.9878 - val_loss: 2.1094 - val_accuracy: 0.7026 - lr: 0.0100\n",
      "Epoch 116/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.9888\n",
      "Epoch 116: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 63s 178ms/step - loss: 0.5428 - accuracy: 0.9888 - val_loss: 2.0942 - val_accuracy: 0.7060 - lr: 0.0100\n",
      "Epoch 117/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5361 - accuracy: 0.9889\n",
      "Epoch 117: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 56s 158ms/step - loss: 0.5361 - accuracy: 0.9889 - val_loss: 2.1067 - val_accuracy: 0.7072 - lr: 0.0100\n",
      "Epoch 118/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.9901\n",
      "Epoch 118: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 61s 175ms/step - loss: 0.5268 - accuracy: 0.9901 - val_loss: 2.1108 - val_accuracy: 0.7076 - lr: 0.0100\n",
      "Epoch 119/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5231 - accuracy: 0.9890\n",
      "Epoch 119: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 0.5231 - accuracy: 0.9890 - val_loss: 2.1112 - val_accuracy: 0.7014 - lr: 0.0100\n",
      "Epoch 120/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.9894\n",
      "Epoch 120: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 62s 176ms/step - loss: 0.5173 - accuracy: 0.9894 - val_loss: 2.1057 - val_accuracy: 0.7070 - lr: 0.0100\n",
      "Epoch 1/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5039 - accuracy: 0.9924\n",
      "Epoch 1: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 52s 119ms/step - loss: 0.5039 - accuracy: 0.9924 - val_loss: 2.0781 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "Epoch 2/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4983 - accuracy: 0.9940\n",
      "Epoch 2: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.4983 - accuracy: 0.9940 - val_loss: 2.0750 - val_accuracy: 0.7150 - lr: 0.0100\n",
      "Epoch 3/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.9948\n",
      "Epoch 3: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4932 - accuracy: 0.9948 - val_loss: 2.0459 - val_accuracy: 0.7136 - lr: 0.0100\n",
      "Epoch 4/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4906 - accuracy: 0.9948\n",
      "Epoch 4: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4906 - accuracy: 0.9948 - val_loss: 2.0713 - val_accuracy: 0.7156 - lr: 0.0100\n",
      "Epoch 5/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4891 - accuracy: 0.9942\n",
      "Epoch 5: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4891 - accuracy: 0.9942 - val_loss: 2.0750 - val_accuracy: 0.7088 - lr: 0.0100\n",
      "Epoch 6/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4858 - accuracy: 0.9950\n",
      "Epoch 6: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4858 - accuracy: 0.9950 - val_loss: 2.0712 - val_accuracy: 0.7128 - lr: 0.0100\n",
      "Epoch 7/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4821 - accuracy: 0.9955\n",
      "Epoch 7: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.4821 - accuracy: 0.9955 - val_loss: 2.0792 - val_accuracy: 0.7114 - lr: 0.0100\n",
      "Epoch 8/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4819 - accuracy: 0.9948\n",
      "Epoch 8: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.4819 - accuracy: 0.9948 - val_loss: 2.0529 - val_accuracy: 0.7124 - lr: 0.0100\n",
      "Epoch 9/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.9953\n",
      "Epoch 9: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4787 - accuracy: 0.9953 - val_loss: 2.0796 - val_accuracy: 0.7146 - lr: 0.0100\n",
      "Epoch 10/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4764 - accuracy: 0.9956\n",
      "Epoch 10: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4764 - accuracy: 0.9956 - val_loss: 2.0737 - val_accuracy: 0.7076 - lr: 0.0100\n",
      "Epoch 11/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.9959\n",
      "Epoch 11: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.4741 - accuracy: 0.9959 - val_loss: 2.0714 - val_accuracy: 0.7104 - lr: 0.0100\n",
      "Epoch 12/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4730 - accuracy: 0.9953\n",
      "Epoch 12: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4730 - accuracy: 0.9953 - val_loss: 2.0628 - val_accuracy: 0.7130 - lr: 0.0100\n",
      "Epoch 13/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.9958\n",
      "Epoch 13: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4697 - accuracy: 0.9958 - val_loss: 2.0789 - val_accuracy: 0.7060 - lr: 0.0100\n",
      "Epoch 14/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4677 - accuracy: 0.9962\n",
      "Epoch 14: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4677 - accuracy: 0.9962 - val_loss: 2.0766 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 15/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4653 - accuracy: 0.9962\n",
      "Epoch 15: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.4653 - accuracy: 0.9962 - val_loss: 2.1254 - val_accuracy: 0.7084 - lr: 0.0100\n",
      "Epoch 16/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4646 - accuracy: 0.9957\n",
      "Epoch 16: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4646 - accuracy: 0.9957 - val_loss: 2.0687 - val_accuracy: 0.7094 - lr: 0.0100\n",
      "Epoch 17/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4625 - accuracy: 0.9959\n",
      "Epoch 17: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4625 - accuracy: 0.9959 - val_loss: 2.0775 - val_accuracy: 0.7104 - lr: 0.0100\n",
      "Epoch 18/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4617 - accuracy: 0.9955\n",
      "Epoch 18: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4617 - accuracy: 0.9955 - val_loss: 2.0702 - val_accuracy: 0.7160 - lr: 0.0100\n",
      "Epoch 19/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4605 - accuracy: 0.9957\n",
      "Epoch 19: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.4605 - accuracy: 0.9957 - val_loss: 2.0563 - val_accuracy: 0.7088 - lr: 0.0100\n",
      "Epoch 20/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4586 - accuracy: 0.9958\n",
      "Epoch 20: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4586 - accuracy: 0.9958 - val_loss: 2.0891 - val_accuracy: 0.7108 - lr: 0.0100\n",
      "Epoch 21/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4566 - accuracy: 0.9956\n",
      "Epoch 21: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.4566 - accuracy: 0.9956 - val_loss: 2.0981 - val_accuracy: 0.7070 - lr: 0.0100\n",
      "Epoch 22/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.9961\n",
      "Epoch 22: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4537 - accuracy: 0.9961 - val_loss: 2.0623 - val_accuracy: 0.7060 - lr: 0.0100\n",
      "Epoch 23/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.9960\n",
      "Epoch 23: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.4523 - accuracy: 0.9960 - val_loss: 2.0491 - val_accuracy: 0.7128 - lr: 0.0100\n",
      "Epoch 24/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4506 - accuracy: 0.9962\n",
      "Epoch 24: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4506 - accuracy: 0.9962 - val_loss: 2.0659 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 25/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.9960\n",
      "Epoch 25: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.4489 - accuracy: 0.9960 - val_loss: 2.0802 - val_accuracy: 0.7064 - lr: 0.0100\n",
      "Epoch 26/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4483 - accuracy: 0.9961\n",
      "Epoch 26: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.4483 - accuracy: 0.9961 - val_loss: 2.0632 - val_accuracy: 0.7086 - lr: 0.0100\n",
      "Epoch 27/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4467 - accuracy: 0.9959\n",
      "Epoch 27: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4467 - accuracy: 0.9959 - val_loss: 2.1054 - val_accuracy: 0.7060 - lr: 0.0100\n",
      "Epoch 28/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4439 - accuracy: 0.9966\n",
      "Epoch 28: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4439 - accuracy: 0.9966 - val_loss: 2.0561 - val_accuracy: 0.7110 - lr: 0.0100\n",
      "Epoch 29/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4432 - accuracy: 0.9961\n",
      "Epoch 29: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.4432 - accuracy: 0.9961 - val_loss: 2.0450 - val_accuracy: 0.7072 - lr: 0.0100\n",
      "Epoch 30/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4423 - accuracy: 0.9961\n",
      "Epoch 30: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 113ms/step - loss: 0.4423 - accuracy: 0.9961 - val_loss: 2.0705 - val_accuracy: 0.7056 - lr: 0.0100\n",
      "Epoch 31/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.9970\n",
      "Epoch 31: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4384 - accuracy: 0.9970 - val_loss: 2.0296 - val_accuracy: 0.7120 - lr: 0.0010\n",
      "Epoch 32/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 0.9973\n",
      "Epoch 32: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.4377 - accuracy: 0.9973 - val_loss: 2.0265 - val_accuracy: 0.7128 - lr: 0.0010\n",
      "Epoch 33/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4381 - accuracy: 0.9971\n",
      "Epoch 33: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4381 - accuracy: 0.9971 - val_loss: 2.0304 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 34/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.9972\n",
      "Epoch 34: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4361 - accuracy: 0.9972 - val_loss: 2.0278 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 35/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4364 - accuracy: 0.9972\n",
      "Epoch 35: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4364 - accuracy: 0.9972 - val_loss: 2.0313 - val_accuracy: 0.7142 - lr: 0.0010\n",
      "Epoch 36/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.9973\n",
      "Epoch 36: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4362 - accuracy: 0.9973 - val_loss: 2.0275 - val_accuracy: 0.7152 - lr: 0.0010\n",
      "Epoch 37/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.9976\n",
      "Epoch 37: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4352 - accuracy: 0.9976 - val_loss: 2.0311 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 38/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.9973\n",
      "Epoch 38: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 113ms/step - loss: 0.4351 - accuracy: 0.9973 - val_loss: 2.0283 - val_accuracy: 0.7108 - lr: 0.0010\n",
      "Epoch 39/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.9973\n",
      "Epoch 39: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 113ms/step - loss: 0.4351 - accuracy: 0.9973 - val_loss: 2.0301 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 40/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4344 - accuracy: 0.9976\n",
      "Epoch 40: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4344 - accuracy: 0.9976 - val_loss: 2.0325 - val_accuracy: 0.7144 - lr: 0.0010\n",
      "Epoch 41/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4338 - accuracy: 0.9980\n",
      "Epoch 41: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.4338 - accuracy: 0.9980 - val_loss: 2.0333 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 42/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4339 - accuracy: 0.9977\n",
      "Epoch 42: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4339 - accuracy: 0.9977 - val_loss: 2.0316 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 43/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.9977\n",
      "Epoch 43: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4335 - accuracy: 0.9977 - val_loss: 2.0389 - val_accuracy: 0.7142 - lr: 0.0010\n",
      "Epoch 44/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4339 - accuracy: 0.9975\n",
      "Epoch 44: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4339 - accuracy: 0.9975 - val_loss: 2.0286 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 45/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.9975\n",
      "Epoch 45: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.4335 - accuracy: 0.9975 - val_loss: 2.0329 - val_accuracy: 0.7146 - lr: 0.0010\n",
      "Epoch 46/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.9979\n",
      "Epoch 46: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.4331 - accuracy: 0.9979 - val_loss: 2.0336 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 47/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.9975\n",
      "Epoch 47: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4329 - accuracy: 0.9975 - val_loss: 2.0349 - val_accuracy: 0.7156 - lr: 0.0010\n",
      "Epoch 48/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.9980\n",
      "Epoch 48: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4320 - accuracy: 0.9980 - val_loss: 2.0319 - val_accuracy: 0.7154 - lr: 0.0010\n",
      "Epoch 49/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4323 - accuracy: 0.9978\n",
      "Epoch 49: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4323 - accuracy: 0.9978 - val_loss: 2.0354 - val_accuracy: 0.7152 - lr: 0.0010\n",
      "Epoch 50/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.9977\n",
      "Epoch 50: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.4320 - accuracy: 0.9977 - val_loss: 2.0341 - val_accuracy: 0.7154 - lr: 0.0010\n",
      "Epoch 51/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.9983\n",
      "Epoch 51: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.4310 - accuracy: 0.9983 - val_loss: 2.0372 - val_accuracy: 0.7158 - lr: 0.0010\n",
      "Epoch 52/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.9978\n",
      "Epoch 52: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4320 - accuracy: 0.9978 - val_loss: 2.0348 - val_accuracy: 0.7142 - lr: 0.0010\n",
      "Epoch 53/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4312 - accuracy: 0.9979\n",
      "Epoch 53: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 107ms/step - loss: 0.4312 - accuracy: 0.9979 - val_loss: 2.0341 - val_accuracy: 0.7152 - lr: 0.0010\n",
      "Epoch 54/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.9977\n",
      "Epoch 54: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.4316 - accuracy: 0.9977 - val_loss: 2.0339 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 55/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.9981\n",
      "Epoch 55: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4307 - accuracy: 0.9981 - val_loss: 2.0363 - val_accuracy: 0.7156 - lr: 0.0010\n",
      "Epoch 56/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.9983\n",
      "Epoch 56: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4307 - accuracy: 0.9983 - val_loss: 2.0340 - val_accuracy: 0.7162 - lr: 0.0010\n",
      "Epoch 57/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.9976\n",
      "Epoch 57: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4310 - accuracy: 0.9976 - val_loss: 2.0377 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 58/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.9983\n",
      "Epoch 58: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4300 - accuracy: 0.9983 - val_loss: 2.0383 - val_accuracy: 0.7158 - lr: 0.0010\n",
      "Epoch 59/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.9978\n",
      "Epoch 59: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4299 - accuracy: 0.9978 - val_loss: 2.0391 - val_accuracy: 0.7154 - lr: 0.0010\n",
      "Epoch 60/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.9977\n",
      "Epoch 60: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4298 - accuracy: 0.9977 - val_loss: 2.0350 - val_accuracy: 0.7146 - lr: 0.0010\n",
      "Epoch 61/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4306 - accuracy: 0.9977\n",
      "Epoch 61: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.4306 - accuracy: 0.9977 - val_loss: 2.0357 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 62/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4304 - accuracy: 0.9974\n",
      "Epoch 62: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.4304 - accuracy: 0.9974 - val_loss: 2.0406 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 63/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.9982\n",
      "Epoch 63: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4299 - accuracy: 0.9982 - val_loss: 2.0374 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 64/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4292 - accuracy: 0.9980\n",
      "Epoch 64: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4292 - accuracy: 0.9980 - val_loss: 2.0402 - val_accuracy: 0.7148 - lr: 0.0010\n",
      "Epoch 65/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.9980\n",
      "Epoch 65: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4291 - accuracy: 0.9980 - val_loss: 2.0393 - val_accuracy: 0.7160 - lr: 0.0010\n",
      "Epoch 66/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.9980\n",
      "Epoch 66: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.4294 - accuracy: 0.9980 - val_loss: 2.0376 - val_accuracy: 0.7146 - lr: 0.0010\n",
      "Epoch 67/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.9977\n",
      "Epoch 67: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4297 - accuracy: 0.9977 - val_loss: 2.0375 - val_accuracy: 0.7162 - lr: 0.0010\n",
      "Epoch 68/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.9978\n",
      "Epoch 68: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4289 - accuracy: 0.9978 - val_loss: 2.0395 - val_accuracy: 0.7148 - lr: 0.0010\n",
      "Epoch 69/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4292 - accuracy: 0.9978\n",
      "Epoch 69: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.4292 - accuracy: 0.9978 - val_loss: 2.0410 - val_accuracy: 0.7148 - lr: 0.0010\n",
      "Epoch 70/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.9982\n",
      "Epoch 70: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 113ms/step - loss: 0.4284 - accuracy: 0.9982 - val_loss: 2.0399 - val_accuracy: 0.7158 - lr: 0.0010\n",
      "Epoch 71/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.9983\n",
      "Epoch 71: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.4281 - accuracy: 0.9983 - val_loss: 2.0386 - val_accuracy: 0.7146 - lr: 0.0010\n",
      "Epoch 72/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.9978\n",
      "Epoch 72: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.4287 - accuracy: 0.9978 - val_loss: 2.0435 - val_accuracy: 0.7164 - lr: 0.0010\n",
      "Epoch 73/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.9979\n",
      "Epoch 73: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4279 - accuracy: 0.9979 - val_loss: 2.0420 - val_accuracy: 0.7168 - lr: 0.0010\n",
      "Epoch 74/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4286 - accuracy: 0.9975\n",
      "Epoch 74: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4286 - accuracy: 0.9975 - val_loss: 2.0361 - val_accuracy: 0.7180 - lr: 0.0010\n",
      "Epoch 75/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.9977\n",
      "Epoch 75: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4282 - accuracy: 0.9977 - val_loss: 2.0409 - val_accuracy: 0.7164 - lr: 0.0010\n",
      "Epoch 76/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.9982\n",
      "Epoch 76: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.4270 - accuracy: 0.9982 - val_loss: 2.0395 - val_accuracy: 0.7162 - lr: 0.0010\n",
      "Epoch 77/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4267 - accuracy: 0.9983\n",
      "Epoch 77: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.4267 - accuracy: 0.9983 - val_loss: 2.0387 - val_accuracy: 0.7164 - lr: 0.0010\n",
      "Epoch 78/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4275 - accuracy: 0.9979\n",
      "Epoch 78: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.4275 - accuracy: 0.9979 - val_loss: 2.0407 - val_accuracy: 0.7140 - lr: 0.0010\n",
      "Epoch 79/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.9980\n",
      "Epoch 79: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4279 - accuracy: 0.9980 - val_loss: 2.0380 - val_accuracy: 0.7148 - lr: 0.0010\n",
      "Epoch 80/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4269 - accuracy: 0.9981\n",
      "Epoch 80: val_loss did not improve from 1.89821\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.4269 - accuracy: 0.9981 - val_loss: 2.0350 - val_accuracy: 0.7150 - lr: 0.0010\n",
      "Current:  308\n",
      "313/313 [==============================] - 9s 20ms/step\n",
      "Accuracy: 70.02000000000001\n",
      "Error: 29.97999999999999\n",
      "ECE: 0.18582916443198916\n",
      "MCE: 0.4018810902565633\n",
      "Loss: 1.691222966691046\n",
      "brier: 0.27485191721377744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[29.97999999999999,\n",
       " 0.18582916443198916,\n",
       " 0.4018810902565633,\n",
       " 1.691222966691046,\n",
       " 0.27485191721377744]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, datagen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list, batch_size=128,lr_schedule = [[0, 0.1],[80,0.01],[150,0.001]],cbks=[checkpointer], name='resnet_cifar100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e16428-c104-4e2c-9328-9ca2d69a303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.9:\n",
    "    freezing_list.append(int(epochs*0.6))\n",
    "  elif i < len(model.layers) * 0.98:\n",
    "    freezing_list.append(int(epochs*0.96))\n",
    "freezing_list.append(epochs)\n",
    "checkpointer = ModelCheckpoint('model_resnet_c100_best_2.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010dfbba-3e84-4fd4-a286-c8df7ca19fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_100\n",
      ".........vars\n",
      "......activation_101\n",
      ".........vars\n",
      "......activation_102\n",
      ".........vars\n",
      "......activation_103\n",
      ".........vars\n",
      "......activation_104\n",
      ".........vars\n",
      "......activation_105\n",
      ".........vars\n",
      "......activation_106\n",
      ".........vars\n",
      "......activation_107\n",
      ".........vars\n",
      "......activation_108\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_55\n",
      ".........vars\n",
      "......activation_56\n",
      ".........vars\n",
      "......activation_57\n",
      ".........vars\n",
      "......activation_58\n",
      ".........vars\n",
      "......activation_59\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_60\n",
      ".........vars\n",
      "......activation_61\n",
      ".........vars\n",
      "......activation_62\n",
      ".........vars\n",
      "......activation_63\n",
      ".........vars\n",
      "......activation_64\n",
      ".........vars\n",
      "......activation_65\n",
      ".........vars\n",
      "......activation_66\n",
      ".........vars\n",
      "......activation_67\n",
      ".........vars\n",
      "......activation_68\n",
      ".........vars\n",
      "......activation_69\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_70\n",
      ".........vars\n",
      "......activation_71\n",
      ".........vars\n",
      "......activation_72\n",
      ".........vars\n",
      "......activation_73\n",
      ".........vars\n",
      "......activation_74\n",
      ".........vars\n",
      "......activation_75\n",
      ".........vars\n",
      "......activation_76\n",
      ".........vars\n",
      "......activation_77\n",
      ".........vars\n",
      "......activation_78\n",
      ".........vars\n",
      "......activation_79\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_80\n",
      ".........vars\n",
      "......activation_81\n",
      ".........vars\n",
      "......activation_82\n",
      ".........vars\n",
      "......activation_83\n",
      ".........vars\n",
      "......activation_84\n",
      ".........vars\n",
      "......activation_85\n",
      ".........vars\n",
      "......activation_86\n",
      ".........vars\n",
      "......activation_87\n",
      ".........vars\n",
      "......activation_88\n",
      ".........vars\n",
      "......activation_89\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......activation_90\n",
      ".........vars\n",
      "......activation_91\n",
      ".........vars\n",
      "......activation_92\n",
      ".........vars\n",
      "......activation_93\n",
      ".........vars\n",
      "......activation_94\n",
      ".........vars\n",
      "......activation_95\n",
      ".........vars\n",
      "......activation_96\n",
      ".........vars\n",
      "......activation_97\n",
      ".........vars\n",
      "......activation_98\n",
      ".........vars\n",
      "......activation_99\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_109\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_110\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-21 16:44:28           64\n",
      "config.json                                    2023-04-21 16:44:28       179539\n",
      "variables.h5                                   2023-04-21 16:44:29      7961936\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-21 16:44:28           64\n",
      "config.json                                    2023-04-21 16:44:28       179539\n",
      "variables.h5                                   2023-04-21 16:44:28      7961936\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_100\n",
      ".........vars\n",
      "......activation_101\n",
      ".........vars\n",
      "......activation_102\n",
      ".........vars\n",
      "......activation_103\n",
      ".........vars\n",
      "......activation_104\n",
      ".........vars\n",
      "......activation_105\n",
      ".........vars\n",
      "......activation_106\n",
      ".........vars\n",
      "......activation_107\n",
      ".........vars\n",
      "......activation_108\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_55\n",
      ".........vars\n",
      "......activation_56\n",
      ".........vars\n",
      "......activation_57\n",
      ".........vars\n",
      "......activation_58\n",
      ".........vars\n",
      "......activation_59\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_60\n",
      ".........vars\n",
      "......activation_61\n",
      ".........vars\n",
      "......activation_62\n",
      ".........vars\n",
      "......activation_63\n",
      ".........vars\n",
      "......activation_64\n",
      ".........vars\n",
      "......activation_65\n",
      ".........vars\n",
      "......activation_66\n",
      ".........vars\n",
      "......activation_67\n",
      ".........vars\n",
      "......activation_68\n",
      ".........vars\n",
      "......activation_69\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_70\n",
      ".........vars\n",
      "......activation_71\n",
      ".........vars\n",
      "......activation_72\n",
      ".........vars\n",
      "......activation_73\n",
      ".........vars\n",
      "......activation_74\n",
      ".........vars\n",
      "......activation_75\n",
      ".........vars\n",
      "......activation_76\n",
      ".........vars\n",
      "......activation_77\n",
      ".........vars\n",
      "......activation_78\n",
      ".........vars\n",
      "......activation_79\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_80\n",
      ".........vars\n",
      "......activation_81\n",
      ".........vars\n",
      "......activation_82\n",
      ".........vars\n",
      "......activation_83\n",
      ".........vars\n",
      "......activation_84\n",
      ".........vars\n",
      "......activation_85\n",
      ".........vars\n",
      "......activation_86\n",
      ".........vars\n",
      "......activation_87\n",
      ".........vars\n",
      "......activation_88\n",
      ".........vars\n",
      "......activation_89\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......activation_90\n",
      ".........vars\n",
      "......activation_91\n",
      ".........vars\n",
      "......activation_92\n",
      ".........vars\n",
      "......activation_93\n",
      ".........vars\n",
      "......activation_94\n",
      ".........vars\n",
      "......activation_95\n",
      ".........vars\n",
      "......activation_96\n",
      ".........vars\n",
      "......activation_97\n",
      ".........vars\n",
      "......activation_98\n",
      ".........vars\n",
      "......activation_99\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_109\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_110\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 16:44:46.946179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - ETA: 0s - loss: 4.8158 - accuracy: 0.0792\n",
      "Epoch 1: val_loss improved from inf to 5.01594, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 61s 112ms/step - loss: 4.8158 - accuracy: 0.0792 - val_loss: 5.0159 - val_accuracy: 0.0826 - lr: 0.1000\n",
      "Epoch 2/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.0994 - accuracy: 0.1757\n",
      "Epoch 2: val_loss improved from 5.01594 to 4.94295, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 4.0994 - accuracy: 0.1757 - val_loss: 4.9429 - val_accuracy: 0.1124 - lr: 0.1000\n",
      "Epoch 3/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5788 - accuracy: 0.2574\n",
      "Epoch 3: val_loss improved from 4.94295 to 4.70354, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 3.5788 - accuracy: 0.2574 - val_loss: 4.7035 - val_accuracy: 0.1598 - lr: 0.1000\n",
      "Epoch 4/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2345 - accuracy: 0.3157\n",
      "Epoch 4: val_loss improved from 4.70354 to 4.03516, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 3.2345 - accuracy: 0.3157 - val_loss: 4.0352 - val_accuracy: 0.2234 - lr: 0.1000\n",
      "Epoch 5/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9585 - accuracy: 0.3684\n",
      "Epoch 5: val_loss improved from 4.03516 to 4.02280, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 2.9585 - accuracy: 0.3684 - val_loss: 4.0228 - val_accuracy: 0.2276 - lr: 0.1000\n",
      "Epoch 6/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.7615 - accuracy: 0.4056\n",
      "Epoch 6: val_loss improved from 4.02280 to 2.93774, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 2.7615 - accuracy: 0.4056 - val_loss: 2.9377 - val_accuracy: 0.3780 - lr: 0.1000\n",
      "Epoch 7/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.5995 - accuracy: 0.4411\n",
      "Epoch 7: val_loss did not improve from 2.93774\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 2.5995 - accuracy: 0.4411 - val_loss: 3.1160 - val_accuracy: 0.3660 - lr: 0.1000\n",
      "Epoch 8/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4652 - accuracy: 0.4679\n",
      "Epoch 8: val_loss did not improve from 2.93774\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 2.4652 - accuracy: 0.4679 - val_loss: 2.9860 - val_accuracy: 0.3674 - lr: 0.1000\n",
      "Epoch 9/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3656 - accuracy: 0.4901\n",
      "Epoch 9: val_loss improved from 2.93774 to 2.93674, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 2.3656 - accuracy: 0.4901 - val_loss: 2.9367 - val_accuracy: 0.4034 - lr: 0.1000\n",
      "Epoch 10/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.2674 - accuracy: 0.5094\n",
      "Epoch 10: val_loss did not improve from 2.93674\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 2.2674 - accuracy: 0.5094 - val_loss: 3.6599 - val_accuracy: 0.3310 - lr: 0.1000\n",
      "Epoch 11/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.1981 - accuracy: 0.5268\n",
      "Epoch 11: val_loss improved from 2.93674 to 2.82776, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 2.1981 - accuracy: 0.5268 - val_loss: 2.8278 - val_accuracy: 0.4254 - lr: 0.1000\n",
      "Epoch 12/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.1270 - accuracy: 0.5413\n",
      "Epoch 12: val_loss did not improve from 2.82776\n",
      "351/351 [==============================] - 35s 99ms/step - loss: 2.1270 - accuracy: 0.5413 - val_loss: 2.8999 - val_accuracy: 0.4114 - lr: 0.1000\n",
      "Epoch 13/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0657 - accuracy: 0.5552\n",
      "Epoch 13: val_loss did not improve from 2.82776\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 2.0657 - accuracy: 0.5552 - val_loss: 3.3469 - val_accuracy: 0.3844 - lr: 0.1000\n",
      "Epoch 14/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0167 - accuracy: 0.5694\n",
      "Epoch 14: val_loss improved from 2.82776 to 2.59656, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 2.0167 - accuracy: 0.5694 - val_loss: 2.5966 - val_accuracy: 0.4534 - lr: 0.1000\n",
      "Epoch 15/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9660 - accuracy: 0.5793\n",
      "Epoch 15: val_loss did not improve from 2.59656\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.9660 - accuracy: 0.5793 - val_loss: 2.7339 - val_accuracy: 0.4408 - lr: 0.1000\n",
      "Epoch 16/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9292 - accuracy: 0.5874\n",
      "Epoch 16: val_loss did not improve from 2.59656\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 1.9292 - accuracy: 0.5874 - val_loss: 3.2113 - val_accuracy: 0.3934 - lr: 0.1000\n",
      "Epoch 17/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9025 - accuracy: 0.5963\n",
      "Epoch 17: val_loss did not improve from 2.59656\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.9025 - accuracy: 0.5963 - val_loss: 2.8005 - val_accuracy: 0.4360 - lr: 0.1000\n",
      "Epoch 18/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8724 - accuracy: 0.6057\n",
      "Epoch 18: val_loss did not improve from 2.59656\n",
      "351/351 [==============================] - 35s 99ms/step - loss: 1.8724 - accuracy: 0.6057 - val_loss: 2.9076 - val_accuracy: 0.4478 - lr: 0.1000\n",
      "Epoch 19/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8414 - accuracy: 0.6135\n",
      "Epoch 19: val_loss improved from 2.59656 to 2.41501, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.8414 - accuracy: 0.6135 - val_loss: 2.4150 - val_accuracy: 0.5036 - lr: 0.1000\n",
      "Epoch 20/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8128 - accuracy: 0.6240\n",
      "Epoch 20: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.8128 - accuracy: 0.6240 - val_loss: 2.4280 - val_accuracy: 0.5026 - lr: 0.1000\n",
      "Epoch 21/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7872 - accuracy: 0.6288\n",
      "Epoch 21: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.7872 - accuracy: 0.6288 - val_loss: 2.5663 - val_accuracy: 0.4752 - lr: 0.1000\n",
      "Epoch 22/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7688 - accuracy: 0.6381\n",
      "Epoch 22: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.7688 - accuracy: 0.6381 - val_loss: 2.9009 - val_accuracy: 0.4576 - lr: 0.1000\n",
      "Epoch 23/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7557 - accuracy: 0.6425\n",
      "Epoch 23: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.7557 - accuracy: 0.6425 - val_loss: 2.5104 - val_accuracy: 0.4894 - lr: 0.1000\n",
      "Epoch 24/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7255 - accuracy: 0.6523\n",
      "Epoch 24: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.7255 - accuracy: 0.6523 - val_loss: 2.8427 - val_accuracy: 0.4768 - lr: 0.1000\n",
      "Epoch 25/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7153 - accuracy: 0.6567\n",
      "Epoch 25: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 1.7153 - accuracy: 0.6567 - val_loss: 2.6970 - val_accuracy: 0.4962 - lr: 0.1000\n",
      "Epoch 26/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7037 - accuracy: 0.6597\n",
      "Epoch 26: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.7037 - accuracy: 0.6597 - val_loss: 2.8953 - val_accuracy: 0.4646 - lr: 0.1000\n",
      "Epoch 27/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6905 - accuracy: 0.6653\n",
      "Epoch 27: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 1.6905 - accuracy: 0.6653 - val_loss: 2.5713 - val_accuracy: 0.5134 - lr: 0.1000\n",
      "Epoch 28/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6797 - accuracy: 0.6693\n",
      "Epoch 28: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.6797 - accuracy: 0.6693 - val_loss: 2.7059 - val_accuracy: 0.4894 - lr: 0.1000\n",
      "Epoch 29/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6706 - accuracy: 0.6723\n",
      "Epoch 29: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 1.6706 - accuracy: 0.6723 - val_loss: 2.5141 - val_accuracy: 0.5156 - lr: 0.1000\n",
      "Epoch 30/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6565 - accuracy: 0.6790\n",
      "Epoch 30: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.6565 - accuracy: 0.6790 - val_loss: 2.8761 - val_accuracy: 0.4710 - lr: 0.1000\n",
      "Epoch 31/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6455 - accuracy: 0.6844\n",
      "Epoch 31: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.6455 - accuracy: 0.6844 - val_loss: 3.1228 - val_accuracy: 0.4446 - lr: 0.1000\n",
      "Epoch 32/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6384 - accuracy: 0.6873\n",
      "Epoch 32: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.6384 - accuracy: 0.6873 - val_loss: 2.6207 - val_accuracy: 0.5134 - lr: 0.1000\n",
      "Epoch 33/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6328 - accuracy: 0.6896\n",
      "Epoch 33: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.6328 - accuracy: 0.6896 - val_loss: 2.4343 - val_accuracy: 0.5478 - lr: 0.1000\n",
      "Epoch 34/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6328 - accuracy: 0.6913\n",
      "Epoch 34: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.6328 - accuracy: 0.6913 - val_loss: 2.9302 - val_accuracy: 0.4782 - lr: 0.1000\n",
      "Epoch 35/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6120 - accuracy: 0.6966\n",
      "Epoch 35: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.6120 - accuracy: 0.6966 - val_loss: 2.6458 - val_accuracy: 0.5116 - lr: 0.1000\n",
      "Epoch 36/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6075 - accuracy: 0.6996\n",
      "Epoch 36: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.6075 - accuracy: 0.6996 - val_loss: 2.7399 - val_accuracy: 0.5192 - lr: 0.1000\n",
      "Epoch 37/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6047 - accuracy: 0.7028\n",
      "Epoch 37: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.6047 - accuracy: 0.7028 - val_loss: 2.6105 - val_accuracy: 0.5120 - lr: 0.1000\n",
      "Epoch 38/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5999 - accuracy: 0.7046\n",
      "Epoch 38: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5999 - accuracy: 0.7046 - val_loss: 3.0005 - val_accuracy: 0.4690 - lr: 0.1000\n",
      "Epoch 39/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5961 - accuracy: 0.7088\n",
      "Epoch 39: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.5961 - accuracy: 0.7088 - val_loss: 2.6176 - val_accuracy: 0.5228 - lr: 0.1000\n",
      "Epoch 40/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5852 - accuracy: 0.7148\n",
      "Epoch 40: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5852 - accuracy: 0.7148 - val_loss: 2.7617 - val_accuracy: 0.4972 - lr: 0.1000\n",
      "Epoch 41/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5898 - accuracy: 0.7121\n",
      "Epoch 41: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5898 - accuracy: 0.7121 - val_loss: 2.8737 - val_accuracy: 0.4998 - lr: 0.1000\n",
      "Epoch 42/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5802 - accuracy: 0.7180\n",
      "Epoch 42: val_loss did not improve from 2.41501\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.5802 - accuracy: 0.7180 - val_loss: 2.9715 - val_accuracy: 0.4864 - lr: 0.1000\n",
      "Epoch 43/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5880 - accuracy: 0.7142\n",
      "Epoch 43: val_loss improved from 2.41501 to 2.28184, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5880 - accuracy: 0.7142 - val_loss: 2.2818 - val_accuracy: 0.5872 - lr: 0.1000\n",
      "Epoch 44/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5716 - accuracy: 0.7230\n",
      "Epoch 44: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5716 - accuracy: 0.7230 - val_loss: 2.6260 - val_accuracy: 0.5242 - lr: 0.1000\n",
      "Epoch 45/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5626 - accuracy: 0.7249\n",
      "Epoch 45: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5626 - accuracy: 0.7249 - val_loss: 2.4249 - val_accuracy: 0.5622 - lr: 0.1000\n",
      "Epoch 46/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5584 - accuracy: 0.7292\n",
      "Epoch 46: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5584 - accuracy: 0.7292 - val_loss: 2.8576 - val_accuracy: 0.5242 - lr: 0.1000\n",
      "Epoch 47/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5529 - accuracy: 0.7311\n",
      "Epoch 47: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5529 - accuracy: 0.7311 - val_loss: 2.5298 - val_accuracy: 0.5526 - lr: 0.1000\n",
      "Epoch 48/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5657 - accuracy: 0.7287\n",
      "Epoch 48: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.5657 - accuracy: 0.7287 - val_loss: 2.6839 - val_accuracy: 0.5306 - lr: 0.1000\n",
      "Epoch 49/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5539 - accuracy: 0.7329\n",
      "Epoch 49: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.5539 - accuracy: 0.7329 - val_loss: 2.4679 - val_accuracy: 0.5610 - lr: 0.1000\n",
      "Epoch 50/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5482 - accuracy: 0.7342\n",
      "Epoch 50: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.5482 - accuracy: 0.7342 - val_loss: 2.8533 - val_accuracy: 0.5116 - lr: 0.1000\n",
      "Epoch 51/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5549 - accuracy: 0.7347\n",
      "Epoch 51: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5549 - accuracy: 0.7347 - val_loss: 2.7353 - val_accuracy: 0.5170 - lr: 0.1000\n",
      "Epoch 52/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5418 - accuracy: 0.7408\n",
      "Epoch 52: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.5418 - accuracy: 0.7408 - val_loss: 2.8059 - val_accuracy: 0.5116 - lr: 0.1000\n",
      "Epoch 53/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5444 - accuracy: 0.7389\n",
      "Epoch 53: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.5444 - accuracy: 0.7389 - val_loss: 2.8773 - val_accuracy: 0.5182 - lr: 0.1000\n",
      "Epoch 54/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5398 - accuracy: 0.7413\n",
      "Epoch 54: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5398 - accuracy: 0.7413 - val_loss: 2.3912 - val_accuracy: 0.5740 - lr: 0.1000\n",
      "Epoch 55/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5460 - accuracy: 0.7417\n",
      "Epoch 55: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.5460 - accuracy: 0.7417 - val_loss: 2.6464 - val_accuracy: 0.5312 - lr: 0.1000\n",
      "Epoch 56/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5331 - accuracy: 0.7440\n",
      "Epoch 56: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5331 - accuracy: 0.7440 - val_loss: 2.5557 - val_accuracy: 0.5520 - lr: 0.1000\n",
      "Epoch 57/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5453 - accuracy: 0.7426\n",
      "Epoch 57: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5453 - accuracy: 0.7426 - val_loss: 2.5236 - val_accuracy: 0.5498 - lr: 0.1000\n",
      "Epoch 58/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5267 - accuracy: 0.7472\n",
      "Epoch 58: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5267 - accuracy: 0.7472 - val_loss: 2.5037 - val_accuracy: 0.5636 - lr: 0.1000\n",
      "Epoch 59/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5337 - accuracy: 0.7482\n",
      "Epoch 59: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.5337 - accuracy: 0.7482 - val_loss: 2.7275 - val_accuracy: 0.5170 - lr: 0.1000\n",
      "Epoch 60/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5293 - accuracy: 0.7526\n",
      "Epoch 60: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5293 - accuracy: 0.7526 - val_loss: 2.8675 - val_accuracy: 0.5224 - lr: 0.1000\n",
      "Epoch 61/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5354 - accuracy: 0.7481\n",
      "Epoch 61: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.5354 - accuracy: 0.7481 - val_loss: 2.5031 - val_accuracy: 0.5538 - lr: 0.1000\n",
      "Epoch 62/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5211 - accuracy: 0.7539\n",
      "Epoch 62: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.5211 - accuracy: 0.7539 - val_loss: 2.8008 - val_accuracy: 0.5268 - lr: 0.1000\n",
      "Epoch 63/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5295 - accuracy: 0.7527\n",
      "Epoch 63: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5295 - accuracy: 0.7527 - val_loss: 2.6991 - val_accuracy: 0.5436 - lr: 0.1000\n",
      "Epoch 64/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5174 - accuracy: 0.7585\n",
      "Epoch 64: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5174 - accuracy: 0.7585 - val_loss: 2.7679 - val_accuracy: 0.5404 - lr: 0.1000\n",
      "Epoch 65/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5193 - accuracy: 0.7567\n",
      "Epoch 65: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.5193 - accuracy: 0.7567 - val_loss: 2.6769 - val_accuracy: 0.5510 - lr: 0.1000\n",
      "Epoch 66/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5103 - accuracy: 0.7619\n",
      "Epoch 66: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5103 - accuracy: 0.7619 - val_loss: 2.4660 - val_accuracy: 0.5770 - lr: 0.1000\n",
      "Epoch 67/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5335 - accuracy: 0.7545\n",
      "Epoch 67: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 1.5335 - accuracy: 0.7545 - val_loss: 2.9438 - val_accuracy: 0.5114 - lr: 0.1000\n",
      "Epoch 68/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5081 - accuracy: 0.7643\n",
      "Epoch 68: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5081 - accuracy: 0.7643 - val_loss: 2.4709 - val_accuracy: 0.5710 - lr: 0.1000\n",
      "Epoch 69/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5124 - accuracy: 0.7606\n",
      "Epoch 69: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.5124 - accuracy: 0.7606 - val_loss: 2.4259 - val_accuracy: 0.5826 - lr: 0.1000\n",
      "Epoch 70/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5166 - accuracy: 0.7631\n",
      "Epoch 70: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 1.5166 - accuracy: 0.7631 - val_loss: 2.6134 - val_accuracy: 0.5580 - lr: 0.1000\n",
      "Epoch 71/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5150 - accuracy: 0.7611\n",
      "Epoch 71: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.5150 - accuracy: 0.7611 - val_loss: 3.8942 - val_accuracy: 0.4184 - lr: 0.1000\n",
      "Epoch 72/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5172 - accuracy: 0.7616\n",
      "Epoch 72: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5172 - accuracy: 0.7616 - val_loss: 2.6099 - val_accuracy: 0.5676 - lr: 0.1000\n",
      "Epoch 73/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5035 - accuracy: 0.7674\n",
      "Epoch 73: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.5035 - accuracy: 0.7674 - val_loss: 2.8301 - val_accuracy: 0.5364 - lr: 0.1000\n",
      "Epoch 74/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5140 - accuracy: 0.7625\n",
      "Epoch 74: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.5140 - accuracy: 0.7625 - val_loss: 3.1326 - val_accuracy: 0.4940 - lr: 0.1000\n",
      "Epoch 75/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5030 - accuracy: 0.7682\n",
      "Epoch 75: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.5030 - accuracy: 0.7682 - val_loss: 3.2113 - val_accuracy: 0.4918 - lr: 0.1000\n",
      "Epoch 76/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5124 - accuracy: 0.7649\n",
      "Epoch 76: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 1.5124 - accuracy: 0.7649 - val_loss: 2.7065 - val_accuracy: 0.5610 - lr: 0.1000\n",
      "Epoch 77/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4981 - accuracy: 0.7702\n",
      "Epoch 77: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.4981 - accuracy: 0.7702 - val_loss: 2.5477 - val_accuracy: 0.5850 - lr: 0.1000\n",
      "Epoch 78/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5005 - accuracy: 0.7714\n",
      "Epoch 78: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5005 - accuracy: 0.7714 - val_loss: 2.8465 - val_accuracy: 0.5422 - lr: 0.1000\n",
      "Epoch 79/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5112 - accuracy: 0.7688\n",
      "Epoch 79: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.5112 - accuracy: 0.7688 - val_loss: 2.6347 - val_accuracy: 0.5624 - lr: 0.1000\n",
      "Epoch 80/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5032 - accuracy: 0.7723\n",
      "Epoch 80: val_loss did not improve from 2.28184\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.5032 - accuracy: 0.7723 - val_loss: 3.1012 - val_accuracy: 0.5126 - lr: 0.1000\n",
      "Epoch 81/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2066 - accuracy: 0.8642\n",
      "Epoch 81: val_loss improved from 2.28184 to 1.93309, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.2066 - accuracy: 0.8642 - val_loss: 1.9331 - val_accuracy: 0.6940 - lr: 0.0100\n",
      "Epoch 82/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0487 - accuracy: 0.9101\n",
      "Epoch 82: val_loss improved from 1.93309 to 1.92179, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0487 - accuracy: 0.9101 - val_loss: 1.9218 - val_accuracy: 0.7028 - lr: 0.0100\n",
      "Epoch 83/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0016 - accuracy: 0.9229\n",
      "Epoch 83: val_loss improved from 1.92179 to 1.91244, saving model to model_resnet_c100_best_2.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0016 - accuracy: 0.9229 - val_loss: 1.9124 - val_accuracy: 0.7050 - lr: 0.0100\n",
      "Epoch 84/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9569 - accuracy: 0.9332\n",
      "Epoch 84: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.9569 - accuracy: 0.9332 - val_loss: 1.9316 - val_accuracy: 0.6990 - lr: 0.0100\n",
      "Epoch 85/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9231 - accuracy: 0.9410\n",
      "Epoch 85: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.9231 - accuracy: 0.9410 - val_loss: 1.9471 - val_accuracy: 0.7050 - lr: 0.0100\n",
      "Epoch 86/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8966 - accuracy: 0.9456\n",
      "Epoch 86: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 0.8966 - accuracy: 0.9456 - val_loss: 1.9382 - val_accuracy: 0.7082 - lr: 0.0100\n",
      "Epoch 87/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8747 - accuracy: 0.9515\n",
      "Epoch 87: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.8747 - accuracy: 0.9515 - val_loss: 1.9444 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 88/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8543 - accuracy: 0.9542\n",
      "Epoch 88: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.8543 - accuracy: 0.9542 - val_loss: 1.9731 - val_accuracy: 0.7072 - lr: 0.0100\n",
      "Epoch 89/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8350 - accuracy: 0.9583\n",
      "Epoch 89: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.8350 - accuracy: 0.9583 - val_loss: 1.9685 - val_accuracy: 0.7088 - lr: 0.0100\n",
      "Epoch 90/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8122 - accuracy: 0.9633\n",
      "Epoch 90: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.8122 - accuracy: 0.9633 - val_loss: 1.9539 - val_accuracy: 0.7078 - lr: 0.0100\n",
      "Epoch 91/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7990 - accuracy: 0.9642\n",
      "Epoch 91: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.7990 - accuracy: 0.9642 - val_loss: 2.0072 - val_accuracy: 0.7070 - lr: 0.0100\n",
      "Epoch 92/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7810 - accuracy: 0.9683\n",
      "Epoch 92: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.7810 - accuracy: 0.9683 - val_loss: 2.0145 - val_accuracy: 0.7056 - lr: 0.0100\n",
      "Epoch 93/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7657 - accuracy: 0.9706\n",
      "Epoch 93: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.7657 - accuracy: 0.9706 - val_loss: 2.0187 - val_accuracy: 0.7084 - lr: 0.0100\n",
      "Epoch 94/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7536 - accuracy: 0.9709\n",
      "Epoch 94: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 0.7536 - accuracy: 0.9709 - val_loss: 2.0164 - val_accuracy: 0.7130 - lr: 0.0100\n",
      "Epoch 95/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7360 - accuracy: 0.9740\n",
      "Epoch 95: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.7360 - accuracy: 0.9740 - val_loss: 2.0312 - val_accuracy: 0.7108 - lr: 0.0100\n",
      "Epoch 96/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7263 - accuracy: 0.9750\n",
      "Epoch 96: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.7263 - accuracy: 0.9750 - val_loss: 2.0323 - val_accuracy: 0.7098 - lr: 0.0100\n",
      "Epoch 97/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7132 - accuracy: 0.9765\n",
      "Epoch 97: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.7132 - accuracy: 0.9765 - val_loss: 2.0388 - val_accuracy: 0.7092 - lr: 0.0100\n",
      "Epoch 98/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.9773\n",
      "Epoch 98: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.7005 - accuracy: 0.9773 - val_loss: 2.0583 - val_accuracy: 0.7004 - lr: 0.0100\n",
      "Epoch 99/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6879 - accuracy: 0.9796\n",
      "Epoch 99: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 0.6879 - accuracy: 0.9796 - val_loss: 2.0751 - val_accuracy: 0.7026 - lr: 0.0100\n",
      "Epoch 100/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6780 - accuracy: 0.9809\n",
      "Epoch 100: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.6780 - accuracy: 0.9809 - val_loss: 2.0390 - val_accuracy: 0.7096 - lr: 0.0100\n",
      "Epoch 101/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6678 - accuracy: 0.9810\n",
      "Epoch 101: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.6678 - accuracy: 0.9810 - val_loss: 2.0872 - val_accuracy: 0.7010 - lr: 0.0100\n",
      "Epoch 102/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.9824\n",
      "Epoch 102: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.6559 - accuracy: 0.9824 - val_loss: 2.0514 - val_accuracy: 0.7000 - lr: 0.0100\n",
      "Epoch 103/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6467 - accuracy: 0.9836\n",
      "Epoch 103: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 0.6467 - accuracy: 0.9836 - val_loss: 2.0913 - val_accuracy: 0.7006 - lr: 0.0100\n",
      "Epoch 104/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6379 - accuracy: 0.9836\n",
      "Epoch 104: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.6379 - accuracy: 0.9836 - val_loss: 2.0954 - val_accuracy: 0.6988 - lr: 0.0100\n",
      "Epoch 105/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.9838\n",
      "Epoch 105: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.6293 - accuracy: 0.9838 - val_loss: 2.0594 - val_accuracy: 0.7018 - lr: 0.0100\n",
      "Epoch 106/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6185 - accuracy: 0.9861\n",
      "Epoch 106: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 0.6185 - accuracy: 0.9861 - val_loss: 2.0813 - val_accuracy: 0.6988 - lr: 0.0100\n",
      "Epoch 107/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6102 - accuracy: 0.9860\n",
      "Epoch 107: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.6102 - accuracy: 0.9860 - val_loss: 2.0891 - val_accuracy: 0.7000 - lr: 0.0100\n",
      "Epoch 108/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6036 - accuracy: 0.9861\n",
      "Epoch 108: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 0.6036 - accuracy: 0.9861 - val_loss: 2.1181 - val_accuracy: 0.6962 - lr: 0.0100\n",
      "Epoch 109/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5955 - accuracy: 0.9862\n",
      "Epoch 109: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 0.5955 - accuracy: 0.9862 - val_loss: 2.1060 - val_accuracy: 0.6974 - lr: 0.0100\n",
      "Epoch 110/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.9866\n",
      "Epoch 110: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5881 - accuracy: 0.9866 - val_loss: 2.0716 - val_accuracy: 0.7014 - lr: 0.0100\n",
      "Epoch 111/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5797 - accuracy: 0.9869\n",
      "Epoch 111: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5797 - accuracy: 0.9869 - val_loss: 2.1004 - val_accuracy: 0.6938 - lr: 0.0100\n",
      "Epoch 112/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5703 - accuracy: 0.9884\n",
      "Epoch 112: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.5703 - accuracy: 0.9884 - val_loss: 2.0927 - val_accuracy: 0.7028 - lr: 0.0100\n",
      "Epoch 113/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.9875\n",
      "Epoch 113: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.5644 - accuracy: 0.9875 - val_loss: 2.1043 - val_accuracy: 0.6920 - lr: 0.0100\n",
      "Epoch 114/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5579 - accuracy: 0.9873\n",
      "Epoch 114: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.5579 - accuracy: 0.9873 - val_loss: 2.0876 - val_accuracy: 0.7006 - lr: 0.0100\n",
      "Epoch 115/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.9887\n",
      "Epoch 115: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 99ms/step - loss: 0.5493 - accuracy: 0.9887 - val_loss: 2.0824 - val_accuracy: 0.6958 - lr: 0.0100\n",
      "Epoch 116/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5425 - accuracy: 0.9886\n",
      "Epoch 116: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.5425 - accuracy: 0.9886 - val_loss: 2.0973 - val_accuracy: 0.6940 - lr: 0.0100\n",
      "Epoch 117/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5367 - accuracy: 0.9887\n",
      "Epoch 117: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5367 - accuracy: 0.9887 - val_loss: 2.0724 - val_accuracy: 0.6982 - lr: 0.0100\n",
      "Epoch 118/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.9893\n",
      "Epoch 118: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.5295 - accuracy: 0.9893 - val_loss: 2.1047 - val_accuracy: 0.6996 - lr: 0.0100\n",
      "Epoch 119/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5227 - accuracy: 0.9898\n",
      "Epoch 119: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.5227 - accuracy: 0.9898 - val_loss: 2.1087 - val_accuracy: 0.6962 - lr: 0.0100\n",
      "Epoch 120/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5193 - accuracy: 0.9887\n",
      "Epoch 120: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.5193 - accuracy: 0.9887 - val_loss: 2.1194 - val_accuracy: 0.7002 - lr: 0.0100\n",
      "Epoch 1/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.9930\n",
      "Epoch 1: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 38s 88ms/step - loss: 0.5032 - accuracy: 0.9930 - val_loss: 2.0343 - val_accuracy: 0.7040 - lr: 0.0100\n",
      "Epoch 2/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.9940\n",
      "Epoch 2: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4990 - accuracy: 0.9940 - val_loss: 2.0365 - val_accuracy: 0.7032 - lr: 0.0100\n",
      "Epoch 3/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4981 - accuracy: 0.9940\n",
      "Epoch 3: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.4981 - accuracy: 0.9940 - val_loss: 2.0393 - val_accuracy: 0.7052 - lr: 0.0100\n",
      "Epoch 4/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4951 - accuracy: 0.9943\n",
      "Epoch 4: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4951 - accuracy: 0.9943 - val_loss: 2.0320 - val_accuracy: 0.7082 - lr: 0.0100\n",
      "Epoch 5/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4953 - accuracy: 0.9940\n",
      "Epoch 5: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4953 - accuracy: 0.9940 - val_loss: 2.0434 - val_accuracy: 0.7048 - lr: 0.0100\n",
      "Epoch 6/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4925 - accuracy: 0.9947\n",
      "Epoch 6: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4925 - accuracy: 0.9947 - val_loss: 2.0365 - val_accuracy: 0.7080 - lr: 0.0100\n",
      "Epoch 7/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4906 - accuracy: 0.9947\n",
      "Epoch 7: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.4906 - accuracy: 0.9947 - val_loss: 2.0436 - val_accuracy: 0.7086 - lr: 0.0100\n",
      "Epoch 8/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4882 - accuracy: 0.9953\n",
      "Epoch 8: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4882 - accuracy: 0.9953 - val_loss: 2.0425 - val_accuracy: 0.7090 - lr: 0.0100\n",
      "Epoch 9/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4880 - accuracy: 0.9953\n",
      "Epoch 9: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4880 - accuracy: 0.9953 - val_loss: 2.0357 - val_accuracy: 0.7070 - lr: 0.0100\n",
      "Epoch 10/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4857 - accuracy: 0.9958\n",
      "Epoch 10: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4857 - accuracy: 0.9958 - val_loss: 2.0386 - val_accuracy: 0.7060 - lr: 0.0100\n",
      "Epoch 11/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.9958\n",
      "Epoch 11: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 29s 82ms/step - loss: 0.4844 - accuracy: 0.9958 - val_loss: 2.0559 - val_accuracy: 0.7050 - lr: 0.0100\n",
      "Epoch 12/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.9957\n",
      "Epoch 12: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4833 - accuracy: 0.9957 - val_loss: 2.0467 - val_accuracy: 0.7018 - lr: 0.0100\n",
      "Epoch 13/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.9950\n",
      "Epoch 13: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4830 - accuracy: 0.9950 - val_loss: 2.0576 - val_accuracy: 0.7040 - lr: 0.0100\n",
      "Epoch 14/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.9950\n",
      "Epoch 14: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4830 - accuracy: 0.9950 - val_loss: 2.0332 - val_accuracy: 0.7050 - lr: 0.0100\n",
      "Epoch 15/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4799 - accuracy: 0.9956\n",
      "Epoch 15: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4799 - accuracy: 0.9956 - val_loss: 2.0444 - val_accuracy: 0.7046 - lr: 0.0100\n",
      "Epoch 16/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4793 - accuracy: 0.9956\n",
      "Epoch 16: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4793 - accuracy: 0.9956 - val_loss: 2.0491 - val_accuracy: 0.7054 - lr: 0.0100\n",
      "Epoch 17/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4783 - accuracy: 0.9956\n",
      "Epoch 17: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4783 - accuracy: 0.9956 - val_loss: 2.0578 - val_accuracy: 0.7084 - lr: 0.0100\n",
      "Epoch 18/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4767 - accuracy: 0.9956\n",
      "Epoch 18: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4767 - accuracy: 0.9956 - val_loss: 2.0438 - val_accuracy: 0.7062 - lr: 0.0100\n",
      "Epoch 19/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4756 - accuracy: 0.9961\n",
      "Epoch 19: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4756 - accuracy: 0.9961 - val_loss: 2.0421 - val_accuracy: 0.7038 - lr: 0.0100\n",
      "Epoch 20/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4750 - accuracy: 0.9957\n",
      "Epoch 20: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4750 - accuracy: 0.9957 - val_loss: 2.0476 - val_accuracy: 0.7038 - lr: 0.0100\n",
      "Epoch 21/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4735 - accuracy: 0.9959\n",
      "Epoch 21: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 29s 82ms/step - loss: 0.4735 - accuracy: 0.9959 - val_loss: 2.0429 - val_accuracy: 0.7068 - lr: 0.0100\n",
      "Epoch 22/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4732 - accuracy: 0.9958\n",
      "Epoch 22: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4732 - accuracy: 0.9958 - val_loss: 2.0401 - val_accuracy: 0.7064 - lr: 0.0100\n",
      "Epoch 23/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.9961\n",
      "Epoch 23: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4720 - accuracy: 0.9961 - val_loss: 2.0592 - val_accuracy: 0.7026 - lr: 0.0100\n",
      "Epoch 24/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.9956\n",
      "Epoch 24: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4720 - accuracy: 0.9956 - val_loss: 2.0168 - val_accuracy: 0.7068 - lr: 0.0100\n",
      "Epoch 25/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.9957\n",
      "Epoch 25: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4712 - accuracy: 0.9957 - val_loss: 2.0362 - val_accuracy: 0.7044 - lr: 0.0100\n",
      "Epoch 26/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4699 - accuracy: 0.9956\n",
      "Epoch 26: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4699 - accuracy: 0.9956 - val_loss: 2.0313 - val_accuracy: 0.7062 - lr: 0.0100\n",
      "Epoch 27/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4687 - accuracy: 0.9962\n",
      "Epoch 27: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4687 - accuracy: 0.9962 - val_loss: 2.0431 - val_accuracy: 0.7070 - lr: 0.0100\n",
      "Epoch 28/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4683 - accuracy: 0.9960\n",
      "Epoch 28: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4683 - accuracy: 0.9960 - val_loss: 2.0371 - val_accuracy: 0.7046 - lr: 0.0100\n",
      "Epoch 29/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4678 - accuracy: 0.9953\n",
      "Epoch 29: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4678 - accuracy: 0.9953 - val_loss: 2.0322 - val_accuracy: 0.7054 - lr: 0.0100\n",
      "Epoch 30/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4665 - accuracy: 0.9959\n",
      "Epoch 30: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4665 - accuracy: 0.9959 - val_loss: 2.0453 - val_accuracy: 0.7026 - lr: 0.0100\n",
      "Epoch 31/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4655 - accuracy: 0.9959\n",
      "Epoch 31: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 29s 82ms/step - loss: 0.4655 - accuracy: 0.9959 - val_loss: 2.0426 - val_accuracy: 0.7060 - lr: 0.0010\n",
      "Epoch 32/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.9963\n",
      "Epoch 32: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.4643 - accuracy: 0.9963 - val_loss: 2.0422 - val_accuracy: 0.7062 - lr: 0.0010\n",
      "Epoch 33/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4639 - accuracy: 0.9967\n",
      "Epoch 33: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4639 - accuracy: 0.9967 - val_loss: 2.0396 - val_accuracy: 0.7056 - lr: 0.0010\n",
      "Epoch 34/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4638 - accuracy: 0.9962\n",
      "Epoch 34: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4638 - accuracy: 0.9962 - val_loss: 2.0384 - val_accuracy: 0.7056 - lr: 0.0010\n",
      "Epoch 35/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4638 - accuracy: 0.9965\n",
      "Epoch 35: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4638 - accuracy: 0.9965 - val_loss: 2.0397 - val_accuracy: 0.7054 - lr: 0.0010\n",
      "Epoch 36/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.9963\n",
      "Epoch 36: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4634 - accuracy: 0.9963 - val_loss: 2.0432 - val_accuracy: 0.7062 - lr: 0.0010\n",
      "Epoch 37/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.9967\n",
      "Epoch 37: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4624 - accuracy: 0.9967 - val_loss: 2.0401 - val_accuracy: 0.7064 - lr: 0.0010\n",
      "Epoch 38/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.9961\n",
      "Epoch 38: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4635 - accuracy: 0.9961 - val_loss: 2.0386 - val_accuracy: 0.7048 - lr: 0.0010\n",
      "Epoch 39/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4629 - accuracy: 0.9966\n",
      "Epoch 39: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4629 - accuracy: 0.9966 - val_loss: 2.0399 - val_accuracy: 0.7066 - lr: 0.0010\n",
      "Epoch 40/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4627 - accuracy: 0.9963\n",
      "Epoch 40: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4627 - accuracy: 0.9963 - val_loss: 2.0413 - val_accuracy: 0.7054 - lr: 0.0010\n",
      "Epoch 41/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.9964\n",
      "Epoch 41: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.4626 - accuracy: 0.9964 - val_loss: 2.0408 - val_accuracy: 0.7054 - lr: 0.0010\n",
      "Epoch 42/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4617 - accuracy: 0.9969\n",
      "Epoch 42: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.4617 - accuracy: 0.9969 - val_loss: 2.0402 - val_accuracy: 0.7054 - lr: 0.0010\n",
      "Epoch 43/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.9967\n",
      "Epoch 43: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4613 - accuracy: 0.9967 - val_loss: 2.0395 - val_accuracy: 0.7056 - lr: 0.0010\n",
      "Epoch 44/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4622 - accuracy: 0.9964\n",
      "Epoch 44: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4622 - accuracy: 0.9964 - val_loss: 2.0398 - val_accuracy: 0.7056 - lr: 0.0010\n",
      "Epoch 45/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.9965\n",
      "Epoch 45: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4624 - accuracy: 0.9965 - val_loss: 2.0356 - val_accuracy: 0.7066 - lr: 0.0010\n",
      "Epoch 46/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4622 - accuracy: 0.9969\n",
      "Epoch 46: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4622 - accuracy: 0.9969 - val_loss: 2.0378 - val_accuracy: 0.7050 - lr: 0.0010\n",
      "Epoch 47/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4615 - accuracy: 0.9967\n",
      "Epoch 47: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4615 - accuracy: 0.9967 - val_loss: 2.0334 - val_accuracy: 0.7066 - lr: 0.0010\n",
      "Epoch 48/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4616 - accuracy: 0.9966\n",
      "Epoch 48: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4616 - accuracy: 0.9966 - val_loss: 2.0369 - val_accuracy: 0.7048 - lr: 0.0010\n",
      "Epoch 49/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.9966\n",
      "Epoch 49: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.4613 - accuracy: 0.9966 - val_loss: 2.0353 - val_accuracy: 0.7054 - lr: 0.0010\n",
      "Epoch 50/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4609 - accuracy: 0.9969\n",
      "Epoch 50: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4609 - accuracy: 0.9969 - val_loss: 2.0401 - val_accuracy: 0.7076 - lr: 0.0010\n",
      "Epoch 51/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4611 - accuracy: 0.9968\n",
      "Epoch 51: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 29s 81ms/step - loss: 0.4611 - accuracy: 0.9968 - val_loss: 2.0393 - val_accuracy: 0.7052 - lr: 0.0010\n",
      "Epoch 52/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.9971\n",
      "Epoch 52: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4602 - accuracy: 0.9971 - val_loss: 2.0346 - val_accuracy: 0.7072 - lr: 0.0010\n",
      "Epoch 53/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4616 - accuracy: 0.9962\n",
      "Epoch 53: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 29s 81ms/step - loss: 0.4616 - accuracy: 0.9962 - val_loss: 2.0381 - val_accuracy: 0.7062 - lr: 0.0010\n",
      "Epoch 54/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.9969\n",
      "Epoch 54: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4610 - accuracy: 0.9969 - val_loss: 2.0393 - val_accuracy: 0.7072 - lr: 0.0010\n",
      "Epoch 55/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4615 - accuracy: 0.9961\n",
      "Epoch 55: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4615 - accuracy: 0.9961 - val_loss: 2.0358 - val_accuracy: 0.7068 - lr: 0.0010\n",
      "Epoch 56/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4598 - accuracy: 0.9971\n",
      "Epoch 56: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4598 - accuracy: 0.9971 - val_loss: 2.0361 - val_accuracy: 0.7078 - lr: 0.0010\n",
      "Epoch 57/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.9969\n",
      "Epoch 57: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4603 - accuracy: 0.9969 - val_loss: 2.0376 - val_accuracy: 0.7086 - lr: 0.0010\n",
      "Epoch 58/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.9962\n",
      "Epoch 58: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4612 - accuracy: 0.9962 - val_loss: 2.0377 - val_accuracy: 0.7070 - lr: 0.0010\n",
      "Epoch 59/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4606 - accuracy: 0.9967\n",
      "Epoch 59: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4606 - accuracy: 0.9967 - val_loss: 2.0402 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 60/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4615 - accuracy: 0.9964\n",
      "Epoch 60: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.4615 - accuracy: 0.9964 - val_loss: 2.0400 - val_accuracy: 0.7076 - lr: 0.0010\n",
      "Epoch 61/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4606 - accuracy: 0.9967\n",
      "Epoch 61: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 29s 82ms/step - loss: 0.4606 - accuracy: 0.9967 - val_loss: 2.0360 - val_accuracy: 0.7082 - lr: 0.0010\n",
      "Epoch 62/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.9966\n",
      "Epoch 62: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4603 - accuracy: 0.9966 - val_loss: 2.0351 - val_accuracy: 0.7086 - lr: 0.0010\n",
      "Epoch 63/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.9971\n",
      "Epoch 63: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4599 - accuracy: 0.9971 - val_loss: 2.0350 - val_accuracy: 0.7080 - lr: 0.0010\n",
      "Epoch 64/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.9963\n",
      "Epoch 64: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4603 - accuracy: 0.9963 - val_loss: 2.0356 - val_accuracy: 0.7092 - lr: 0.0010\n",
      "Epoch 65/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4592 - accuracy: 0.9969\n",
      "Epoch 65: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4592 - accuracy: 0.9969 - val_loss: 2.0363 - val_accuracy: 0.7088 - lr: 0.0010\n",
      "Epoch 66/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4594 - accuracy: 0.9970\n",
      "Epoch 66: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4594 - accuracy: 0.9970 - val_loss: 2.0371 - val_accuracy: 0.7076 - lr: 0.0010\n",
      "Epoch 67/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.9966\n",
      "Epoch 67: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4603 - accuracy: 0.9966 - val_loss: 2.0375 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 68/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4597 - accuracy: 0.9970\n",
      "Epoch 68: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.4597 - accuracy: 0.9970 - val_loss: 2.0388 - val_accuracy: 0.7086 - lr: 0.0010\n",
      "Epoch 69/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4590 - accuracy: 0.9971\n",
      "Epoch 69: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4590 - accuracy: 0.9971 - val_loss: 2.0370 - val_accuracy: 0.7084 - lr: 0.0010\n",
      "Epoch 70/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.9972\n",
      "Epoch 70: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4585 - accuracy: 0.9972 - val_loss: 2.0378 - val_accuracy: 0.7086 - lr: 0.0010\n",
      "Epoch 71/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.9970\n",
      "Epoch 71: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 29s 82ms/step - loss: 0.4589 - accuracy: 0.9970 - val_loss: 2.0390 - val_accuracy: 0.7064 - lr: 0.0010\n",
      "Epoch 72/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.9972\n",
      "Epoch 72: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.4584 - accuracy: 0.9972 - val_loss: 2.0403 - val_accuracy: 0.7088 - lr: 0.0010\n",
      "Current:  347\n",
      "Epoch 1/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.9977\n",
      "Epoch 1: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 37s 87ms/step - loss: 0.4563 - accuracy: 0.9977 - val_loss: 2.0369 - val_accuracy: 0.7080 - lr: 0.0010\n",
      "Epoch 2/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.9973\n",
      "Epoch 2: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.4568 - accuracy: 0.9973 - val_loss: 2.0360 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 3/8\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4558 - accuracy: 0.9976\n",
      "Epoch 3: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.4558 - accuracy: 0.9976 - val_loss: 2.0341 - val_accuracy: 0.7062 - lr: 0.0010\n",
      "Epoch 4/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4560 - accuracy: 0.9979\n",
      "Epoch 4: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.4560 - accuracy: 0.9979 - val_loss: 2.0332 - val_accuracy: 0.7080 - lr: 0.0010\n",
      "Epoch 5/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.9975\n",
      "Epoch 5: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.4559 - accuracy: 0.9975 - val_loss: 2.0343 - val_accuracy: 0.7072 - lr: 0.0010\n",
      "Epoch 6/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.9975\n",
      "Epoch 6: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.4563 - accuracy: 0.9975 - val_loss: 2.0339 - val_accuracy: 0.7060 - lr: 0.0010\n",
      "Epoch 7/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4555 - accuracy: 0.9979\n",
      "Epoch 7: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.4555 - accuracy: 0.9979 - val_loss: 2.0325 - val_accuracy: 0.7072 - lr: 0.0010\n",
      "Epoch 8/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4560 - accuracy: 0.9976\n",
      "Epoch 8: val_loss did not improve from 1.91244\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.4560 - accuracy: 0.9976 - val_loss: 2.0332 - val_accuracy: 0.7070 - lr: 0.0010\n",
      "Current:  378\n",
      "313/313 [==============================] - 6s 14ms/step\n",
      "Accuracy: 69.67999999999999\n",
      "Error: 30.320000000000007\n",
      "ECE: 0.18645092304944993\n",
      "MCE: 0.3648860073455459\n",
      "Loss: 1.7314718625250298\n",
      "brier: 0.28036724916853056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30.320000000000007,\n",
       " 0.18645092304944993,\n",
       " 0.3648860073455459,\n",
       " 1.7314718625250298,\n",
       " 0.28036724916853056]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, datagen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list, batch_size=128,lr_schedule = [[0, 0.1],[80,0.01],[150,0.001]],cbks=[checkpointer], name='resnet_cifar100_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada2265-c65d-4f27-a4f6-dfcb1db538db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
