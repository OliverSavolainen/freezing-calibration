{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b59169-f213-4481-991f-1bac902ae9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import densenet\n",
    "import freezing\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce54fe07-3909-48e5-b8ac-5132910f79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 10\n",
    "nb_epoch = 300\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "\n",
    "img_dim = (img_channels, img_rows, img_cols) if K.image_data_format() == \"channels_first\" else (img_rows, img_cols, img_channels)\n",
    "depth = 40\n",
    "nb_dense_block = 3\n",
    "growth_rate = 12\n",
    "nb_filter = -1\n",
    "dropout_rate = 0.0  # 0.0 for data augmentation\n",
    "seed = 333\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Preprocessing for DenseNet https://arxiv.org/pdf/1608.06993v3.pdf\n",
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41d92e5-da3d-4f60-8264-c8534787da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 09:45:16.092437: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 09:45:17.046832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30961 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"densenet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 24)   648         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 24)  96          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 24)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 12)   2592        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 36)   0           ['conv2d[0][0]',                 \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 36)  144         ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 36)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 12)   3888        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 48)   0           ['concatenate[0][0]',            \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 48)  192         ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 48)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 12)   5184        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 60)   0           ['concatenate_1[0][0]',          \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 60)  240         ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 60)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 12)   6480        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 72)   0           ['concatenate_2[0][0]',          \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 72)  288         ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 72)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 12)   7776        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 84)   0           ['concatenate_3[0][0]',          \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 84)  336         ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 84)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 12)   9072        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 96)   0           ['concatenate_4[0][0]',          \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 96)  384         ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 96)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 12)   10368       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 108)  0           ['concatenate_5[0][0]',          \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 108)  432        ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 108)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 12)   11664       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 32, 32, 120)  0           ['concatenate_6[0][0]',          \n",
      "                                                                  'conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 120)  480        ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 120)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 12)   12960       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 32, 32, 132)  0           ['concatenate_7[0][0]',          \n",
      "                                                                  'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 132)  528        ['concatenate_8[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 132)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 12)   14256       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 144)  0           ['concatenate_8[0][0]',          \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 144)  576        ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 144)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 12)   15552       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 156)  0           ['concatenate_9[0][0]',          \n",
      "                                                                  'conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 156)  624        ['concatenate_10[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 156)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 12)   16848       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 168)  0           ['concatenate_10[0][0]',         \n",
      "                                                                  'conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 168)  672        ['concatenate_11[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 168)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 168)  28224       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 168)  0          ['conv2d_13[0][0]']              \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 168)  672        ['average_pooling2d[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 168)  0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 12)   18144       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 16, 16, 180)  0           ['average_pooling2d[0][0]',      \n",
      "                                                                  'conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 180)  720        ['concatenate_12[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 180)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 12)   19440       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 16, 16, 192)  0           ['concatenate_12[0][0]',         \n",
      "                                                                  'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 192)  768        ['concatenate_13[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 192)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 12)   20736       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 16, 16, 204)  0           ['concatenate_13[0][0]',         \n",
      "                                                                  'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 204)  816        ['concatenate_14[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 16, 204)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 12)   22032       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 16, 16, 216)  0           ['concatenate_14[0][0]',         \n",
      "                                                                  'conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 216)  864        ['concatenate_15[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 16, 16, 216)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 12)   23328       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 228)  0           ['concatenate_15[0][0]',         \n",
      "                                                                  'conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 228)  912        ['concatenate_16[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 16, 16, 228)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 12)   24624       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 16, 16, 240)  0           ['concatenate_16[0][0]',         \n",
      "                                                                  'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 240)  960        ['concatenate_17[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 16, 240)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 12)   25920       ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 16, 16, 252)  0           ['concatenate_17[0][0]',         \n",
      "                                                                  'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 252)  1008       ['concatenate_18[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 16, 16, 252)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 12)   27216       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 16, 16, 264)  0           ['concatenate_18[0][0]',         \n",
      "                                                                  'conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 264)  1056       ['concatenate_19[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 16, 16, 264)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 12)   28512       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 16, 16, 276)  0           ['concatenate_19[0][0]',         \n",
      "                                                                  'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 276)  1104       ['concatenate_20[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 16, 16, 276)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 12)   29808       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 16, 16, 288)  0           ['concatenate_20[0][0]',         \n",
      "                                                                  'conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 288)  1152       ['concatenate_21[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 288)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 12)   31104       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 16, 16, 300)  0           ['concatenate_21[0][0]',         \n",
      "                                                                  'conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 300)  1200       ['concatenate_22[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 300)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 12)   32400       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 16, 16, 312)  0           ['concatenate_22[0][0]',         \n",
      "                                                                  'conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 312)  1248       ['concatenate_23[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 312)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 312)  97344       ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 8, 8, 312)   0           ['conv2d_26[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 312)   1248        ['average_pooling2d_1[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 8, 8, 312)    0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 8, 8, 12)     33696       ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 8, 8, 324)    0           ['average_pooling2d_1[0][0]',    \n",
      "                                                                  'conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 8, 8, 324)   1296        ['concatenate_24[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 8, 8, 324)    0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 8, 8, 12)     34992       ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 8, 8, 336)    0           ['concatenate_24[0][0]',         \n",
      "                                                                  'conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 8, 8, 336)   1344        ['concatenate_25[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 8, 8, 336)    0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 8, 8, 12)     36288       ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 8, 8, 348)    0           ['concatenate_25[0][0]',         \n",
      "                                                                  'conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 8, 8, 348)   1392        ['concatenate_26[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 8, 8, 348)    0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 12)     37584       ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 8, 8, 360)    0           ['concatenate_26[0][0]',         \n",
      "                                                                  'conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 360)   1440        ['concatenate_27[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 8, 8, 360)    0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 12)     38880       ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 8, 8, 372)    0           ['concatenate_27[0][0]',         \n",
      "                                                                  'conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 372)   1488        ['concatenate_28[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 8, 8, 372)    0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 12)     40176       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 8, 8, 384)    0           ['concatenate_28[0][0]',         \n",
      "                                                                  'conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 384)   1536        ['concatenate_29[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 12)     41472       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 8, 8, 396)    0           ['concatenate_29[0][0]',         \n",
      "                                                                  'conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 396)   1584        ['concatenate_30[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 396)    0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 12)     42768       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 8, 8, 408)    0           ['concatenate_30[0][0]',         \n",
      "                                                                  'conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 408)   1632        ['concatenate_31[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 408)    0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 12)     44064       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 8, 8, 420)    0           ['concatenate_31[0][0]',         \n",
      "                                                                  'conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 420)   1680        ['concatenate_32[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 420)    0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 12)     45360       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 8, 8, 432)    0           ['concatenate_32[0][0]',         \n",
      "                                                                  'conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 432)   1728        ['concatenate_33[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 432)    0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 12)     46656       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 8, 8, 444)    0           ['concatenate_33[0][0]',         \n",
      "                                                                  'conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 444)   1776        ['concatenate_34[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 444)    0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 12)     47952       ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 8, 8, 456)    0           ['concatenate_34[0][0]',         \n",
      "                                                                  'conv2d_38[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 456)   1824        ['concatenate_35[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 456)    0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 456)         0           ['activation_38[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           4570        ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,078,018\n",
      "Trainable params: 1,059,298\n",
      "Non-trainable params: 18,720\n",
      "__________________________________________________________________________________________________\n",
      "Finished compiling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/space/home/olisavo/.conda/envs/tf_thesis/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = densenet.DenseNet(img_dim, classes=nb_classes, depth=depth, nb_dense_block=nb_dense_block,\n",
    "                          growth_rate=growth_rate, nb_filter=nb_filter, dropout_rate=dropout_rate, weights=None, weight_decay=1e-4)\n",
    "\n",
    "model.summary()\n",
    "sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)  # dampening = 0.9? Should be zero?\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "print(\"Finished compiling\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#For data preprocessing, we normalize the data using the channel means and standard deviations (https://arxiv.org/pdf/1608.06993v3.pdf)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)\n",
    "\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "\n",
    "\n",
    "img_gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.125,  # 0.125*32 = 4 so max padding of 4 pixels, as described in paper.\n",
    "    height_shift_range=0.125,  # first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32×32 images\n",
    "    fill_mode = \"constant\",\n",
    "    cval = 0\n",
    ")\n",
    "\n",
    "y_train45 = to_categorical(y_train45, nb_classes)  # 1-hot vector\n",
    "y_val = to_categorical(y_val, nb_classes)\n",
    "y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "img_gen.fit(x_train45, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d145e55-edc4-4eae-9917-fb221bba0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.8:\n",
    "    freezing_list.append(int(nb_epoch*0.6))\n",
    "freezing_list.append(nb_epoch)\n",
    "checkpointer = ModelCheckpoint('model_dense_c10_best.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a781e5f3-0891-4795-aca0-1dd3a020a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-25 23:44:00           64\n",
      "config.json                                    2023-04-25 23:44:00        68223\n",
      "variables.h5                                   2023-04-25 23:44:01      4703192\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-25 23:44:00           64\n",
      "config.json                                    2023-04-25 23:44:00        68223\n",
      "variables.h5                                   2023-04-25 23:44:00      4703192\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 23:44:10.257176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/703 [============================>.] - ETA: 0s - loss: 1.5424 - accuracy: 0.4765\n",
      "Epoch 1: val_loss improved from inf to 1.38849, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 60s 60ms/step - loss: 1.5419 - accuracy: 0.4768 - val_loss: 1.3885 - val_accuracy: 0.5436 - lr: 0.1000\n",
      "Epoch 2/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0485 - accuracy: 0.6593\n",
      "Epoch 2: val_loss improved from 1.38849 to 0.95965, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 46s 66ms/step - loss: 1.0485 - accuracy: 0.6593 - val_loss: 0.9597 - val_accuracy: 0.7006 - lr: 0.1000\n",
      "Epoch 3/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8552 - accuracy: 0.7284\n",
      "Epoch 3: val_loss did not improve from 0.95965\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.8552 - accuracy: 0.7284 - val_loss: 1.6539 - val_accuracy: 0.5916 - lr: 0.1000\n",
      "Epoch 4/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7392 - accuracy: 0.7704\n",
      "Epoch 4: val_loss improved from 0.95965 to 0.89671, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.7392 - accuracy: 0.7704 - val_loss: 0.8967 - val_accuracy: 0.7288 - lr: 0.1000\n",
      "Epoch 5/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.7980\n",
      "Epoch 5: val_loss improved from 0.89671 to 0.85562, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.6630 - accuracy: 0.7980 - val_loss: 0.8556 - val_accuracy: 0.7328 - lr: 0.1000\n",
      "Epoch 6/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.8141\n",
      "Epoch 6: val_loss improved from 0.85562 to 0.82115, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.6069 - accuracy: 0.8141 - val_loss: 0.8211 - val_accuracy: 0.7560 - lr: 0.1000\n",
      "Epoch 7/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5695 - accuracy: 0.8278\n",
      "Epoch 7: val_loss improved from 0.82115 to 0.75684, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.5695 - accuracy: 0.8278 - val_loss: 0.7568 - val_accuracy: 0.7742 - lr: 0.1000\n",
      "Epoch 8/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.8430\n",
      "Epoch 8: val_loss did not improve from 0.75684\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.5248 - accuracy: 0.8430 - val_loss: 0.9533 - val_accuracy: 0.7130 - lr: 0.1000\n",
      "Epoch 9/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4984 - accuracy: 0.8512\n",
      "Epoch 9: val_loss did not improve from 0.75684\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.4984 - accuracy: 0.8512 - val_loss: 0.9677 - val_accuracy: 0.7424 - lr: 0.1000\n",
      "Epoch 10/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4766 - accuracy: 0.8566\n",
      "Epoch 10: val_loss did not improve from 0.75684\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.4766 - accuracy: 0.8566 - val_loss: 0.8449 - val_accuracy: 0.7498 - lr: 0.1000\n",
      "Epoch 11/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4503 - accuracy: 0.8677\n",
      "Epoch 11: val_loss improved from 0.75684 to 0.58447, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.4503 - accuracy: 0.8677 - val_loss: 0.5845 - val_accuracy: 0.8248 - lr: 0.1000\n",
      "Epoch 12/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4363 - accuracy: 0.8729\n",
      "Epoch 12: val_loss did not improve from 0.58447\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.4363 - accuracy: 0.8729 - val_loss: 0.7575 - val_accuracy: 0.7828 - lr: 0.1000\n",
      "Epoch 13/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4179 - accuracy: 0.8790\n",
      "Epoch 13: val_loss did not improve from 0.58447\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.4179 - accuracy: 0.8790 - val_loss: 0.6104 - val_accuracy: 0.8198 - lr: 0.1000\n",
      "Epoch 14/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.8832\n",
      "Epoch 14: val_loss did not improve from 0.58447\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.4055 - accuracy: 0.8832 - val_loss: 0.7130 - val_accuracy: 0.8014 - lr: 0.1000\n",
      "Epoch 15/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.8873\n",
      "Epoch 15: val_loss did not improve from 0.58447\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.3898 - accuracy: 0.8873 - val_loss: 0.5893 - val_accuracy: 0.8276 - lr: 0.1000\n",
      "Epoch 16/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3747 - accuracy: 0.8932\n",
      "Epoch 16: val_loss did not improve from 0.58447\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.3747 - accuracy: 0.8932 - val_loss: 0.6567 - val_accuracy: 0.8252 - lr: 0.1000\n",
      "Epoch 17/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3659 - accuracy: 0.8955\n",
      "Epoch 17: val_loss did not improve from 0.58447\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.3659 - accuracy: 0.8955 - val_loss: 0.5916 - val_accuracy: 0.8384 - lr: 0.1000\n",
      "Epoch 18/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3555 - accuracy: 0.8988\n",
      "Epoch 18: val_loss improved from 0.58447 to 0.49207, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.3555 - accuracy: 0.8988 - val_loss: 0.4921 - val_accuracy: 0.8620 - lr: 0.1000\n",
      "Epoch 19/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.9045\n",
      "Epoch 19: val_loss did not improve from 0.49207\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.3413 - accuracy: 0.9045 - val_loss: 0.5869 - val_accuracy: 0.8388 - lr: 0.1000\n",
      "Epoch 20/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.9072\n",
      "Epoch 20: val_loss did not improve from 0.49207\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.3348 - accuracy: 0.9072 - val_loss: 0.5948 - val_accuracy: 0.8454 - lr: 0.1000\n",
      "Epoch 21/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.9104\n",
      "Epoch 21: val_loss did not improve from 0.49207\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.3281 - accuracy: 0.9104 - val_loss: 0.5466 - val_accuracy: 0.8450 - lr: 0.1000\n",
      "Epoch 22/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.9136\n",
      "Epoch 22: val_loss did not improve from 0.49207\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.3198 - accuracy: 0.9136 - val_loss: 0.5487 - val_accuracy: 0.8520 - lr: 0.1000\n",
      "Epoch 23/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.9143\n",
      "Epoch 23: val_loss did not improve from 0.49207\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.3157 - accuracy: 0.9143 - val_loss: 0.5192 - val_accuracy: 0.8664 - lr: 0.1000\n",
      "Epoch 24/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3016 - accuracy: 0.9184\n",
      "Epoch 24: val_loss did not improve from 0.49207\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.3016 - accuracy: 0.9184 - val_loss: 0.6913 - val_accuracy: 0.8294 - lr: 0.1000\n",
      "Epoch 25/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.9198\n",
      "Epoch 25: val_loss did not improve from 0.49207\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.2980 - accuracy: 0.9198 - val_loss: 0.5125 - val_accuracy: 0.8744 - lr: 0.1000\n",
      "Epoch 26/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.9234\n",
      "Epoch 26: val_loss improved from 0.49207 to 0.46845, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.2882 - accuracy: 0.9234 - val_loss: 0.4684 - val_accuracy: 0.8806 - lr: 0.1000\n",
      "Epoch 27/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.9236\n",
      "Epoch 27: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.2871 - accuracy: 0.9236 - val_loss: 0.5901 - val_accuracy: 0.8434 - lr: 0.1000\n",
      "Epoch 28/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.9272\n",
      "Epoch 28: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.2827 - accuracy: 0.9272 - val_loss: 0.6017 - val_accuracy: 0.8414 - lr: 0.1000\n",
      "Epoch 29/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9275\n",
      "Epoch 29: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.2779 - accuracy: 0.9275 - val_loss: 0.4701 - val_accuracy: 0.8706 - lr: 0.1000\n",
      "Epoch 30/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9296\n",
      "Epoch 30: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.2749 - accuracy: 0.9296 - val_loss: 0.5994 - val_accuracy: 0.8504 - lr: 0.1000\n",
      "Epoch 31/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.9286\n",
      "Epoch 31: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.2762 - accuracy: 0.9286 - val_loss: 0.5621 - val_accuracy: 0.8596 - lr: 0.1000\n",
      "Epoch 32/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.2590 - accuracy: 0.9352\n",
      "Epoch 32: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.2589 - accuracy: 0.9352 - val_loss: 0.5500 - val_accuracy: 0.8566 - lr: 0.1000\n",
      "Epoch 33/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.9331\n",
      "Epoch 33: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.2629 - accuracy: 0.9331 - val_loss: 0.5154 - val_accuracy: 0.8730 - lr: 0.1000\n",
      "Epoch 34/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9361\n",
      "Epoch 34: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.2543 - accuracy: 0.9361 - val_loss: 0.4766 - val_accuracy: 0.8792 - lr: 0.1000\n",
      "Epoch 35/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9381\n",
      "Epoch 35: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.2495 - accuracy: 0.9381 - val_loss: 0.5910 - val_accuracy: 0.8542 - lr: 0.1000\n",
      "Epoch 36/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.9399\n",
      "Epoch 36: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.2442 - accuracy: 0.9399 - val_loss: 0.6038 - val_accuracy: 0.8574 - lr: 0.1000\n",
      "Epoch 37/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.9405\n",
      "Epoch 37: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.2423 - accuracy: 0.9405 - val_loss: 0.4955 - val_accuracy: 0.8764 - lr: 0.1000\n",
      "Epoch 38/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.9401\n",
      "Epoch 38: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.2422 - accuracy: 0.9401 - val_loss: 0.7396 - val_accuracy: 0.8352 - lr: 0.1000\n",
      "Epoch 39/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.9428\n",
      "Epoch 39: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.2352 - accuracy: 0.9428 - val_loss: 0.5329 - val_accuracy: 0.8656 - lr: 0.1000\n",
      "Epoch 40/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.9441\n",
      "Epoch 40: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.2298 - accuracy: 0.9441 - val_loss: 0.4746 - val_accuracy: 0.8806 - lr: 0.1000\n",
      "Epoch 41/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 0.9429\n",
      "Epoch 41: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.2314 - accuracy: 0.9429 - val_loss: 0.5478 - val_accuracy: 0.8706 - lr: 0.1000\n",
      "Epoch 42/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9482\n",
      "Epoch 42: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.2253 - accuracy: 0.9482 - val_loss: 0.4932 - val_accuracy: 0.8864 - lr: 0.1000\n",
      "Epoch 43/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9476\n",
      "Epoch 43: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.2227 - accuracy: 0.9476 - val_loss: 0.4924 - val_accuracy: 0.8842 - lr: 0.1000\n",
      "Epoch 44/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9468\n",
      "Epoch 44: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.2252 - accuracy: 0.9468 - val_loss: 0.5719 - val_accuracy: 0.8610 - lr: 0.1000\n",
      "Epoch 45/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.9456\n",
      "Epoch 45: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.2294 - accuracy: 0.9456 - val_loss: 0.6252 - val_accuracy: 0.8428 - lr: 0.1000\n",
      "Epoch 46/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.9496\n",
      "Epoch 46: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.2159 - accuracy: 0.9496 - val_loss: 0.5050 - val_accuracy: 0.8860 - lr: 0.1000\n",
      "Epoch 47/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9518\n",
      "Epoch 47: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.2097 - accuracy: 0.9517 - val_loss: 0.5169 - val_accuracy: 0.8764 - lr: 0.1000\n",
      "Epoch 48/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9516\n",
      "Epoch 48: val_loss did not improve from 0.46845\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.2103 - accuracy: 0.9516 - val_loss: 0.5578 - val_accuracy: 0.8766 - lr: 0.1000\n",
      "Epoch 49/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9523\n",
      "Epoch 49: val_loss improved from 0.46845 to 0.45646, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.2102 - accuracy: 0.9523 - val_loss: 0.4565 - val_accuracy: 0.8912 - lr: 0.1000\n",
      "Epoch 50/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9546\n",
      "Epoch 50: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1992 - accuracy: 0.9546 - val_loss: 0.5159 - val_accuracy: 0.8806 - lr: 0.1000\n",
      "Epoch 51/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9515\n",
      "Epoch 51: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.2110 - accuracy: 0.9515 - val_loss: 0.4813 - val_accuracy: 0.8854 - lr: 0.1000\n",
      "Epoch 52/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2040 - accuracy: 0.9537\n",
      "Epoch 52: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.2040 - accuracy: 0.9537 - val_loss: 0.9032 - val_accuracy: 0.8130 - lr: 0.1000\n",
      "Epoch 53/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.9544\n",
      "Epoch 53: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.2043 - accuracy: 0.9544 - val_loss: 0.7418 - val_accuracy: 0.8394 - lr: 0.1000\n",
      "Epoch 54/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1922 - accuracy: 0.9584\n",
      "Epoch 54: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.1922 - accuracy: 0.9584 - val_loss: 0.6233 - val_accuracy: 0.8614 - lr: 0.1000\n",
      "Epoch 55/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9544\n",
      "Epoch 55: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.1996 - accuracy: 0.9544 - val_loss: 0.5693 - val_accuracy: 0.8666 - lr: 0.1000\n",
      "Epoch 56/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.9589\n",
      "Epoch 56: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1904 - accuracy: 0.9589 - val_loss: 0.7862 - val_accuracy: 0.8410 - lr: 0.1000\n",
      "Epoch 57/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9556\n",
      "Epoch 57: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.2009 - accuracy: 0.9556 - val_loss: 0.5157 - val_accuracy: 0.8796 - lr: 0.1000\n",
      "Epoch 58/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.9592\n",
      "Epoch 58: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1911 - accuracy: 0.9592 - val_loss: 0.6131 - val_accuracy: 0.8734 - lr: 0.1000\n",
      "Epoch 59/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9575\n",
      "Epoch 59: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1941 - accuracy: 0.9575 - val_loss: 0.5862 - val_accuracy: 0.8736 - lr: 0.1000\n",
      "Epoch 60/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9595\n",
      "Epoch 60: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.1878 - accuracy: 0.9595 - val_loss: 0.5380 - val_accuracy: 0.8904 - lr: 0.1000\n",
      "Epoch 61/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9601\n",
      "Epoch 61: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1851 - accuracy: 0.9601 - val_loss: 0.5306 - val_accuracy: 0.8836 - lr: 0.1000\n",
      "Epoch 62/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1824 - accuracy: 0.9611\n",
      "Epoch 62: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1824 - accuracy: 0.9611 - val_loss: 0.5747 - val_accuracy: 0.8782 - lr: 0.1000\n",
      "Epoch 63/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9600\n",
      "Epoch 63: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.1872 - accuracy: 0.9600 - val_loss: 0.6920 - val_accuracy: 0.8594 - lr: 0.1000\n",
      "Epoch 64/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9661\n",
      "Epoch 64: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1701 - accuracy: 0.9661 - val_loss: 0.5087 - val_accuracy: 0.8868 - lr: 0.1000\n",
      "Epoch 65/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.9614\n",
      "Epoch 65: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1812 - accuracy: 0.9614 - val_loss: 0.4766 - val_accuracy: 0.8886 - lr: 0.1000\n",
      "Epoch 66/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9657\n",
      "Epoch 66: val_loss did not improve from 0.45646\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1702 - accuracy: 0.9657 - val_loss: 0.5109 - val_accuracy: 0.8922 - lr: 0.1000\n",
      "Epoch 67/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9626\n",
      "Epoch 67: val_loss improved from 0.45646 to 0.44735, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1769 - accuracy: 0.9626 - val_loss: 0.4473 - val_accuracy: 0.9000 - lr: 0.1000\n",
      "Epoch 68/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.9574\n",
      "Epoch 68: val_loss did not improve from 0.44735\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1948 - accuracy: 0.9574 - val_loss: 0.4599 - val_accuracy: 0.8988 - lr: 0.1000\n",
      "Epoch 69/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9656\n",
      "Epoch 69: val_loss did not improve from 0.44735\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1732 - accuracy: 0.9656 - val_loss: 0.5295 - val_accuracy: 0.8894 - lr: 0.1000\n",
      "Epoch 70/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.9643\n",
      "Epoch 70: val_loss did not improve from 0.44735\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1758 - accuracy: 0.9643 - val_loss: 0.4716 - val_accuracy: 0.8986 - lr: 0.1000\n",
      "Epoch 71/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9665\n",
      "Epoch 71: val_loss did not improve from 0.44735\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1682 - accuracy: 0.9665 - val_loss: 0.4761 - val_accuracy: 0.8950 - lr: 0.1000\n",
      "Epoch 72/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9638\n",
      "Epoch 72: val_loss improved from 0.44735 to 0.39255, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1774 - accuracy: 0.9638 - val_loss: 0.3926 - val_accuracy: 0.9028 - lr: 0.1000\n",
      "Epoch 73/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1663 - accuracy: 0.9664\n",
      "Epoch 73: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1663 - accuracy: 0.9664 - val_loss: 0.6658 - val_accuracy: 0.8594 - lr: 0.1000\n",
      "Epoch 74/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9663\n",
      "Epoch 74: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.1669 - accuracy: 0.9663 - val_loss: 0.5010 - val_accuracy: 0.8986 - lr: 0.1000\n",
      "Epoch 75/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.9669\n",
      "Epoch 75: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1650 - accuracy: 0.9669 - val_loss: 0.5515 - val_accuracy: 0.8834 - lr: 0.1000\n",
      "Epoch 76/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9678\n",
      "Epoch 76: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1639 - accuracy: 0.9678 - val_loss: 0.4933 - val_accuracy: 0.8934 - lr: 0.1000\n",
      "Epoch 77/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.9666\n",
      "Epoch 77: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1651 - accuracy: 0.9666 - val_loss: 0.6329 - val_accuracy: 0.8810 - lr: 0.1000\n",
      "Epoch 78/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9694\n",
      "Epoch 78: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1581 - accuracy: 0.9694 - val_loss: 0.3950 - val_accuracy: 0.9104 - lr: 0.1000\n",
      "Epoch 79/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9681\n",
      "Epoch 79: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1601 - accuracy: 0.9681 - val_loss: 0.6078 - val_accuracy: 0.8798 - lr: 0.1000\n",
      "Epoch 80/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9658\n",
      "Epoch 80: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1697 - accuracy: 0.9658 - val_loss: 0.5355 - val_accuracy: 0.8964 - lr: 0.1000\n",
      "Epoch 81/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9693\n",
      "Epoch 81: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1590 - accuracy: 0.9693 - val_loss: 0.5485 - val_accuracy: 0.8948 - lr: 0.1000\n",
      "Epoch 82/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9683\n",
      "Epoch 82: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1634 - accuracy: 0.9683 - val_loss: 0.4770 - val_accuracy: 0.8962 - lr: 0.1000\n",
      "Epoch 83/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1582 - accuracy: 0.9692\n",
      "Epoch 83: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1582 - accuracy: 0.9692 - val_loss: 0.5038 - val_accuracy: 0.8948 - lr: 0.1000\n",
      "Epoch 84/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.9709\n",
      "Epoch 84: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1568 - accuracy: 0.9709 - val_loss: 0.5456 - val_accuracy: 0.8900 - lr: 0.1000\n",
      "Epoch 85/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9673\n",
      "Epoch 85: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.1670 - accuracy: 0.9673 - val_loss: 0.4747 - val_accuracy: 0.8994 - lr: 0.1000\n",
      "Epoch 86/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9709\n",
      "Epoch 86: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1546 - accuracy: 0.9709 - val_loss: 0.5153 - val_accuracy: 0.8916 - lr: 0.1000\n",
      "Epoch 87/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9725\n",
      "Epoch 87: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1490 - accuracy: 0.9725 - val_loss: 0.4697 - val_accuracy: 0.9020 - lr: 0.1000\n",
      "Epoch 88/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9724\n",
      "Epoch 88: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1474 - accuracy: 0.9724 - val_loss: 0.5138 - val_accuracy: 0.8936 - lr: 0.1000\n",
      "Epoch 89/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9689\n",
      "Epoch 89: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1570 - accuracy: 0.9689 - val_loss: 0.5150 - val_accuracy: 0.8924 - lr: 0.1000\n",
      "Epoch 90/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1478 - accuracy: 0.9727\n",
      "Epoch 90: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1478 - accuracy: 0.9727 - val_loss: 0.5333 - val_accuracy: 0.8938 - lr: 0.1000\n",
      "Epoch 91/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9735\n",
      "Epoch 91: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1422 - accuracy: 0.9735 - val_loss: 0.6203 - val_accuracy: 0.8826 - lr: 0.1000\n",
      "Epoch 92/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9693\n",
      "Epoch 92: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.1573 - accuracy: 0.9693 - val_loss: 0.4761 - val_accuracy: 0.9066 - lr: 0.1000\n",
      "Epoch 93/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9708\n",
      "Epoch 93: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1543 - accuracy: 0.9708 - val_loss: 0.5500 - val_accuracy: 0.8872 - lr: 0.1000\n",
      "Epoch 94/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.9693\n",
      "Epoch 94: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1592 - accuracy: 0.9693 - val_loss: 0.5661 - val_accuracy: 0.8902 - lr: 0.1000\n",
      "Epoch 95/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9741\n",
      "Epoch 95: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1438 - accuracy: 0.9741 - val_loss: 0.4710 - val_accuracy: 0.9002 - lr: 0.1000\n",
      "Epoch 96/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1468 - accuracy: 0.9730\n",
      "Epoch 96: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1469 - accuracy: 0.9730 - val_loss: 0.5375 - val_accuracy: 0.8898 - lr: 0.1000\n",
      "Epoch 97/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9750\n",
      "Epoch 97: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1400 - accuracy: 0.9750 - val_loss: 0.5032 - val_accuracy: 0.8902 - lr: 0.1000\n",
      "Epoch 98/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9751\n",
      "Epoch 98: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1405 - accuracy: 0.9751 - val_loss: 0.5862 - val_accuracy: 0.8822 - lr: 0.1000\n",
      "Epoch 99/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9736\n",
      "Epoch 99: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1455 - accuracy: 0.9736 - val_loss: 0.4654 - val_accuracy: 0.9036 - lr: 0.1000\n",
      "Epoch 100/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9756\n",
      "Epoch 100: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1363 - accuracy: 0.9756 - val_loss: 0.4713 - val_accuracy: 0.9042 - lr: 0.1000\n",
      "Epoch 101/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 0.9721\n",
      "Epoch 101: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1471 - accuracy: 0.9721 - val_loss: 0.4339 - val_accuracy: 0.9070 - lr: 0.1000\n",
      "Epoch 102/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9749\n",
      "Epoch 102: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1391 - accuracy: 0.9749 - val_loss: 0.5807 - val_accuracy: 0.8854 - lr: 0.1000\n",
      "Epoch 103/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9724\n",
      "Epoch 103: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1476 - accuracy: 0.9724 - val_loss: 0.4834 - val_accuracy: 0.9032 - lr: 0.1000\n",
      "Epoch 104/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9756\n",
      "Epoch 104: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1402 - accuracy: 0.9756 - val_loss: 0.5906 - val_accuracy: 0.8898 - lr: 0.1000\n",
      "Epoch 105/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9768\n",
      "Epoch 105: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1364 - accuracy: 0.9768 - val_loss: 0.4475 - val_accuracy: 0.9056 - lr: 0.1000\n",
      "Epoch 106/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9736\n",
      "Epoch 106: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1453 - accuracy: 0.9736 - val_loss: 0.5046 - val_accuracy: 0.9006 - lr: 0.1000\n",
      "Epoch 107/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9745\n",
      "Epoch 107: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1440 - accuracy: 0.9745 - val_loss: 0.4802 - val_accuracy: 0.9028 - lr: 0.1000\n",
      "Epoch 108/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9771\n",
      "Epoch 108: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1345 - accuracy: 0.9771 - val_loss: 0.6455 - val_accuracy: 0.8806 - lr: 0.1000\n",
      "Epoch 109/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9759\n",
      "Epoch 109: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1370 - accuracy: 0.9759 - val_loss: 0.5535 - val_accuracy: 0.8962 - lr: 0.1000\n",
      "Epoch 110/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9757\n",
      "Epoch 110: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1359 - accuracy: 0.9757 - val_loss: 0.7077 - val_accuracy: 0.8742 - lr: 0.1000\n",
      "Epoch 111/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.9774\n",
      "Epoch 111: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1298 - accuracy: 0.9774 - val_loss: 0.6676 - val_accuracy: 0.8866 - lr: 0.1000\n",
      "Epoch 112/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9754\n",
      "Epoch 112: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1365 - accuracy: 0.9754 - val_loss: 0.5410 - val_accuracy: 0.8798 - lr: 0.1000\n",
      "Epoch 113/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9735\n",
      "Epoch 113: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1439 - accuracy: 0.9735 - val_loss: 0.5362 - val_accuracy: 0.8910 - lr: 0.1000\n",
      "Epoch 114/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9769\n",
      "Epoch 114: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1339 - accuracy: 0.9769 - val_loss: 0.6214 - val_accuracy: 0.8784 - lr: 0.1000\n",
      "Epoch 115/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9775\n",
      "Epoch 115: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1315 - accuracy: 0.9775 - val_loss: 0.5342 - val_accuracy: 0.8994 - lr: 0.1000\n",
      "Epoch 116/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9766\n",
      "Epoch 116: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1342 - accuracy: 0.9766 - val_loss: 0.5783 - val_accuracy: 0.8864 - lr: 0.1000\n",
      "Epoch 117/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 0.9794\n",
      "Epoch 117: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1248 - accuracy: 0.9794 - val_loss: 0.5230 - val_accuracy: 0.9056 - lr: 0.1000\n",
      "Epoch 118/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.9724\n",
      "Epoch 118: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1460 - accuracy: 0.9724 - val_loss: 0.4910 - val_accuracy: 0.8980 - lr: 0.1000\n",
      "Epoch 119/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.9780\n",
      "Epoch 119: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1325 - accuracy: 0.9780 - val_loss: 0.4907 - val_accuracy: 0.9038 - lr: 0.1000\n",
      "Epoch 120/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9798\n",
      "Epoch 120: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1232 - accuracy: 0.9798 - val_loss: 0.6568 - val_accuracy: 0.8820 - lr: 0.1000\n",
      "Epoch 121/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9749\n",
      "Epoch 121: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1405 - accuracy: 0.9749 - val_loss: 0.4957 - val_accuracy: 0.8952 - lr: 0.1000\n",
      "Epoch 122/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9794\n",
      "Epoch 122: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.1272 - accuracy: 0.9794 - val_loss: 0.5530 - val_accuracy: 0.8942 - lr: 0.1000\n",
      "Epoch 123/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9768\n",
      "Epoch 123: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1319 - accuracy: 0.9768 - val_loss: 0.5618 - val_accuracy: 0.8868 - lr: 0.1000\n",
      "Epoch 124/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9799\n",
      "Epoch 124: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1247 - accuracy: 0.9799 - val_loss: 0.5502 - val_accuracy: 0.8928 - lr: 0.1000\n",
      "Epoch 125/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9788\n",
      "Epoch 125: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1258 - accuracy: 0.9788 - val_loss: 0.5255 - val_accuracy: 0.9010 - lr: 0.1000\n",
      "Epoch 126/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.9771\n",
      "Epoch 126: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1298 - accuracy: 0.9771 - val_loss: 0.4945 - val_accuracy: 0.9022 - lr: 0.1000\n",
      "Epoch 127/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9812\n",
      "Epoch 127: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1194 - accuracy: 0.9812 - val_loss: 0.5363 - val_accuracy: 0.8952 - lr: 0.1000\n",
      "Epoch 128/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9788\n",
      "Epoch 128: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 38s 55ms/step - loss: 0.1250 - accuracy: 0.9788 - val_loss: 0.6400 - val_accuracy: 0.8838 - lr: 0.1000\n",
      "Epoch 129/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9778\n",
      "Epoch 129: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1289 - accuracy: 0.9778 - val_loss: 0.5809 - val_accuracy: 0.8866 - lr: 0.1000\n",
      "Epoch 130/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9756\n",
      "Epoch 130: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1358 - accuracy: 0.9756 - val_loss: 0.7762 - val_accuracy: 0.8620 - lr: 0.1000\n",
      "Epoch 131/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9797\n",
      "Epoch 131: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1251 - accuracy: 0.9797 - val_loss: 0.6560 - val_accuracy: 0.8866 - lr: 0.1000\n",
      "Epoch 132/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9771\n",
      "Epoch 132: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1308 - accuracy: 0.9771 - val_loss: 0.5105 - val_accuracy: 0.8994 - lr: 0.1000\n",
      "Epoch 133/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9795\n",
      "Epoch 133: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1254 - accuracy: 0.9795 - val_loss: 0.6207 - val_accuracy: 0.8886 - lr: 0.1000\n",
      "Epoch 134/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9784\n",
      "Epoch 134: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1301 - accuracy: 0.9784 - val_loss: 0.4287 - val_accuracy: 0.9138 - lr: 0.1000\n",
      "Epoch 135/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9787\n",
      "Epoch 135: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1279 - accuracy: 0.9787 - val_loss: 0.6174 - val_accuracy: 0.8810 - lr: 0.1000\n",
      "Epoch 136/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9790\n",
      "Epoch 136: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1273 - accuracy: 0.9790 - val_loss: 0.4883 - val_accuracy: 0.9016 - lr: 0.1000\n",
      "Epoch 137/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9807\n",
      "Epoch 137: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1203 - accuracy: 0.9807 - val_loss: 0.5023 - val_accuracy: 0.8996 - lr: 0.1000\n",
      "Epoch 138/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9797\n",
      "Epoch 138: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1225 - accuracy: 0.9797 - val_loss: 0.5333 - val_accuracy: 0.9006 - lr: 0.1000\n",
      "Epoch 139/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9812\n",
      "Epoch 139: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1179 - accuracy: 0.9812 - val_loss: 0.5772 - val_accuracy: 0.8890 - lr: 0.1000\n",
      "Epoch 140/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9759\n",
      "Epoch 140: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1316 - accuracy: 0.9759 - val_loss: 0.6124 - val_accuracy: 0.8744 - lr: 0.1000\n",
      "Epoch 141/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9763\n",
      "Epoch 141: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1396 - accuracy: 0.9763 - val_loss: 0.5035 - val_accuracy: 0.9044 - lr: 0.1000\n",
      "Epoch 142/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9814\n",
      "Epoch 142: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1223 - accuracy: 0.9814 - val_loss: 0.4946 - val_accuracy: 0.9070 - lr: 0.1000\n",
      "Epoch 143/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9818\n",
      "Epoch 143: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1164 - accuracy: 0.9818 - val_loss: 0.5085 - val_accuracy: 0.8992 - lr: 0.1000\n",
      "Epoch 144/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9804\n",
      "Epoch 144: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1208 - accuracy: 0.9804 - val_loss: 0.4951 - val_accuracy: 0.9088 - lr: 0.1000\n",
      "Epoch 145/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9808\n",
      "Epoch 145: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1227 - accuracy: 0.9808 - val_loss: 0.4323 - val_accuracy: 0.9172 - lr: 0.1000\n",
      "Epoch 146/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9749\n",
      "Epoch 146: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1398 - accuracy: 0.9749 - val_loss: 0.4609 - val_accuracy: 0.9092 - lr: 0.1000\n",
      "Epoch 147/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9834\n",
      "Epoch 147: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1166 - accuracy: 0.9834 - val_loss: 0.4420 - val_accuracy: 0.9134 - lr: 0.1000\n",
      "Epoch 148/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9817\n",
      "Epoch 148: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1164 - accuracy: 0.9817 - val_loss: 0.7505 - val_accuracy: 0.8754 - lr: 0.1000\n",
      "Epoch 149/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.9775\n",
      "Epoch 149: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1327 - accuracy: 0.9775 - val_loss: 0.5602 - val_accuracy: 0.8944 - lr: 0.1000\n",
      "Epoch 150/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9819\n",
      "Epoch 150: val_loss did not improve from 0.39255\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1158 - accuracy: 0.9819 - val_loss: 0.5582 - val_accuracy: 0.8954 - lr: 0.1000\n",
      "Epoch 151/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9916\n",
      "Epoch 151: val_loss improved from 0.39255 to 0.37046, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.0896 - accuracy: 0.9916 - val_loss: 0.3705 - val_accuracy: 0.9280 - lr: 0.0100\n",
      "Epoch 152/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9948\n",
      "Epoch 152: val_loss improved from 0.37046 to 0.35451, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0799 - accuracy: 0.9948 - val_loss: 0.3545 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 153/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9956\n",
      "Epoch 153: val_loss did not improve from 0.35451\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0750 - accuracy: 0.9956 - val_loss: 0.3572 - val_accuracy: 0.9322 - lr: 0.0100\n",
      "Epoch 154/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9967\n",
      "Epoch 154: val_loss improved from 0.35451 to 0.35331, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.0709 - accuracy: 0.9967 - val_loss: 0.3533 - val_accuracy: 0.9336 - lr: 0.0100\n",
      "Epoch 155/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9968\n",
      "Epoch 155: val_loss did not improve from 0.35331\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0683 - accuracy: 0.9968 - val_loss: 0.3540 - val_accuracy: 0.9332 - lr: 0.0100\n",
      "Epoch 156/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9971\n",
      "Epoch 156: val_loss did not improve from 0.35331\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.0659 - accuracy: 0.9970 - val_loss: 0.3547 - val_accuracy: 0.9328 - lr: 0.0100\n",
      "Epoch 157/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9978\n",
      "Epoch 157: val_loss improved from 0.35331 to 0.35309, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0631 - accuracy: 0.9978 - val_loss: 0.3531 - val_accuracy: 0.9350 - lr: 0.0100\n",
      "Epoch 158/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9976\n",
      "Epoch 158: val_loss improved from 0.35309 to 0.35191, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0617 - accuracy: 0.9976 - val_loss: 0.3519 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 159/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9976\n",
      "Epoch 159: val_loss improved from 0.35191 to 0.34821, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0597 - accuracy: 0.9976 - val_loss: 0.3482 - val_accuracy: 0.9344 - lr: 0.0100\n",
      "Epoch 160/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9981\n",
      "Epoch 160: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0578 - accuracy: 0.9981 - val_loss: 0.3509 - val_accuracy: 0.9342 - lr: 0.0100\n",
      "Epoch 161/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9980\n",
      "Epoch 161: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.0564 - accuracy: 0.9980 - val_loss: 0.3484 - val_accuracy: 0.9344 - lr: 0.0100\n",
      "Epoch 162/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9980\n",
      "Epoch 162: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0547 - accuracy: 0.9980 - val_loss: 0.3527 - val_accuracy: 0.9352 - lr: 0.0100\n",
      "Epoch 163/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9983\n",
      "Epoch 163: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0530 - accuracy: 0.9983 - val_loss: 0.3504 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 164/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9978\n",
      "Epoch 164: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.0524 - accuracy: 0.9978 - val_loss: 0.3536 - val_accuracy: 0.9336 - lr: 0.0100\n",
      "Epoch 165/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9983\n",
      "Epoch 165: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.0503 - accuracy: 0.9983 - val_loss: 0.3531 - val_accuracy: 0.9356 - lr: 0.0100\n",
      "Epoch 166/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9986\n",
      "Epoch 166: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0485 - accuracy: 0.9986 - val_loss: 0.3500 - val_accuracy: 0.9352 - lr: 0.0100\n",
      "Epoch 167/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9985\n",
      "Epoch 167: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0472 - accuracy: 0.9985 - val_loss: 0.3500 - val_accuracy: 0.9372 - lr: 0.0100\n",
      "Epoch 168/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9988\n",
      "Epoch 168: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0459 - accuracy: 0.9988 - val_loss: 0.3504 - val_accuracy: 0.9378 - lr: 0.0100\n",
      "Epoch 169/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 0.9985\n",
      "Epoch 169: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0449 - accuracy: 0.9985 - val_loss: 0.3484 - val_accuracy: 0.9382 - lr: 0.0100\n",
      "Epoch 170/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9986\n",
      "Epoch 170: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 39s 55ms/step - loss: 0.0434 - accuracy: 0.9986 - val_loss: 0.3484 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 171/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9983\n",
      "Epoch 171: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.0435 - accuracy: 0.9983 - val_loss: 0.3518 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 172/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9986\n",
      "Epoch 172: val_loss did not improve from 0.34821\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0414 - accuracy: 0.9986 - val_loss: 0.3489 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 173/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9989\n",
      "Epoch 173: val_loss improved from 0.34821 to 0.34792, saving model to model_dense_c10_best.hdf5\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0403 - accuracy: 0.9989 - val_loss: 0.3479 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 174/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9990\n",
      "Epoch 174: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0386 - accuracy: 0.9990 - val_loss: 0.3504 - val_accuracy: 0.9356 - lr: 0.0100\n",
      "Epoch 175/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9987\n",
      "Epoch 175: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0381 - accuracy: 0.9987 - val_loss: 0.3481 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 176/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9985\n",
      "Epoch 176: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0375 - accuracy: 0.9985 - val_loss: 0.3538 - val_accuracy: 0.9352 - lr: 0.0100\n",
      "Epoch 177/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9989\n",
      "Epoch 177: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0363 - accuracy: 0.9989 - val_loss: 0.3528 - val_accuracy: 0.9354 - lr: 0.0100\n",
      "Epoch 178/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9988\n",
      "Epoch 178: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0356 - accuracy: 0.9988 - val_loss: 0.3553 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 179/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9987\n",
      "Epoch 179: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0345 - accuracy: 0.9987 - val_loss: 0.3541 - val_accuracy: 0.9354 - lr: 0.0100\n",
      "Epoch 180/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9991\n",
      "Epoch 180: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0334 - accuracy: 0.9991 - val_loss: 0.3530 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 1/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9993\n",
      "Epoch 1: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 35s 45ms/step - loss: 0.0324 - accuracy: 0.9993 - val_loss: 0.3524 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 2/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9993\n",
      "Epoch 2: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0322 - accuracy: 0.9993 - val_loss: 0.3530 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 3/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9992\n",
      "Epoch 3: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0321 - accuracy: 0.9992 - val_loss: 0.3537 - val_accuracy: 0.9372 - lr: 0.0100\n",
      "Epoch 4/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0324 - accuracy: 0.9991\n",
      "Epoch 4: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0324 - accuracy: 0.9991 - val_loss: 0.3544 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 5/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0321 - accuracy: 0.9993\n",
      "Epoch 5: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0321 - accuracy: 0.9993 - val_loss: 0.3557 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 6/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9993\n",
      "Epoch 6: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0321 - accuracy: 0.9993 - val_loss: 0.3559 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 7/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0321 - accuracy: 0.9993\n",
      "Epoch 7: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0321 - accuracy: 0.9993 - val_loss: 0.3575 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 8/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9994\n",
      "Epoch 8: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0318 - accuracy: 0.9994 - val_loss: 0.3582 - val_accuracy: 0.9368 - lr: 0.0100\n",
      "Epoch 9/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9994\n",
      "Epoch 9: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0319 - accuracy: 0.9994 - val_loss: 0.3567 - val_accuracy: 0.9372 - lr: 0.0100\n",
      "Epoch 10/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9994\n",
      "Epoch 10: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0318 - accuracy: 0.9994 - val_loss: 0.3582 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 11/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9992\n",
      "Epoch 11: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0321 - accuracy: 0.9992 - val_loss: 0.3588 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 12/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9992\n",
      "Epoch 12: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 43ms/step - loss: 0.0321 - accuracy: 0.9992 - val_loss: 0.3597 - val_accuracy: 0.9368 - lr: 0.0100\n",
      "Epoch 13/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9993\n",
      "Epoch 13: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0319 - accuracy: 0.9993 - val_loss: 0.3605 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 14/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9991\n",
      "Epoch 14: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0323 - accuracy: 0.9991 - val_loss: 0.3617 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 15/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0318 - accuracy: 0.9992\n",
      "Epoch 15: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0318 - accuracy: 0.9992 - val_loss: 0.3613 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 16/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9994\n",
      "Epoch 16: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0320 - accuracy: 0.9994 - val_loss: 0.3619 - val_accuracy: 0.9372 - lr: 0.0100\n",
      "Epoch 17/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9993\n",
      "Epoch 17: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0319 - accuracy: 0.9993 - val_loss: 0.3631 - val_accuracy: 0.9368 - lr: 0.0100\n",
      "Epoch 18/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0318 - accuracy: 0.9993\n",
      "Epoch 18: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0318 - accuracy: 0.9993 - val_loss: 0.3627 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 19/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9992\n",
      "Epoch 19: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0320 - accuracy: 0.9992 - val_loss: 0.3619 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 20/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9992\n",
      "Epoch 20: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0319 - accuracy: 0.9992 - val_loss: 0.3652 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 21/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9990\n",
      "Epoch 21: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0324 - accuracy: 0.9990 - val_loss: 0.3642 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 22/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0324 - accuracy: 0.9989\n",
      "Epoch 22: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0324 - accuracy: 0.9989 - val_loss: 0.3667 - val_accuracy: 0.9376 - lr: 0.0100\n",
      "Epoch 23/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9992\n",
      "Epoch 23: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0320 - accuracy: 0.9992 - val_loss: 0.3682 - val_accuracy: 0.9368 - lr: 0.0100\n",
      "Epoch 24/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0319 - accuracy: 0.9993\n",
      "Epoch 24: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0319 - accuracy: 0.9993 - val_loss: 0.3681 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 25/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9993\n",
      "Epoch 25: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0320 - accuracy: 0.9993 - val_loss: 0.3679 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 26/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9992\n",
      "Epoch 26: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0320 - accuracy: 0.9992 - val_loss: 0.3679 - val_accuracy: 0.9368 - lr: 0.0100\n",
      "Epoch 27/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9993\n",
      "Epoch 27: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0319 - accuracy: 0.9993 - val_loss: 0.3703 - val_accuracy: 0.9368 - lr: 0.0100\n",
      "Epoch 28/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0319 - accuracy: 0.9991\n",
      "Epoch 28: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0319 - accuracy: 0.9991 - val_loss: 0.3700 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 29/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9992\n",
      "Epoch 29: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0316 - accuracy: 0.9992 - val_loss: 0.3694 - val_accuracy: 0.9356 - lr: 0.0100\n",
      "Epoch 30/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9992\n",
      "Epoch 30: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0320 - accuracy: 0.9992 - val_loss: 0.3741 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 31/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0321 - accuracy: 0.9992\n",
      "Epoch 31: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0321 - accuracy: 0.9992 - val_loss: 0.3694 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 32/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9993\n",
      "Epoch 32: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0316 - accuracy: 0.9993 - val_loss: 0.3706 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 33/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9995\n",
      "Epoch 33: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0312 - accuracy: 0.9995 - val_loss: 0.3698 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 34/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9992\n",
      "Epoch 34: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0317 - accuracy: 0.9992 - val_loss: 0.3705 - val_accuracy: 0.9372 - lr: 0.0100\n",
      "Epoch 35/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9994\n",
      "Epoch 35: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0318 - accuracy: 0.9994 - val_loss: 0.3727 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 36/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9993\n",
      "Epoch 36: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0317 - accuracy: 0.9993 - val_loss: 0.3735 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 37/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9991\n",
      "Epoch 37: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0318 - accuracy: 0.9991 - val_loss: 0.3741 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 38/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9994\n",
      "Epoch 38: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 43ms/step - loss: 0.0317 - accuracy: 0.9994 - val_loss: 0.3753 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 39/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9994\n",
      "Epoch 39: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0314 - accuracy: 0.9994 - val_loss: 0.3750 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 40/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9992\n",
      "Epoch 40: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0317 - accuracy: 0.9992 - val_loss: 0.3760 - val_accuracy: 0.9356 - lr: 0.0100\n",
      "Epoch 41/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9995\n",
      "Epoch 41: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0313 - accuracy: 0.9995 - val_loss: 0.3758 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 42/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9994\n",
      "Epoch 42: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0315 - accuracy: 0.9994 - val_loss: 0.3775 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 43/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9995\n",
      "Epoch 43: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0316 - accuracy: 0.9995 - val_loss: 0.3778 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 44/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9993\n",
      "Epoch 44: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0314 - accuracy: 0.9993 - val_loss: 0.3791 - val_accuracy: 0.9354 - lr: 0.0100\n",
      "Epoch 45/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9992\n",
      "Epoch 45: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0319 - accuracy: 0.9992 - val_loss: 0.3791 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 46/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 46: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3796 - val_accuracy: 0.9356 - lr: 0.0010\n",
      "Epoch 47/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9994\n",
      "Epoch 47: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0314 - accuracy: 0.9994 - val_loss: 0.3786 - val_accuracy: 0.9356 - lr: 0.0010\n",
      "Epoch 48/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9993\n",
      "Epoch 48: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0315 - accuracy: 0.9993 - val_loss: 0.3790 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 49/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 49: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3789 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 50/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9995\n",
      "Epoch 50: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0312 - accuracy: 0.9995 - val_loss: 0.3795 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 51/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9993\n",
      "Epoch 51: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0316 - accuracy: 0.9993 - val_loss: 0.3798 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 52/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9991\n",
      "Epoch 52: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0317 - accuracy: 0.9991 - val_loss: 0.3803 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 53/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9992\n",
      "Epoch 53: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0315 - accuracy: 0.9992 - val_loss: 0.3794 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 54/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9993\n",
      "Epoch 54: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0315 - accuracy: 0.9993 - val_loss: 0.3805 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 55/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9995\n",
      "Epoch 55: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0312 - accuracy: 0.9995 - val_loss: 0.3791 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 56/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9992\n",
      "Epoch 56: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0315 - accuracy: 0.9992 - val_loss: 0.3798 - val_accuracy: 0.9356 - lr: 0.0010\n",
      "Epoch 57/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0310 - accuracy: 0.9996\n",
      "Epoch 57: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0310 - accuracy: 0.9996 - val_loss: 0.3791 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 58/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9992\n",
      "Epoch 58: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0316 - accuracy: 0.9992 - val_loss: 0.3806 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 59/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 59: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3785 - val_accuracy: 0.9356 - lr: 0.0010\n",
      "Epoch 60/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9993\n",
      "Epoch 60: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0315 - accuracy: 0.9993 - val_loss: 0.3799 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 61/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9994\n",
      "Epoch 61: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.0314 - accuracy: 0.9994 - val_loss: 0.3786 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 62/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 62: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3788 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 63/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0314 - accuracy: 0.9995\n",
      "Epoch 63: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0314 - accuracy: 0.9995 - val_loss: 0.3806 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 64/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9995\n",
      "Epoch 64: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0312 - accuracy: 0.9995 - val_loss: 0.3797 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 65/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9994\n",
      "Epoch 65: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0315 - accuracy: 0.9994 - val_loss: 0.3798 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 66/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9995\n",
      "Epoch 66: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 43ms/step - loss: 0.0312 - accuracy: 0.9995 - val_loss: 0.3798 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 67/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9995\n",
      "Epoch 67: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0311 - accuracy: 0.9995 - val_loss: 0.3794 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 68/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 68: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3801 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 69/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9990\n",
      "Epoch 69: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0320 - accuracy: 0.9990 - val_loss: 0.3797 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 70/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9993\n",
      "Epoch 70: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0316 - accuracy: 0.9993 - val_loss: 0.3792 - val_accuracy: 0.9356 - lr: 0.0010\n",
      "Epoch 71/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0314 - accuracy: 0.9995\n",
      "Epoch 71: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0314 - accuracy: 0.9995 - val_loss: 0.3798 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 72/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9993\n",
      "Epoch 72: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0317 - accuracy: 0.9993 - val_loss: 0.3791 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 73/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9993\n",
      "Epoch 73: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0318 - accuracy: 0.9993 - val_loss: 0.3794 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 74/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0312 - accuracy: 0.9994\n",
      "Epoch 74: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0312 - accuracy: 0.9994 - val_loss: 0.3801 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 75/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0312 - accuracy: 0.9995\n",
      "Epoch 75: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0312 - accuracy: 0.9995 - val_loss: 0.3781 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 76/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9993\n",
      "Epoch 76: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 43ms/step - loss: 0.0315 - accuracy: 0.9993 - val_loss: 0.3794 - val_accuracy: 0.9354 - lr: 0.0010\n",
      "Epoch 77/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9991\n",
      "Epoch 77: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0320 - accuracy: 0.9991 - val_loss: 0.3788 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 78/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9994\n",
      "Epoch 78: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0314 - accuracy: 0.9994 - val_loss: 0.3788 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 79/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9991\n",
      "Epoch 79: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0320 - accuracy: 0.9991 - val_loss: 0.3787 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 80/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9995\n",
      "Epoch 80: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0312 - accuracy: 0.9995 - val_loss: 0.3800 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 81/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9992\n",
      "Epoch 81: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0316 - accuracy: 0.9992 - val_loss: 0.3786 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 82/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9994\n",
      "Epoch 82: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0314 - accuracy: 0.9994 - val_loss: 0.3791 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 83/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 83: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3778 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 84/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9993\n",
      "Epoch 84: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0317 - accuracy: 0.9993 - val_loss: 0.3787 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 85/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9993\n",
      "Epoch 85: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0315 - accuracy: 0.9993 - val_loss: 0.3799 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 86/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 86: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3805 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 87/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9992\n",
      "Epoch 87: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 43ms/step - loss: 0.0316 - accuracy: 0.9992 - val_loss: 0.3793 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 88/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0311 - accuracy: 0.9994\n",
      "Epoch 88: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0311 - accuracy: 0.9994 - val_loss: 0.3812 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 89/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9993\n",
      "Epoch 89: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0315 - accuracy: 0.9993 - val_loss: 0.3797 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 90/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9992\n",
      "Epoch 90: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0319 - accuracy: 0.9992 - val_loss: 0.3807 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 91/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0310 - accuracy: 0.9995\n",
      "Epoch 91: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0310 - accuracy: 0.9995 - val_loss: 0.3803 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 92/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9994\n",
      "Epoch 92: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0314 - accuracy: 0.9994 - val_loss: 0.3799 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 93/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9994\n",
      "Epoch 93: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0316 - accuracy: 0.9994 - val_loss: 0.3805 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 94/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9992\n",
      "Epoch 94: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0317 - accuracy: 0.9992 - val_loss: 0.3799 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 95/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0312 - accuracy: 0.9994\n",
      "Epoch 95: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0312 - accuracy: 0.9994 - val_loss: 0.3793 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 96/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9993\n",
      "Epoch 96: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0314 - accuracy: 0.9993 - val_loss: 0.3797 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 97/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9994\n",
      "Epoch 97: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0316 - accuracy: 0.9994 - val_loss: 0.3802 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 98/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9994\n",
      "Epoch 98: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0317 - accuracy: 0.9994 - val_loss: 0.3794 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 99/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9992\n",
      "Epoch 99: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0315 - accuracy: 0.9992 - val_loss: 0.3807 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 100/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 100: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3800 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 101/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9993\n",
      "Epoch 101: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0313 - accuracy: 0.9993 - val_loss: 0.3808 - val_accuracy: 0.9356 - lr: 0.0010\n",
      "Epoch 102/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9992\n",
      "Epoch 102: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0316 - accuracy: 0.9992 - val_loss: 0.3796 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 103/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9995\n",
      "Epoch 103: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0312 - accuracy: 0.9995 - val_loss: 0.3804 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 104/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9995\n",
      "Epoch 104: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0312 - accuracy: 0.9995 - val_loss: 0.3801 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 105/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 105: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3800 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 106/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9995\n",
      "Epoch 106: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0315 - accuracy: 0.9995 - val_loss: 0.3800 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 107/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9994\n",
      "Epoch 107: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0314 - accuracy: 0.9994 - val_loss: 0.3815 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 108/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 108: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3805 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 109/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9993\n",
      "Epoch 109: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 34s 49ms/step - loss: 0.0314 - accuracy: 0.9993 - val_loss: 0.3813 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 110/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9994\n",
      "Epoch 110: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0315 - accuracy: 0.9994 - val_loss: 0.3808 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 111/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9993\n",
      "Epoch 111: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0315 - accuracy: 0.9993 - val_loss: 0.3808 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 112/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 112: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3794 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 113/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9994\n",
      "Epoch 113: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0314 - accuracy: 0.9994 - val_loss: 0.3816 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 114/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9992\n",
      "Epoch 114: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0316 - accuracy: 0.9992 - val_loss: 0.3802 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 115/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9992\n",
      "Epoch 115: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0315 - accuracy: 0.9992 - val_loss: 0.3799 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 116/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 116: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3815 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 117/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9994\n",
      "Epoch 117: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0314 - accuracy: 0.9994 - val_loss: 0.3802 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 118/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9997\n",
      "Epoch 118: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0310 - accuracy: 0.9997 - val_loss: 0.3811 - val_accuracy: 0.9372 - lr: 0.0010\n",
      "Epoch 119/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9994\n",
      "Epoch 119: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0313 - accuracy: 0.9994 - val_loss: 0.3808 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 120/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0318 - accuracy: 0.9992\n",
      "Epoch 120: val_loss did not improve from 0.34792\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0318 - accuracy: 0.9992 - val_loss: 0.3807 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Current:  126\n",
      "313/313 [==============================] - 4s 9ms/step\n",
      "Accuracy: 92.43\n",
      "Error: 7.569999999999993\n",
      "ECE: 0.055834416005015326\n",
      "MCE: 0.32317792477570184\n",
      "Loss: 0.42384715016430374\n",
      "brier: 0.0664976546995827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.569999999999993,\n",
       " 0.055834416005015326,\n",
       " 0.32317792477570184,\n",
       " 0.42384715016430374,\n",
       " 0.0664976546995827]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, img_gen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list,batch_size=batch_size,lr_schedule = [[0, 0.1],[nb_epoch*0.5,0.01],[nb_epoch*0.75,0.001]],cbks=[checkpointer], name='dense_cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e5ebe-c65b-4b44-9ccb-eb6d2ca51935",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet.DenseNet(img_dim, classes=nb_classes, depth=depth, nb_dense_block=nb_dense_block,\n",
    "                          growth_rate=growth_rate, nb_filter=nb_filter, dropout_rate=dropout_rate, weights=None, weight_decay=1e-4)\n",
    "\n",
    "model.summary()\n",
    "sgd = SGD(lr=0.1, momentum=0.9, nesterov=True) \n",
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "    if i < len(model.layers) * 0.9:\n",
    "        freezing_list.append(int(nb_epoch*0.6))\n",
    "    elif i < len(model.layers) * 0.98:\n",
    "        freezing_list.append(int(nb_epoch*0.96))\n",
    "freezing_list.append(nb_epoch)\n",
    "checkpointer = ModelCheckpoint('model_dense_c10_best_2.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a286849-3648-4cc3-9d0d-941e68fdf4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-26 03:02:48           64\n",
      "config.json                                    2023-04-26 03:02:48        68006\n",
      "variables.h5                                   2023-04-26 03:02:50      4703192\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-26 03:02:48           64\n",
      "config.json                                    2023-04-26 03:02:48        68006\n",
      "variables.h5                                   2023-04-26 03:02:50      4703192\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.6024 - accuracy: 0.4485\n",
      "Epoch 1: val_loss improved from inf to 1.99461, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 49s 63ms/step - loss: 1.6024 - accuracy: 0.4485 - val_loss: 1.9946 - val_accuracy: 0.4584 - lr: 0.1000\n",
      "Epoch 2/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.1101 - accuracy: 0.6353\n",
      "Epoch 2: val_loss improved from 1.99461 to 1.20429, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 1.1101 - accuracy: 0.6353 - val_loss: 1.2043 - val_accuracy: 0.6246 - lr: 0.1000\n",
      "Epoch 3/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9078 - accuracy: 0.7079\n",
      "Epoch 3: val_loss did not improve from 1.20429\n",
      "703/703 [==============================] - 43s 62ms/step - loss: 0.9078 - accuracy: 0.7079 - val_loss: 1.4497 - val_accuracy: 0.6088 - lr: 0.1000\n",
      "Epoch 4/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7753 - accuracy: 0.7559\n",
      "Epoch 4: val_loss improved from 1.20429 to 1.01628, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 44s 63ms/step - loss: 0.7753 - accuracy: 0.7559 - val_loss: 1.0163 - val_accuracy: 0.7140 - lr: 0.1000\n",
      "Epoch 5/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6889 - accuracy: 0.7859\n",
      "Epoch 5: val_loss did not improve from 1.01628\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.6889 - accuracy: 0.7859 - val_loss: 1.0173 - val_accuracy: 0.7072 - lr: 0.1000\n",
      "Epoch 6/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.8093\n",
      "Epoch 6: val_loss improved from 1.01628 to 0.72384, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.6269 - accuracy: 0.8093 - val_loss: 0.7238 - val_accuracy: 0.7736 - lr: 0.1000\n",
      "Epoch 7/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.8246\n",
      "Epoch 7: val_loss did not improve from 0.72384\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.5784 - accuracy: 0.8246 - val_loss: 1.0370 - val_accuracy: 0.6854 - lr: 0.1000\n",
      "Epoch 8/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.8354\n",
      "Epoch 8: val_loss did not improve from 0.72384\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.5405 - accuracy: 0.8354 - val_loss: 0.7319 - val_accuracy: 0.7788 - lr: 0.1000\n",
      "Epoch 9/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5058 - accuracy: 0.8499\n",
      "Epoch 9: val_loss improved from 0.72384 to 0.62636, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.5058 - accuracy: 0.8499 - val_loss: 0.6264 - val_accuracy: 0.8136 - lr: 0.1000\n",
      "Epoch 10/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.4806 - accuracy: 0.8570\n",
      "Epoch 10: val_loss did not improve from 0.62636\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.4806 - accuracy: 0.8570 - val_loss: 0.8408 - val_accuracy: 0.7722 - lr: 0.1000\n",
      "Epoch 11/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.8649\n",
      "Epoch 11: val_loss improved from 0.62636 to 0.54473, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.4599 - accuracy: 0.8649 - val_loss: 0.5447 - val_accuracy: 0.8360 - lr: 0.1000\n",
      "Epoch 12/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.8698\n",
      "Epoch 12: val_loss did not improve from 0.54473\n",
      "703/703 [==============================] - 43s 60ms/step - loss: 0.4453 - accuracy: 0.8698 - val_loss: 0.6631 - val_accuracy: 0.8098 - lr: 0.1000\n",
      "Epoch 13/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.4246 - accuracy: 0.8766\n",
      "Epoch 13: val_loss did not improve from 0.54473\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.4246 - accuracy: 0.8765 - val_loss: 0.7789 - val_accuracy: 0.7716 - lr: 0.1000\n",
      "Epoch 14/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4072 - accuracy: 0.8832\n",
      "Epoch 14: val_loss did not improve from 0.54473\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.4072 - accuracy: 0.8832 - val_loss: 0.6924 - val_accuracy: 0.8062 - lr: 0.1000\n",
      "Epoch 15/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3913 - accuracy: 0.8888\n",
      "Epoch 15: val_loss improved from 0.54473 to 0.51646, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.3913 - accuracy: 0.8888 - val_loss: 0.5165 - val_accuracy: 0.8578 - lr: 0.1000\n",
      "Epoch 16/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.8908\n",
      "Epoch 16: val_loss improved from 0.51646 to 0.44933, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.3832 - accuracy: 0.8908 - val_loss: 0.4493 - val_accuracy: 0.8746 - lr: 0.1000\n",
      "Epoch 17/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3692 - accuracy: 0.8959\n",
      "Epoch 17: val_loss did not improve from 0.44933\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.3692 - accuracy: 0.8959 - val_loss: 0.4642 - val_accuracy: 0.8716 - lr: 0.1000\n",
      "Epoch 18/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3574 - accuracy: 0.9000\n",
      "Epoch 18: val_loss did not improve from 0.44933\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.3574 - accuracy: 0.9000 - val_loss: 0.7116 - val_accuracy: 0.8032 - lr: 0.1000\n",
      "Epoch 19/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.9013\n",
      "Epoch 19: val_loss improved from 0.44933 to 0.44257, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.3525 - accuracy: 0.9013 - val_loss: 0.4426 - val_accuracy: 0.8764 - lr: 0.1000\n",
      "Epoch 20/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.9061\n",
      "Epoch 20: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.3406 - accuracy: 0.9061 - val_loss: 0.7340 - val_accuracy: 0.7952 - lr: 0.1000\n",
      "Epoch 21/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.3278 - accuracy: 0.9095\n",
      "Epoch 21: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.3279 - accuracy: 0.9094 - val_loss: 0.6273 - val_accuracy: 0.8230 - lr: 0.1000\n",
      "Epoch 22/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.9108\n",
      "Epoch 22: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.3261 - accuracy: 0.9108 - val_loss: 0.6451 - val_accuracy: 0.8296 - lr: 0.1000\n",
      "Epoch 23/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9156\n",
      "Epoch 23: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.3127 - accuracy: 0.9156 - val_loss: 0.9119 - val_accuracy: 0.7776 - lr: 0.1000\n",
      "Epoch 24/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.3112 - accuracy: 0.9165\n",
      "Epoch 24: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.3114 - accuracy: 0.9165 - val_loss: 0.4689 - val_accuracy: 0.8764 - lr: 0.1000\n",
      "Epoch 25/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.9196\n",
      "Epoch 25: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.3008 - accuracy: 0.9196 - val_loss: 0.5161 - val_accuracy: 0.8672 - lr: 0.1000\n",
      "Epoch 26/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.9223\n",
      "Epoch 26: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.2973 - accuracy: 0.9223 - val_loss: 0.6926 - val_accuracy: 0.8286 - lr: 0.1000\n",
      "Epoch 27/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.9238\n",
      "Epoch 27: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.2899 - accuracy: 0.9238 - val_loss: 0.5779 - val_accuracy: 0.8502 - lr: 0.1000\n",
      "Epoch 28/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.9262\n",
      "Epoch 28: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.2802 - accuracy: 0.9262 - val_loss: 0.5735 - val_accuracy: 0.8600 - lr: 0.1000\n",
      "Epoch 29/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.9256\n",
      "Epoch 29: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 39s 56ms/step - loss: 0.2835 - accuracy: 0.9256 - val_loss: 0.4685 - val_accuracy: 0.8782 - lr: 0.1000\n",
      "Epoch 30/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.9274\n",
      "Epoch 30: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 43s 60ms/step - loss: 0.2835 - accuracy: 0.9274 - val_loss: 0.5203 - val_accuracy: 0.8658 - lr: 0.1000\n",
      "Epoch 31/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.9309\n",
      "Epoch 31: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.2704 - accuracy: 0.9309 - val_loss: 0.5017 - val_accuracy: 0.8708 - lr: 0.1000\n",
      "Epoch 32/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.9313\n",
      "Epoch 32: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.2649 - accuracy: 0.9313 - val_loss: 0.5269 - val_accuracy: 0.8620 - lr: 0.1000\n",
      "Epoch 33/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9354\n",
      "Epoch 33: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.2576 - accuracy: 0.9354 - val_loss: 1.0466 - val_accuracy: 0.7374 - lr: 0.1000\n",
      "Epoch 34/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.9340\n",
      "Epoch 34: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.2591 - accuracy: 0.9340 - val_loss: 0.6193 - val_accuracy: 0.8510 - lr: 0.1000\n",
      "Epoch 35/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9384\n",
      "Epoch 35: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.2495 - accuracy: 0.9384 - val_loss: 0.5251 - val_accuracy: 0.8728 - lr: 0.1000\n",
      "Epoch 36/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9386\n",
      "Epoch 36: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 44s 62ms/step - loss: 0.2471 - accuracy: 0.9386 - val_loss: 0.6221 - val_accuracy: 0.8580 - lr: 0.1000\n",
      "Epoch 37/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9395\n",
      "Epoch 37: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.2443 - accuracy: 0.9395 - val_loss: 0.5766 - val_accuracy: 0.8572 - lr: 0.1000\n",
      "Epoch 38/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9421\n",
      "Epoch 38: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.2390 - accuracy: 0.9421 - val_loss: 0.5175 - val_accuracy: 0.8734 - lr: 0.1000\n",
      "Epoch 39/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2400 - accuracy: 0.9426\n",
      "Epoch 39: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.2400 - accuracy: 0.9426 - val_loss: 0.5147 - val_accuracy: 0.8746 - lr: 0.1000\n",
      "Epoch 40/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2380 - accuracy: 0.9418\n",
      "Epoch 40: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.2380 - accuracy: 0.9418 - val_loss: 0.5181 - val_accuracy: 0.8756 - lr: 0.1000\n",
      "Epoch 41/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.9426\n",
      "Epoch 41: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.2375 - accuracy: 0.9426 - val_loss: 0.4921 - val_accuracy: 0.8810 - lr: 0.1000\n",
      "Epoch 42/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2292 - accuracy: 0.9460\n",
      "Epoch 42: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.2292 - accuracy: 0.9460 - val_loss: 0.4616 - val_accuracy: 0.8908 - lr: 0.1000\n",
      "Epoch 43/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9468\n",
      "Epoch 43: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.2220 - accuracy: 0.9468 - val_loss: 0.5213 - val_accuracy: 0.8806 - lr: 0.1000\n",
      "Epoch 44/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.9461\n",
      "Epoch 44: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.2255 - accuracy: 0.9461 - val_loss: 0.5180 - val_accuracy: 0.8812 - lr: 0.1000\n",
      "Epoch 45/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2199 - accuracy: 0.9491\n",
      "Epoch 45: val_loss did not improve from 0.44257\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.2199 - accuracy: 0.9491 - val_loss: 0.4643 - val_accuracy: 0.8922 - lr: 0.1000\n",
      "Epoch 46/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9479\n",
      "Epoch 46: val_loss improved from 0.44257 to 0.43814, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.2228 - accuracy: 0.9479 - val_loss: 0.4381 - val_accuracy: 0.8930 - lr: 0.1000\n",
      "Epoch 47/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9494\n",
      "Epoch 47: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 44s 62ms/step - loss: 0.2182 - accuracy: 0.9494 - val_loss: 0.5306 - val_accuracy: 0.8750 - lr: 0.1000\n",
      "Epoch 48/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9507\n",
      "Epoch 48: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.2132 - accuracy: 0.9507 - val_loss: 0.4854 - val_accuracy: 0.8826 - lr: 0.1000\n",
      "Epoch 49/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.9535\n",
      "Epoch 49: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.2077 - accuracy: 0.9535 - val_loss: 0.4582 - val_accuracy: 0.8926 - lr: 0.1000\n",
      "Epoch 50/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9505\n",
      "Epoch 50: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.2118 - accuracy: 0.9505 - val_loss: 0.5025 - val_accuracy: 0.8838 - lr: 0.1000\n",
      "Epoch 51/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.9553\n",
      "Epoch 51: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.2008 - accuracy: 0.9553 - val_loss: 0.5230 - val_accuracy: 0.8796 - lr: 0.1000\n",
      "Epoch 52/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9556\n",
      "Epoch 52: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.2000 - accuracy: 0.9556 - val_loss: 0.4478 - val_accuracy: 0.8910 - lr: 0.1000\n",
      "Epoch 53/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9546\n",
      "Epoch 53: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.2003 - accuracy: 0.9546 - val_loss: 0.5059 - val_accuracy: 0.8880 - lr: 0.1000\n",
      "Epoch 54/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.9571\n",
      "Epoch 54: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1946 - accuracy: 0.9571 - val_loss: 0.6132 - val_accuracy: 0.8662 - lr: 0.1000\n",
      "Epoch 55/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1945 - accuracy: 0.9572\n",
      "Epoch 55: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1945 - accuracy: 0.9572 - val_loss: 0.5027 - val_accuracy: 0.8910 - lr: 0.1000\n",
      "Epoch 56/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9561\n",
      "Epoch 56: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1973 - accuracy: 0.9561 - val_loss: 0.5360 - val_accuracy: 0.8808 - lr: 0.1000\n",
      "Epoch 57/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9567\n",
      "Epoch 57: val_loss did not improve from 0.43814\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1951 - accuracy: 0.9567 - val_loss: 0.5713 - val_accuracy: 0.8716 - lr: 0.1000\n",
      "Epoch 58/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9575\n",
      "Epoch 58: val_loss improved from 0.43814 to 0.41735, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1950 - accuracy: 0.9575 - val_loss: 0.4173 - val_accuracy: 0.9078 - lr: 0.1000\n",
      "Epoch 59/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1893 - accuracy: 0.9586\n",
      "Epoch 59: val_loss did not improve from 0.41735\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1893 - accuracy: 0.9586 - val_loss: 0.4509 - val_accuracy: 0.8914 - lr: 0.1000\n",
      "Epoch 60/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1834 - accuracy: 0.9604\n",
      "Epoch 60: val_loss improved from 0.41735 to 0.40212, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1834 - accuracy: 0.9604 - val_loss: 0.4021 - val_accuracy: 0.9132 - lr: 0.1000\n",
      "Epoch 61/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9594\n",
      "Epoch 61: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1868 - accuracy: 0.9594 - val_loss: 0.5488 - val_accuracy: 0.8832 - lr: 0.1000\n",
      "Epoch 62/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9605\n",
      "Epoch 62: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1838 - accuracy: 0.9605 - val_loss: 0.4798 - val_accuracy: 0.8964 - lr: 0.1000\n",
      "Epoch 63/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1835 - accuracy: 0.9611\n",
      "Epoch 63: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1835 - accuracy: 0.9611 - val_loss: 1.0722 - val_accuracy: 0.7970 - lr: 0.1000\n",
      "Epoch 64/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9605\n",
      "Epoch 64: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1876 - accuracy: 0.9605 - val_loss: 0.5331 - val_accuracy: 0.8812 - lr: 0.1000\n",
      "Epoch 65/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9640\n",
      "Epoch 65: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1770 - accuracy: 0.9640 - val_loss: 0.6053 - val_accuracy: 0.8786 - lr: 0.1000\n",
      "Epoch 66/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9604\n",
      "Epoch 66: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1865 - accuracy: 0.9604 - val_loss: 0.5038 - val_accuracy: 0.8842 - lr: 0.1000\n",
      "Epoch 67/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9653\n",
      "Epoch 67: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1720 - accuracy: 0.9653 - val_loss: 0.5241 - val_accuracy: 0.8824 - lr: 0.1000\n",
      "Epoch 68/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9604\n",
      "Epoch 68: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1869 - accuracy: 0.9604 - val_loss: 0.5453 - val_accuracy: 0.8796 - lr: 0.1000\n",
      "Epoch 69/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9662\n",
      "Epoch 69: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1711 - accuracy: 0.9662 - val_loss: 0.6099 - val_accuracy: 0.8858 - lr: 0.1000\n",
      "Epoch 70/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9650\n",
      "Epoch 70: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 60ms/step - loss: 0.1715 - accuracy: 0.9650 - val_loss: 0.5917 - val_accuracy: 0.8832 - lr: 0.1000\n",
      "Epoch 71/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9626\n",
      "Epoch 71: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1797 - accuracy: 0.9626 - val_loss: 0.4809 - val_accuracy: 0.8994 - lr: 0.1000\n",
      "Epoch 72/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9670\n",
      "Epoch 72: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1680 - accuracy: 0.9670 - val_loss: 0.5017 - val_accuracy: 0.8938 - lr: 0.1000\n",
      "Epoch 73/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1825 - accuracy: 0.9612\n",
      "Epoch 73: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1825 - accuracy: 0.9612 - val_loss: 0.4787 - val_accuracy: 0.8962 - lr: 0.1000\n",
      "Epoch 74/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1643 - accuracy: 0.9679\n",
      "Epoch 74: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1643 - accuracy: 0.9679 - val_loss: 0.5338 - val_accuracy: 0.8822 - lr: 0.1000\n",
      "Epoch 75/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9660\n",
      "Epoch 75: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1683 - accuracy: 0.9660 - val_loss: 0.5712 - val_accuracy: 0.8808 - lr: 0.1000\n",
      "Epoch 76/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9657\n",
      "Epoch 76: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1684 - accuracy: 0.9657 - val_loss: 0.5470 - val_accuracy: 0.8888 - lr: 0.1000\n",
      "Epoch 77/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1640 - accuracy: 0.9684\n",
      "Epoch 77: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1640 - accuracy: 0.9684 - val_loss: 0.4261 - val_accuracy: 0.9100 - lr: 0.1000\n",
      "Epoch 78/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9682\n",
      "Epoch 78: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1613 - accuracy: 0.9682 - val_loss: 0.4378 - val_accuracy: 0.9060 - lr: 0.1000\n",
      "Epoch 79/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9667\n",
      "Epoch 79: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1679 - accuracy: 0.9667 - val_loss: 0.7402 - val_accuracy: 0.8522 - lr: 0.1000\n",
      "Epoch 80/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9667\n",
      "Epoch 80: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1703 - accuracy: 0.9667 - val_loss: 0.4875 - val_accuracy: 0.8884 - lr: 0.1000\n",
      "Epoch 81/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9698\n",
      "Epoch 81: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1586 - accuracy: 0.9698 - val_loss: 0.5626 - val_accuracy: 0.8884 - lr: 0.1000\n",
      "Epoch 82/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9696\n",
      "Epoch 82: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1581 - accuracy: 0.9696 - val_loss: 0.6326 - val_accuracy: 0.8792 - lr: 0.1000\n",
      "Epoch 83/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1681 - accuracy: 0.9666\n",
      "Epoch 83: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1681 - accuracy: 0.9666 - val_loss: 0.4486 - val_accuracy: 0.9028 - lr: 0.1000\n",
      "Epoch 84/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9670\n",
      "Epoch 84: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1669 - accuracy: 0.9670 - val_loss: 0.7233 - val_accuracy: 0.8582 - lr: 0.1000\n",
      "Epoch 85/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9688\n",
      "Epoch 85: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1590 - accuracy: 0.9688 - val_loss: 0.5353 - val_accuracy: 0.8920 - lr: 0.1000\n",
      "Epoch 86/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1545 - accuracy: 0.9714\n",
      "Epoch 86: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1545 - accuracy: 0.9714 - val_loss: 0.4286 - val_accuracy: 0.9042 - lr: 0.1000\n",
      "Epoch 87/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9702\n",
      "Epoch 87: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 62ms/step - loss: 0.1554 - accuracy: 0.9702 - val_loss: 0.5358 - val_accuracy: 0.8892 - lr: 0.1000\n",
      "Epoch 88/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9716\n",
      "Epoch 88: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1505 - accuracy: 0.9716 - val_loss: 0.5146 - val_accuracy: 0.8936 - lr: 0.1000\n",
      "Epoch 89/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1546 - accuracy: 0.9697\n",
      "Epoch 89: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1546 - accuracy: 0.9697 - val_loss: 0.6316 - val_accuracy: 0.8758 - lr: 0.1000\n",
      "Epoch 90/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1541 - accuracy: 0.9706\n",
      "Epoch 90: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1541 - accuracy: 0.9706 - val_loss: 0.4693 - val_accuracy: 0.9000 - lr: 0.1000\n",
      "Epoch 91/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9706\n",
      "Epoch 91: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1534 - accuracy: 0.9706 - val_loss: 0.5195 - val_accuracy: 0.8908 - lr: 0.1000\n",
      "Epoch 92/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 0.9723\n",
      "Epoch 92: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1471 - accuracy: 0.9723 - val_loss: 0.5221 - val_accuracy: 0.8988 - lr: 0.1000\n",
      "Epoch 93/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9730\n",
      "Epoch 93: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1451 - accuracy: 0.9730 - val_loss: 0.4981 - val_accuracy: 0.9052 - lr: 0.1000\n",
      "Epoch 94/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.9699\n",
      "Epoch 94: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1549 - accuracy: 0.9699 - val_loss: 0.5430 - val_accuracy: 0.8922 - lr: 0.1000\n",
      "Epoch 95/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1535 - accuracy: 0.9709\n",
      "Epoch 95: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1535 - accuracy: 0.9709 - val_loss: 0.4339 - val_accuracy: 0.9060 - lr: 0.1000\n",
      "Epoch 96/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9737\n",
      "Epoch 96: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1461 - accuracy: 0.9737 - val_loss: 0.5625 - val_accuracy: 0.8898 - lr: 0.1000\n",
      "Epoch 97/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9735\n",
      "Epoch 97: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1452 - accuracy: 0.9735 - val_loss: 0.4745 - val_accuracy: 0.9052 - lr: 0.1000\n",
      "Epoch 98/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9710\n",
      "Epoch 98: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 44s 62ms/step - loss: 0.1513 - accuracy: 0.9710 - val_loss: 0.6006 - val_accuracy: 0.8820 - lr: 0.1000\n",
      "Epoch 99/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9737\n",
      "Epoch 99: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 60ms/step - loss: 0.1440 - accuracy: 0.9737 - val_loss: 0.4580 - val_accuracy: 0.9052 - lr: 0.1000\n",
      "Epoch 100/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1429 - accuracy: 0.9744\n",
      "Epoch 100: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1429 - accuracy: 0.9744 - val_loss: 0.4556 - val_accuracy: 0.9074 - lr: 0.1000\n",
      "Epoch 101/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9735\n",
      "Epoch 101: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1417 - accuracy: 0.9735 - val_loss: 0.4977 - val_accuracy: 0.8962 - lr: 0.1000\n",
      "Epoch 102/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9726\n",
      "Epoch 102: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1451 - accuracy: 0.9726 - val_loss: 0.5556 - val_accuracy: 0.8902 - lr: 0.1000\n",
      "Epoch 103/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9736\n",
      "Epoch 103: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1434 - accuracy: 0.9736 - val_loss: 0.5819 - val_accuracy: 0.8880 - lr: 0.1000\n",
      "Epoch 104/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.9753\n",
      "Epoch 104: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1384 - accuracy: 0.9753 - val_loss: 0.5515 - val_accuracy: 0.8928 - lr: 0.1000\n",
      "Epoch 105/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9734\n",
      "Epoch 105: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1416 - accuracy: 0.9734 - val_loss: 0.5194 - val_accuracy: 0.8962 - lr: 0.1000\n",
      "Epoch 106/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9736\n",
      "Epoch 106: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1434 - accuracy: 0.9736 - val_loss: 0.5485 - val_accuracy: 0.8950 - lr: 0.1000\n",
      "Epoch 107/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9754\n",
      "Epoch 107: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1390 - accuracy: 0.9754 - val_loss: 0.4555 - val_accuracy: 0.9100 - lr: 0.1000\n",
      "Epoch 108/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9705\n",
      "Epoch 108: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 60ms/step - loss: 0.1597 - accuracy: 0.9705 - val_loss: 0.5525 - val_accuracy: 0.8902 - lr: 0.1000\n",
      "Epoch 109/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9748\n",
      "Epoch 109: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 60ms/step - loss: 0.1411 - accuracy: 0.9748 - val_loss: 0.5789 - val_accuracy: 0.8852 - lr: 0.1000\n",
      "Epoch 110/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.9755\n",
      "Epoch 110: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1373 - accuracy: 0.9755 - val_loss: 0.4419 - val_accuracy: 0.9042 - lr: 0.1000\n",
      "Epoch 111/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9755\n",
      "Epoch 111: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1340 - accuracy: 0.9755 - val_loss: 0.5905 - val_accuracy: 0.8828 - lr: 0.1000\n",
      "Epoch 112/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1382 - accuracy: 0.9756\n",
      "Epoch 112: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1382 - accuracy: 0.9756 - val_loss: 0.4938 - val_accuracy: 0.9008 - lr: 0.1000\n",
      "Epoch 113/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1329 - accuracy: 0.9772\n",
      "Epoch 113: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1329 - accuracy: 0.9772 - val_loss: 0.4965 - val_accuracy: 0.9030 - lr: 0.1000\n",
      "Epoch 114/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1428 - accuracy: 0.9732\n",
      "Epoch 114: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1428 - accuracy: 0.9732 - val_loss: 0.6160 - val_accuracy: 0.8866 - lr: 0.1000\n",
      "Epoch 115/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9719\n",
      "Epoch 115: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1527 - accuracy: 0.9719 - val_loss: 0.4330 - val_accuracy: 0.9118 - lr: 0.1000\n",
      "Epoch 116/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9761\n",
      "Epoch 116: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1387 - accuracy: 0.9761 - val_loss: 0.4939 - val_accuracy: 0.9012 - lr: 0.1000\n",
      "Epoch 117/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.9773\n",
      "Epoch 117: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1327 - accuracy: 0.9773 - val_loss: 0.4662 - val_accuracy: 0.9134 - lr: 0.1000\n",
      "Epoch 118/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9793\n",
      "Epoch 118: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1251 - accuracy: 0.9793 - val_loss: 0.4467 - val_accuracy: 0.9130 - lr: 0.1000\n",
      "Epoch 119/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9750\n",
      "Epoch 119: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1368 - accuracy: 0.9750 - val_loss: 0.5781 - val_accuracy: 0.8896 - lr: 0.1000\n",
      "Epoch 120/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9761\n",
      "Epoch 120: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1369 - accuracy: 0.9761 - val_loss: 0.4570 - val_accuracy: 0.9068 - lr: 0.1000\n",
      "Epoch 121/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9770\n",
      "Epoch 121: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1316 - accuracy: 0.9770 - val_loss: 0.4812 - val_accuracy: 0.9070 - lr: 0.1000\n",
      "Epoch 122/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9781\n",
      "Epoch 122: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1281 - accuracy: 0.9781 - val_loss: 0.4907 - val_accuracy: 0.9076 - lr: 0.1000\n",
      "Epoch 123/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9787\n",
      "Epoch 123: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1245 - accuracy: 0.9787 - val_loss: 0.5952 - val_accuracy: 0.8870 - lr: 0.1000\n",
      "Epoch 124/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9733\n",
      "Epoch 124: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1451 - accuracy: 0.9733 - val_loss: 0.4904 - val_accuracy: 0.9056 - lr: 0.1000\n",
      "Epoch 125/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9791\n",
      "Epoch 125: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 40s 58ms/step - loss: 0.1283 - accuracy: 0.9791 - val_loss: 0.4770 - val_accuracy: 0.9090 - lr: 0.1000\n",
      "Epoch 126/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9790\n",
      "Epoch 126: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.1239 - accuracy: 0.9790 - val_loss: 0.6311 - val_accuracy: 0.8886 - lr: 0.1000\n",
      "Epoch 127/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9787\n",
      "Epoch 127: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.1259 - accuracy: 0.9787 - val_loss: 0.4736 - val_accuracy: 0.9076 - lr: 0.1000\n",
      "Epoch 128/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9793\n",
      "Epoch 128: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1255 - accuracy: 0.9793 - val_loss: 0.5099 - val_accuracy: 0.9024 - lr: 0.1000\n",
      "Epoch 129/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9802\n",
      "Epoch 129: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.1214 - accuracy: 0.9802 - val_loss: 0.4479 - val_accuracy: 0.9054 - lr: 0.1000\n",
      "Epoch 130/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9778\n",
      "Epoch 130: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1282 - accuracy: 0.9778 - val_loss: 0.4710 - val_accuracy: 0.9080 - lr: 0.1000\n",
      "Epoch 131/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.9797\n",
      "Epoch 131: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.1234 - accuracy: 0.9797 - val_loss: 0.5020 - val_accuracy: 0.9076 - lr: 0.1000\n",
      "Epoch 132/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9802\n",
      "Epoch 132: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1238 - accuracy: 0.9802 - val_loss: 0.5257 - val_accuracy: 0.9004 - lr: 0.1000\n",
      "Epoch 133/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1266 - accuracy: 0.9778\n",
      "Epoch 133: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1266 - accuracy: 0.9778 - val_loss: 0.5107 - val_accuracy: 0.9036 - lr: 0.1000\n",
      "Epoch 134/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9782\n",
      "Epoch 134: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1268 - accuracy: 0.9782 - val_loss: 0.4897 - val_accuracy: 0.9028 - lr: 0.1000\n",
      "Epoch 135/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9806\n",
      "Epoch 135: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1207 - accuracy: 0.9806 - val_loss: 0.5606 - val_accuracy: 0.8970 - lr: 0.1000\n",
      "Epoch 136/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1215 - accuracy: 0.9793\n",
      "Epoch 136: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1215 - accuracy: 0.9793 - val_loss: 0.4804 - val_accuracy: 0.9036 - lr: 0.1000\n",
      "Epoch 137/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9802\n",
      "Epoch 137: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1198 - accuracy: 0.9802 - val_loss: 0.4584 - val_accuracy: 0.9052 - lr: 0.1000\n",
      "Epoch 138/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9803\n",
      "Epoch 138: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1194 - accuracy: 0.9803 - val_loss: 0.4831 - val_accuracy: 0.9070 - lr: 0.1000\n",
      "Epoch 139/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9818\n",
      "Epoch 139: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1147 - accuracy: 0.9818 - val_loss: 0.5987 - val_accuracy: 0.8794 - lr: 0.1000\n",
      "Epoch 140/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9748\n",
      "Epoch 140: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1386 - accuracy: 0.9748 - val_loss: 0.6615 - val_accuracy: 0.8836 - lr: 0.1000\n",
      "Epoch 141/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9801\n",
      "Epoch 141: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1257 - accuracy: 0.9801 - val_loss: 0.4994 - val_accuracy: 0.9038 - lr: 0.1000\n",
      "Epoch 142/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9795\n",
      "Epoch 142: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1249 - accuracy: 0.9795 - val_loss: 0.4255 - val_accuracy: 0.9148 - lr: 0.1000\n",
      "Epoch 143/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9807\n",
      "Epoch 143: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1193 - accuracy: 0.9807 - val_loss: 0.4471 - val_accuracy: 0.9146 - lr: 0.1000\n",
      "Epoch 144/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9804\n",
      "Epoch 144: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1203 - accuracy: 0.9804 - val_loss: 0.5603 - val_accuracy: 0.8952 - lr: 0.1000\n",
      "Epoch 145/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9755\n",
      "Epoch 145: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1391 - accuracy: 0.9755 - val_loss: 0.4516 - val_accuracy: 0.9110 - lr: 0.1000\n",
      "Epoch 146/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9830\n",
      "Epoch 146: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1136 - accuracy: 0.9830 - val_loss: 0.7257 - val_accuracy: 0.8714 - lr: 0.1000\n",
      "Epoch 147/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9780\n",
      "Epoch 147: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.1268 - accuracy: 0.9780 - val_loss: 0.5218 - val_accuracy: 0.9030 - lr: 0.1000\n",
      "Epoch 148/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9819\n",
      "Epoch 148: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1174 - accuracy: 0.9819 - val_loss: 0.5164 - val_accuracy: 0.9038 - lr: 0.1000\n",
      "Epoch 149/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9806\n",
      "Epoch 149: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.1184 - accuracy: 0.9806 - val_loss: 0.5326 - val_accuracy: 0.9076 - lr: 0.1000\n",
      "Epoch 150/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9822\n",
      "Epoch 150: val_loss did not improve from 0.40212\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.1136 - accuracy: 0.9822 - val_loss: 0.5407 - val_accuracy: 0.9028 - lr: 0.1000\n",
      "Epoch 151/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9898\n",
      "Epoch 151: val_loss improved from 0.40212 to 0.38671, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 46s 65ms/step - loss: 0.0914 - accuracy: 0.9898 - val_loss: 0.3867 - val_accuracy: 0.9268 - lr: 0.0100\n",
      "Epoch 152/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9949\n",
      "Epoch 152: val_loss improved from 0.38671 to 0.37010, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0779 - accuracy: 0.9949 - val_loss: 0.3701 - val_accuracy: 0.9332 - lr: 0.0100\n",
      "Epoch 153/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9957\n",
      "Epoch 153: val_loss improved from 0.37010 to 0.36442, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.0732 - accuracy: 0.9957 - val_loss: 0.3644 - val_accuracy: 0.9338 - lr: 0.0100\n",
      "Epoch 154/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9966\n",
      "Epoch 154: val_loss improved from 0.36442 to 0.36141, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.0694 - accuracy: 0.9966 - val_loss: 0.3614 - val_accuracy: 0.9346 - lr: 0.0100\n",
      "Epoch 155/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9970\n",
      "Epoch 155: val_loss did not improve from 0.36141\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0665 - accuracy: 0.9970 - val_loss: 0.3642 - val_accuracy: 0.9342 - lr: 0.0100\n",
      "Epoch 156/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9972\n",
      "Epoch 156: val_loss did not improve from 0.36141\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.0644 - accuracy: 0.9972 - val_loss: 0.3615 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 157/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9978\n",
      "Epoch 157: val_loss improved from 0.36141 to 0.35483, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.0613 - accuracy: 0.9978 - val_loss: 0.3548 - val_accuracy: 0.9336 - lr: 0.0100\n",
      "Epoch 158/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9977\n",
      "Epoch 158: val_loss improved from 0.35483 to 0.35404, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.0604 - accuracy: 0.9977 - val_loss: 0.3540 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 159/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9978\n",
      "Epoch 159: val_loss improved from 0.35404 to 0.35355, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 43s 61ms/step - loss: 0.0575 - accuracy: 0.9978 - val_loss: 0.3536 - val_accuracy: 0.9354 - lr: 0.0100\n",
      "Epoch 160/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9980\n",
      "Epoch 160: val_loss did not improve from 0.35355\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.0562 - accuracy: 0.9980 - val_loss: 0.3558 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 161/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9980\n",
      "Epoch 161: val_loss improved from 0.35355 to 0.34787, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 41s 59ms/step - loss: 0.0549 - accuracy: 0.9980 - val_loss: 0.3479 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 162/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9985\n",
      "Epoch 162: val_loss did not improve from 0.34787\n",
      "703/703 [==============================] - 40s 57ms/step - loss: 0.0529 - accuracy: 0.9985 - val_loss: 0.3558 - val_accuracy: 0.9332 - lr: 0.0100\n",
      "Epoch 163/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9981\n",
      "Epoch 163: val_loss did not improve from 0.34787\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0518 - accuracy: 0.9981 - val_loss: 0.3536 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 164/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9980\n",
      "Epoch 164: val_loss improved from 0.34787 to 0.34593, saving model to model_dense_c10_best_2.hdf5\n",
      "703/703 [==============================] - 42s 60ms/step - loss: 0.0505 - accuracy: 0.9980 - val_loss: 0.3459 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 165/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9983\n",
      "Epoch 165: val_loss did not improve from 0.34593\n",
      "703/703 [==============================] - 42s 59ms/step - loss: 0.0490 - accuracy: 0.9983 - val_loss: 0.3511 - val_accuracy: 0.9342 - lr: 0.0100\n",
      "Epoch 166/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9986\n",
      "Epoch 166: val_loss did not improve from 0.34593\n",
      "703/703 [==============================] - 41s 58ms/step - loss: 0.0472 - accuracy: 0.9986 - val_loss: 0.3496 - val_accuracy: 0.9354 - lr: 0.0100\n",
      "Epoch 167/180\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9988\n",
      "Epoch 167: val_loss did not improve from 0.34593\n",
      "703/703 [==============================] - 40s 56ms/step - loss: 0.0457 - accuracy: 0.9988 - val_loss: 0.3517 - val_accuracy: 0.9330 - lr: 0.0100\n",
      "Epoch 168/180\n",
      "390/703 [===============>..............] - ETA: 17s - loss: 0.0442 - accuracy: 0.9988"
     ]
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, img_gen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list,batch_size=batch_size,lr_schedule = [[0, 0.1],[nb_epoch*0.5,0.01],[nb_epoch*0.75,0.001]],cbks=[checkpointer], name='dense_cifar10_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538f0c94-e29a-42bc-9092-f27985cf01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "    if i < len(model.layers) * 0.9:\n",
    "        freezing_list.append(int(nb_epoch*0.6))\n",
    "    elif i < len(model.layers) * 0.98:\n",
    "        freezing_list.append(int(nb_epoch*0.96))\n",
    "freezing_list.append(nb_epoch)\n",
    "checkpointer = ModelCheckpoint('model_dense_c10_best_2_cont.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738e376c-0046-44e9-a1ae-a4a0b09b0030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-26 10:25:27           64\n",
      "config.json                                    2023-04-26 10:25:27        68396\n",
      "variables.h5                                   2023-04-26 10:25:27      4709440\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-26 10:25:26           64\n",
      "config.json                                    2023-04-26 10:25:26        68396\n",
      "variables.h5                                   2023-04-26 10:25:26      4709440\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...vars\n",
      "Loaded weights\n",
      "Epoch 1/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9954\n",
      "Epoch 1: val_loss improved from 0.42662 to 0.38651, saving model to model_dense_c10_best_2_cont.hdf5\n",
      "703/703 [==============================] - 34s 44ms/step - loss: 0.0768 - accuracy: 0.9954 - val_loss: 0.3865 - val_accuracy: 0.9294 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9954\n",
      "Epoch 2: val_loss improved from 0.38651 to 0.38577, saving model to model_dense_c10_best_2_cont.hdf5\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0765 - accuracy: 0.9954 - val_loss: 0.3858 - val_accuracy: 0.9288 - lr: 0.0100\n",
      "Epoch 3/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9954\n",
      "Epoch 3: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0766 - accuracy: 0.9954 - val_loss: 0.3868 - val_accuracy: 0.9294 - lr: 0.0100\n",
      "Epoch 4/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9955\n",
      "Epoch 4: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 43ms/step - loss: 0.0766 - accuracy: 0.9955 - val_loss: 0.3888 - val_accuracy: 0.9292 - lr: 0.0100\n",
      "Epoch 5/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9957\n",
      "Epoch 5: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0751 - accuracy: 0.9957 - val_loss: 0.3895 - val_accuracy: 0.9286 - lr: 0.0100\n",
      "Epoch 6/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9956\n",
      "Epoch 6: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0758 - accuracy: 0.9956 - val_loss: 0.3889 - val_accuracy: 0.9310 - lr: 0.0100\n",
      "Epoch 7/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9953\n",
      "Epoch 7: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0763 - accuracy: 0.9953 - val_loss: 0.3919 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 8/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9956\n",
      "Epoch 8: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0755 - accuracy: 0.9956 - val_loss: 0.3927 - val_accuracy: 0.9284 - lr: 0.0100\n",
      "Epoch 9/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9957\n",
      "Epoch 9: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0753 - accuracy: 0.9957 - val_loss: 0.3924 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 10/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9959\n",
      "Epoch 10: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0751 - accuracy: 0.9959 - val_loss: 0.3955 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 11/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9957\n",
      "Epoch 11: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0750 - accuracy: 0.9957 - val_loss: 0.3948 - val_accuracy: 0.9302 - lr: 0.0100\n",
      "Epoch 12/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9966\n",
      "Epoch 12: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0732 - accuracy: 0.9966 - val_loss: 0.3967 - val_accuracy: 0.9296 - lr: 0.0100\n",
      "Epoch 13/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9953\n",
      "Epoch 13: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0760 - accuracy: 0.9953 - val_loss: 0.3967 - val_accuracy: 0.9302 - lr: 0.0100\n",
      "Epoch 14/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9962\n",
      "Epoch 14: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0747 - accuracy: 0.9962 - val_loss: 0.3978 - val_accuracy: 0.9312 - lr: 0.0100\n",
      "Epoch 15/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9957\n",
      "Epoch 15: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.0756 - accuracy: 0.9957 - val_loss: 0.3944 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 16/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9957\n",
      "Epoch 16: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0748 - accuracy: 0.9957 - val_loss: 0.4016 - val_accuracy: 0.9294 - lr: 0.0100\n",
      "Epoch 17/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9961\n",
      "Epoch 17: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0744 - accuracy: 0.9961 - val_loss: 0.4007 - val_accuracy: 0.9300 - lr: 0.0100\n",
      "Epoch 18/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9960\n",
      "Epoch 18: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0739 - accuracy: 0.9960 - val_loss: 0.4023 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 19/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9968\n",
      "Epoch 19: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0731 - accuracy: 0.9968 - val_loss: 0.4021 - val_accuracy: 0.9310 - lr: 0.0100\n",
      "Epoch 20/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9966\n",
      "Epoch 20: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0730 - accuracy: 0.9966 - val_loss: 0.4036 - val_accuracy: 0.9288 - lr: 0.0100\n",
      "Epoch 21/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9962\n",
      "Epoch 21: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0737 - accuracy: 0.9962 - val_loss: 0.4024 - val_accuracy: 0.9298 - lr: 0.0100\n",
      "Epoch 22/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9964\n",
      "Epoch 22: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0732 - accuracy: 0.9964 - val_loss: 0.4023 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 23/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9970\n",
      "Epoch 23: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0731 - accuracy: 0.9970 - val_loss: 0.4057 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 24/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9966\n",
      "Epoch 24: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0733 - accuracy: 0.9966 - val_loss: 0.4051 - val_accuracy: 0.9300 - lr: 0.0100\n",
      "Epoch 25/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9958\n",
      "Epoch 25: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0740 - accuracy: 0.9958 - val_loss: 0.4069 - val_accuracy: 0.9310 - lr: 0.0100\n",
      "Epoch 26/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9961\n",
      "Epoch 26: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0739 - accuracy: 0.9960 - val_loss: 0.4050 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 27/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9964\n",
      "Epoch 27: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0733 - accuracy: 0.9964 - val_loss: 0.4069 - val_accuracy: 0.9314 - lr: 0.0100\n",
      "Epoch 28/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9962\n",
      "Epoch 28: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0732 - accuracy: 0.9962 - val_loss: 0.4061 - val_accuracy: 0.9312 - lr: 0.0100\n",
      "Epoch 29/30\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9960\n",
      "Epoch 29: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0734 - accuracy: 0.9960 - val_loss: 0.4051 - val_accuracy: 0.9308 - lr: 0.0100\n",
      "Epoch 30/30\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9963\n",
      "Epoch 30: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0732 - accuracy: 0.9963 - val_loss: 0.4058 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 1/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9964\n",
      "Epoch 1: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 43ms/step - loss: 0.0729 - accuracy: 0.9964 - val_loss: 0.4083 - val_accuracy: 0.9322 - lr: 0.0100\n",
      "Epoch 2/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9962\n",
      "Epoch 2: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0730 - accuracy: 0.9962 - val_loss: 0.4096 - val_accuracy: 0.9312 - lr: 0.0100\n",
      "Epoch 3/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9959\n",
      "Epoch 3: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0737 - accuracy: 0.9959 - val_loss: 0.4113 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 4/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9968\n",
      "Epoch 4: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0728 - accuracy: 0.9968 - val_loss: 0.4110 - val_accuracy: 0.9312 - lr: 0.0100\n",
      "Epoch 5/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9966\n",
      "Epoch 5: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0729 - accuracy: 0.9966 - val_loss: 0.4125 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 6/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9966\n",
      "Epoch 6: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0730 - accuracy: 0.9966 - val_loss: 0.4117 - val_accuracy: 0.9308 - lr: 0.0100\n",
      "Epoch 7/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9971\n",
      "Epoch 7: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0721 - accuracy: 0.9971 - val_loss: 0.4135 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 8/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9966\n",
      "Epoch 8: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0725 - accuracy: 0.9966 - val_loss: 0.4104 - val_accuracy: 0.9314 - lr: 0.0100\n",
      "Epoch 9/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9966\n",
      "Epoch 9: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0725 - accuracy: 0.9966 - val_loss: 0.4114 - val_accuracy: 0.9302 - lr: 0.0100\n",
      "Epoch 10/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9966\n",
      "Epoch 10: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0723 - accuracy: 0.9966 - val_loss: 0.4101 - val_accuracy: 0.9308 - lr: 0.0100\n",
      "Epoch 11/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9962\n",
      "Epoch 11: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0735 - accuracy: 0.9962 - val_loss: 0.4139 - val_accuracy: 0.9308 - lr: 0.0100\n",
      "Epoch 12/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9964\n",
      "Epoch 12: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0723 - accuracy: 0.9964 - val_loss: 0.4144 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 13/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9964\n",
      "Epoch 13: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0725 - accuracy: 0.9964 - val_loss: 0.4144 - val_accuracy: 0.9308 - lr: 0.0100\n",
      "Epoch 14/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9963\n",
      "Epoch 14: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0729 - accuracy: 0.9963 - val_loss: 0.4163 - val_accuracy: 0.9310 - lr: 0.0100\n",
      "Epoch 15/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9966\n",
      "Epoch 15: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0721 - accuracy: 0.9966 - val_loss: 0.4131 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 16/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9964\n",
      "Epoch 16: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0719 - accuracy: 0.9964 - val_loss: 0.4155 - val_accuracy: 0.9310 - lr: 0.0100\n",
      "Epoch 17/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9964\n",
      "Epoch 17: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0724 - accuracy: 0.9964 - val_loss: 0.4155 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 18/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9964\n",
      "Epoch 18: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0723 - accuracy: 0.9964 - val_loss: 0.4197 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 19/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9965\n",
      "Epoch 19: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0722 - accuracy: 0.9965 - val_loss: 0.4165 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 20/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9966\n",
      "Epoch 20: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0719 - accuracy: 0.9966 - val_loss: 0.4183 - val_accuracy: 0.9294 - lr: 0.0100\n",
      "Epoch 21/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9969\n",
      "Epoch 21: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0714 - accuracy: 0.9969 - val_loss: 0.4213 - val_accuracy: 0.9292 - lr: 0.0100\n",
      "Epoch 22/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9967\n",
      "Epoch 22: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0719 - accuracy: 0.9967 - val_loss: 0.4203 - val_accuracy: 0.9302 - lr: 0.0100\n",
      "Epoch 23/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9972\n",
      "Epoch 23: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0709 - accuracy: 0.9972 - val_loss: 0.4226 - val_accuracy: 0.9298 - lr: 0.0100\n",
      "Epoch 24/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9968\n",
      "Epoch 24: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0714 - accuracy: 0.9968 - val_loss: 0.4234 - val_accuracy: 0.9300 - lr: 0.0100\n",
      "Epoch 25/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9968\n",
      "Epoch 25: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0718 - accuracy: 0.9968 - val_loss: 0.4214 - val_accuracy: 0.9310 - lr: 0.0100\n",
      "Epoch 26/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9971\n",
      "Epoch 26: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0709 - accuracy: 0.9971 - val_loss: 0.4231 - val_accuracy: 0.9298 - lr: 0.0100\n",
      "Epoch 27/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9965\n",
      "Epoch 27: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0715 - accuracy: 0.9965 - val_loss: 0.4196 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 28/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9970\n",
      "Epoch 28: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0711 - accuracy: 0.9970 - val_loss: 0.4246 - val_accuracy: 0.9302 - lr: 0.0100\n",
      "Epoch 29/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9968\n",
      "Epoch 29: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0713 - accuracy: 0.9968 - val_loss: 0.4229 - val_accuracy: 0.9286 - lr: 0.0100\n",
      "Epoch 30/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9969\n",
      "Epoch 30: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0710 - accuracy: 0.9969 - val_loss: 0.4256 - val_accuracy: 0.9296 - lr: 0.0100\n",
      "Epoch 31/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9970\n",
      "Epoch 31: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0707 - accuracy: 0.9970 - val_loss: 0.4274 - val_accuracy: 0.9302 - lr: 0.0100\n",
      "Epoch 32/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9969\n",
      "Epoch 32: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 43ms/step - loss: 0.0707 - accuracy: 0.9969 - val_loss: 0.4256 - val_accuracy: 0.9312 - lr: 0.0100\n",
      "Epoch 33/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9965\n",
      "Epoch 33: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0719 - accuracy: 0.9965 - val_loss: 0.4254 - val_accuracy: 0.9308 - lr: 0.0100\n",
      "Epoch 34/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9969\n",
      "Epoch 34: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0715 - accuracy: 0.9969 - val_loss: 0.4266 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 35/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9966\n",
      "Epoch 35: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0717 - accuracy: 0.9966 - val_loss: 0.4261 - val_accuracy: 0.9296 - lr: 0.0100\n",
      "Epoch 36/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9969\n",
      "Epoch 36: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0709 - accuracy: 0.9969 - val_loss: 0.4272 - val_accuracy: 0.9308 - lr: 0.0100\n",
      "Epoch 37/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9969\n",
      "Epoch 37: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0710 - accuracy: 0.9969 - val_loss: 0.4266 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 38/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9968\n",
      "Epoch 38: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0714 - accuracy: 0.9968 - val_loss: 0.4311 - val_accuracy: 0.9304 - lr: 0.0100\n",
      "Epoch 39/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9967\n",
      "Epoch 39: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0709 - accuracy: 0.9967 - val_loss: 0.4308 - val_accuracy: 0.9308 - lr: 0.0100\n",
      "Epoch 40/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9970\n",
      "Epoch 40: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0711 - accuracy: 0.9970 - val_loss: 0.4304 - val_accuracy: 0.9308 - lr: 0.0100\n",
      "Epoch 41/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9969\n",
      "Epoch 41: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0711 - accuracy: 0.9969 - val_loss: 0.4304 - val_accuracy: 0.9310 - lr: 0.0100\n",
      "Epoch 42/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9968\n",
      "Epoch 42: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0710 - accuracy: 0.9968 - val_loss: 0.4284 - val_accuracy: 0.9312 - lr: 0.0100\n",
      "Epoch 43/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9965\n",
      "Epoch 43: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0710 - accuracy: 0.9965 - val_loss: 0.4323 - val_accuracy: 0.9300 - lr: 0.0100\n",
      "Epoch 44/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9969\n",
      "Epoch 44: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0705 - accuracy: 0.9969 - val_loss: 0.4317 - val_accuracy: 0.9310 - lr: 0.0100\n",
      "Epoch 45/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9970\n",
      "Epoch 45: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0706 - accuracy: 0.9970 - val_loss: 0.4321 - val_accuracy: 0.9296 - lr: 0.0100\n",
      "Epoch 46/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9972\n",
      "Epoch 46: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0707 - accuracy: 0.9972 - val_loss: 0.4313 - val_accuracy: 0.9300 - lr: 0.0010\n",
      "Epoch 47/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9969\n",
      "Epoch 47: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0714 - accuracy: 0.9969 - val_loss: 0.4327 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 48/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9974\n",
      "Epoch 48: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0710 - accuracy: 0.9974 - val_loss: 0.4318 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 49/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9969\n",
      "Epoch 49: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0712 - accuracy: 0.9969 - val_loss: 0.4313 - val_accuracy: 0.9300 - lr: 0.0010\n",
      "Epoch 50/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9965\n",
      "Epoch 50: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0715 - accuracy: 0.9965 - val_loss: 0.4326 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 51/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9976\n",
      "Epoch 51: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0697 - accuracy: 0.9976 - val_loss: 0.4316 - val_accuracy: 0.9302 - lr: 0.0010\n",
      "Epoch 52/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9973\n",
      "Epoch 52: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0702 - accuracy: 0.9973 - val_loss: 0.4313 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 53/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9970\n",
      "Epoch 53: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0701 - accuracy: 0.9970 - val_loss: 0.4309 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Epoch 54/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9973\n",
      "Epoch 54: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0702 - accuracy: 0.9973 - val_loss: 0.4322 - val_accuracy: 0.9302 - lr: 0.0010\n",
      "Epoch 55/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9969\n",
      "Epoch 55: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0708 - accuracy: 0.9969 - val_loss: 0.4317 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 56/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9973\n",
      "Epoch 56: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0702 - accuracy: 0.9973 - val_loss: 0.4325 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 57/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9972\n",
      "Epoch 57: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0704 - accuracy: 0.9972 - val_loss: 0.4319 - val_accuracy: 0.9312 - lr: 0.0010\n",
      "Epoch 58/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9973\n",
      "Epoch 58: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0698 - accuracy: 0.9973 - val_loss: 0.4313 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 59/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9968\n",
      "Epoch 59: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0715 - accuracy: 0.9968 - val_loss: 0.4343 - val_accuracy: 0.9294 - lr: 0.0010\n",
      "Epoch 60/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9971\n",
      "Epoch 60: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0709 - accuracy: 0.9971 - val_loss: 0.4314 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 61/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9972\n",
      "Epoch 61: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0701 - accuracy: 0.9972 - val_loss: 0.4325 - val_accuracy: 0.9302 - lr: 0.0010\n",
      "Epoch 62/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9969\n",
      "Epoch 62: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0705 - accuracy: 0.9969 - val_loss: 0.4328 - val_accuracy: 0.9296 - lr: 0.0010\n",
      "Epoch 63/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9965\n",
      "Epoch 63: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0715 - accuracy: 0.9965 - val_loss: 0.4321 - val_accuracy: 0.9300 - lr: 0.0010\n",
      "Epoch 64/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9969\n",
      "Epoch 64: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0709 - accuracy: 0.9969 - val_loss: 0.4315 - val_accuracy: 0.9312 - lr: 0.0010\n",
      "Epoch 65/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9969\n",
      "Epoch 65: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0711 - accuracy: 0.9969 - val_loss: 0.4328 - val_accuracy: 0.9300 - lr: 0.0010\n",
      "Epoch 66/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9971\n",
      "Epoch 66: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0704 - accuracy: 0.9971 - val_loss: 0.4331 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 67/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9966\n",
      "Epoch 67: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0708 - accuracy: 0.9966 - val_loss: 0.4333 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 68/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9977\n",
      "Epoch 68: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0694 - accuracy: 0.9977 - val_loss: 0.4325 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 69/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9971\n",
      "Epoch 69: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0701 - accuracy: 0.9971 - val_loss: 0.4328 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 70/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9970\n",
      "Epoch 70: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0706 - accuracy: 0.9970 - val_loss: 0.4333 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Epoch 71/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9972\n",
      "Epoch 71: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0704 - accuracy: 0.9972 - val_loss: 0.4331 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Epoch 72/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9975\n",
      "Epoch 72: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0698 - accuracy: 0.9975 - val_loss: 0.4306 - val_accuracy: 0.9298 - lr: 0.0010\n",
      "Epoch 73/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9970\n",
      "Epoch 73: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0712 - accuracy: 0.9970 - val_loss: 0.4322 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 74/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9972\n",
      "Epoch 74: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0701 - accuracy: 0.9972 - val_loss: 0.4326 - val_accuracy: 0.9302 - lr: 0.0010\n",
      "Epoch 75/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9971\n",
      "Epoch 75: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0703 - accuracy: 0.9971 - val_loss: 0.4321 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 76/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9972\n",
      "Epoch 76: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0700 - accuracy: 0.9972 - val_loss: 0.4328 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 77/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9970\n",
      "Epoch 77: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0708 - accuracy: 0.9970 - val_loss: 0.4333 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Epoch 78/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9967\n",
      "Epoch 78: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0710 - accuracy: 0.9967 - val_loss: 0.4333 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 79/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9971\n",
      "Epoch 79: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0701 - accuracy: 0.9971 - val_loss: 0.4341 - val_accuracy: 0.9298 - lr: 0.0010\n",
      "Epoch 80/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.9971\n",
      "Epoch 80: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0705 - accuracy: 0.9971 - val_loss: 0.4333 - val_accuracy: 0.9312 - lr: 0.0010\n",
      "Epoch 81/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9969\n",
      "Epoch 81: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0705 - accuracy: 0.9969 - val_loss: 0.4321 - val_accuracy: 0.9312 - lr: 0.0010\n",
      "Epoch 82/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.9971\n",
      "Epoch 82: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0705 - accuracy: 0.9971 - val_loss: 0.4335 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 83/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9972\n",
      "Epoch 83: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.0702 - accuracy: 0.9972 - val_loss: 0.4338 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Epoch 84/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9969\n",
      "Epoch 84: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0710 - accuracy: 0.9969 - val_loss: 0.4329 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 85/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9971\n",
      "Epoch 85: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 43ms/step - loss: 0.0704 - accuracy: 0.9971 - val_loss: 0.4339 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 86/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9971\n",
      "Epoch 86: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0704 - accuracy: 0.9971 - val_loss: 0.4344 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 87/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9972\n",
      "Epoch 87: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0705 - accuracy: 0.9972 - val_loss: 0.4354 - val_accuracy: 0.9316 - lr: 0.0010\n",
      "Epoch 88/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9973\n",
      "Epoch 88: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0702 - accuracy: 0.9973 - val_loss: 0.4331 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 89/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9970\n",
      "Epoch 89: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0707 - accuracy: 0.9970 - val_loss: 0.4350 - val_accuracy: 0.9312 - lr: 0.0010\n",
      "Epoch 90/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9971\n",
      "Epoch 90: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0707 - accuracy: 0.9971 - val_loss: 0.4347 - val_accuracy: 0.9312 - lr: 0.0010\n",
      "Epoch 91/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9970\n",
      "Epoch 91: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0704 - accuracy: 0.9970 - val_loss: 0.4338 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 92/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9970\n",
      "Epoch 92: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0706 - accuracy: 0.9970 - val_loss: 0.4339 - val_accuracy: 0.9316 - lr: 0.0010\n",
      "Epoch 93/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9969\n",
      "Epoch 93: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0705 - accuracy: 0.9969 - val_loss: 0.4327 - val_accuracy: 0.9318 - lr: 0.0010\n",
      "Epoch 94/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9971\n",
      "Epoch 94: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0704 - accuracy: 0.9971 - val_loss: 0.4325 - val_accuracy: 0.9314 - lr: 0.0010\n",
      "Epoch 95/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9973\n",
      "Epoch 95: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0704 - accuracy: 0.9973 - val_loss: 0.4325 - val_accuracy: 0.9312 - lr: 0.0010\n",
      "Epoch 96/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9967\n",
      "Epoch 96: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0721 - accuracy: 0.9967 - val_loss: 0.4320 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Epoch 97/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9970\n",
      "Epoch 97: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0711 - accuracy: 0.9970 - val_loss: 0.4353 - val_accuracy: 0.9314 - lr: 0.0010\n",
      "Epoch 98/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9970\n",
      "Epoch 98: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0709 - accuracy: 0.9970 - val_loss: 0.4340 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Epoch 99/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9967\n",
      "Epoch 99: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0707 - accuracy: 0.9968 - val_loss: 0.4341 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 100/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9966\n",
      "Epoch 100: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0715 - accuracy: 0.9966 - val_loss: 0.4335 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 101/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9965\n",
      "Epoch 101: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0713 - accuracy: 0.9965 - val_loss: 0.4324 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 102/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9970\n",
      "Epoch 102: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0704 - accuracy: 0.9970 - val_loss: 0.4327 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 103/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9973\n",
      "Epoch 103: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0700 - accuracy: 0.9973 - val_loss: 0.4353 - val_accuracy: 0.9302 - lr: 0.0010\n",
      "Epoch 104/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9966\n",
      "Epoch 104: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0711 - accuracy: 0.9966 - val_loss: 0.4348 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 105/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.9969\n",
      "Epoch 105: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0705 - accuracy: 0.9970 - val_loss: 0.4341 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Epoch 106/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9972\n",
      "Epoch 106: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0701 - accuracy: 0.9972 - val_loss: 0.4336 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 107/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9970\n",
      "Epoch 107: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0710 - accuracy: 0.9970 - val_loss: 0.4353 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Epoch 108/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9975\n",
      "Epoch 108: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0699 - accuracy: 0.9975 - val_loss: 0.4346 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Current:  142\n",
      "Epoch 1/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9983\n",
      "Epoch 1: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 35s 44ms/step - loss: 0.0671 - accuracy: 0.9983 - val_loss: 0.4347 - val_accuracy: 0.9302 - lr: 0.0010\n",
      "Epoch 2/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9979\n",
      "Epoch 2: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0681 - accuracy: 0.9980 - val_loss: 0.4346 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 3/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9980\n",
      "Epoch 3: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 43ms/step - loss: 0.0676 - accuracy: 0.9980 - val_loss: 0.4348 - val_accuracy: 0.9302 - lr: 0.0010\n",
      "Epoch 4/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9980\n",
      "Epoch 4: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0684 - accuracy: 0.9980 - val_loss: 0.4348 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 5/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9983\n",
      "Epoch 5: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0673 - accuracy: 0.9983 - val_loss: 0.4348 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 6/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.9982\n",
      "Epoch 6: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0679 - accuracy: 0.9982 - val_loss: 0.4347 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 7/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9980\n",
      "Epoch 7: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0679 - accuracy: 0.9980 - val_loss: 0.4348 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 8/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9978\n",
      "Epoch 8: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0677 - accuracy: 0.9978 - val_loss: 0.4349 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 9/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0669 - accuracy: 0.9985\n",
      "Epoch 9: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0669 - accuracy: 0.9985 - val_loss: 0.4349 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 10/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.9981\n",
      "Epoch 10: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0679 - accuracy: 0.9981 - val_loss: 0.4349 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Epoch 11/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9979\n",
      "Epoch 11: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0680 - accuracy: 0.9979 - val_loss: 0.4349 - val_accuracy: 0.9302 - lr: 0.0010\n",
      "Epoch 12/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9984\n",
      "Epoch 12: val_loss did not improve from 0.38577\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0677 - accuracy: 0.9984 - val_loss: 0.4349 - val_accuracy: 0.9306 - lr: 0.0010\n",
      "Current:  154\n",
      "313/313 [==============================] - 5s 10ms/step\n",
      "Accuracy: 92.01\n",
      "Error: 7.989999999999995\n",
      "ECE: 0.058042032223939936\n",
      "MCE: 0.3539296210247234\n",
      "Loss: 0.44504994593241937\n",
      "brier: 0.06945804049649414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.989999999999995,\n",
       " 0.058042032223939936,\n",
       " 0.3539296210247234,\n",
       " 0.44504994593241937,\n",
       " 0.06945804049649414]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.continue_training(model, img_gen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list,batch_size=batch_size,lr_schedule = [[nb_epoch*0.5,0.01],[nb_epoch*0.75,0.001]],all_epochs=150,weights='dense_cifar10_2_150.h5',cbks=[checkpointer], name='dense_cifar10_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea79646-a7bb-4c4d-9b9f-0f7f40f901a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
