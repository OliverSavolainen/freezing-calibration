{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b1089e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T15:08:52.139340Z",
     "iopub.status.busy": "2023-04-24T15:08:52.138379Z",
     "iopub.status.idle": "2023-04-24T15:08:58.788249Z",
     "shell.execute_reply": "2023-04-24T15:08:58.787144Z"
    },
    "papermill": {
     "duration": 6.658291,
     "end_time": "2023-04-24T15:08:58.790956",
     "exception": false,
     "start_time": "2023-04-24T15:08:52.132665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import cifar100\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2857f68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T15:08:58.799634Z",
     "iopub.status.busy": "2023-04-24T15:08:58.798994Z",
     "iopub.status.idle": "2023-04-24T15:08:58.811024Z",
     "shell.execute_reply": "2023-04-24T15:08:58.809941Z"
    },
    "papermill": {
     "duration": 0.019077,
     "end_time": "2023-04-24T15:08:58.813552",
     "exception": false,
     "start_time": "2023-04-24T15:08:58.794475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import densenet\n",
    "import freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfde30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T15:09:05.239139Z",
     "iopub.status.busy": "2023-04-24T15:09:05.238855Z",
     "iopub.status.idle": "2023-04-24T15:09:05.247505Z",
     "shell.execute_reply": "2023-04-24T15:09:05.246566Z"
    },
    "papermill": {
     "duration": 0.056736,
     "end_time": "2023-04-24T15:09:05.249618",
     "exception": false,
     "start_time": "2023-04-24T15:09:05.192882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 100\n",
    "nb_epoch = 300\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "\n",
    "img_dim = (img_channels, img_rows, img_cols) if K.image_data_format() == \"channels_first\" else (img_rows, img_cols, img_channels)\n",
    "depth = 40\n",
    "nb_dense_block = 3\n",
    "growth_rate = 12\n",
    "bottle_neck = False\n",
    "nb_filter = 12\n",
    "reduction = 0.0\n",
    "dropout_rate = 0.0 # 0.0 for data augmentation\n",
    "seed = 333\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Preprocessing for DenseNet https://arxiv.org/pdf/1608.06993v3.pdf\n",
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7193ce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T15:09:05.341729Z",
     "iopub.status.busy": "2023-04-24T15:09:05.341455Z",
     "iopub.status.idle": "2023-04-24T15:09:23.464208Z",
     "shell.execute_reply": "2023-04-24T15:09:23.463162Z"
    },
    "papermill": {
     "duration": 18.172307,
     "end_time": "2023-04-24T15:09:23.467076",
     "exception": false,
     "start_time": "2023-04-24T15:09:05.294769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = densenet.DenseNet(img_dim, classes=nb_classes, depth=depth, nb_dense_block=nb_dense_block,\n",
    "                          growth_rate=growth_rate, nb_filter=nb_filter, dropout_rate=dropout_rate, weights=None, weight_decay=1e-4)\n",
    "print(\"Model created\")\n",
    "\n",
    "model.summary()\n",
    "sgd = SGD(learning_rate=0.1, momentum=0.9, nesterov=True)  # dampening = 0.9? Should be zero?\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "print(\"Finished compiling\")\n",
    "print(\"Building model...\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "#For data preprocessing, we normalize the data using the channel means and standard deviations (https://arxiv.org/pdf/1608.06993v3.pdf)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)\n",
    "\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "\n",
    "\n",
    "img_gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.125,  # 0.125*32 = 4 so max padding of 4 pixels, as described in paper.\n",
    "    height_shift_range=0.125,  # first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32Ã—32 images\n",
    "    fill_mode = \"constant\",\n",
    "    cval = 0\n",
    ")\n",
    "\n",
    "y_train45 = to_categorical(y_train45, nb_classes)  # 1-hot vector\n",
    "y_val = to_categorical(y_val, nb_classes)\n",
    "y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "img_gen.fit(x_train45, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ff1102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T15:09:23.617825Z",
     "iopub.status.busy": "2023-04-24T15:09:23.617496Z",
     "iopub.status.idle": "2023-04-24T15:09:23.655220Z",
     "shell.execute_reply": "2023-04-24T15:09:23.654346Z"
    },
    "papermill": {
     "duration": 0.114295,
     "end_time": "2023-04-24T15:09:23.657438",
     "exception": false,
     "start_time": "2023-04-24T15:09:23.543143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.9:\n",
    "    freezing_list.append(int(nb_epoch*0.6))\n",
    "  elif i < len(model.layers) * 0.98:\n",
    "    freezing_list.append(int(nb_epoch*0.96))\n",
    "freezing_list.append(nb_epoch)\n",
    "checkpointer = ModelCheckpoint('model_dense_c100_best_2.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "319ab7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T15:09:23.806943Z",
     "iopub.status.busy": "2023-04-24T15:09:23.806002Z",
     "iopub.status.idle": "2023-04-24T19:38:30.741789Z",
     "shell.execute_reply": "2023-04-24T19:38:30.740714Z"
    },
    "papermill": {
     "duration": 16152.140016,
     "end_time": "2023-04-24T19:38:35.871181",
     "exception": false,
     "start_time": "2023-04-24T15:09:23.731165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-24 15:09:23        68745\n",
      "variables.h5                                   2023-04-24 15:09:24      4622744\n",
      "metadata.json                                  2023-04-24 15:09:23           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-24 15:09:22        68745\n",
      "variables.h5                                   2023-04-24 15:09:24      4622744\n",
      "metadata.json                                  2023-04-24 15:09:22           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 3.9387 - accuracy: 0.1103\n",
      "Epoch 1: val_loss improved from inf to 4.38427, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 57s 68ms/step - loss: 3.9387 - accuracy: 0.1103 - val_loss: 4.3843 - val_accuracy: 0.1220 - lr: 0.1000\n",
      "Epoch 2/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 3.1751 - accuracy: 0.2302\n",
      "Epoch 2: val_loss improved from 4.38427 to 3.18125, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 3.1751 - accuracy: 0.2302 - val_loss: 3.1813 - val_accuracy: 0.2424 - lr: 0.1000\n",
      "Epoch 3/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 2.7050 - accuracy: 0.3214\n",
      "Epoch 3: val_loss improved from 3.18125 to 2.80759, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 2.7050 - accuracy: 0.3214 - val_loss: 2.8076 - val_accuracy: 0.3128 - lr: 0.1000\n",
      "Epoch 4/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 2.3836 - accuracy: 0.3884\n",
      "Epoch 4: val_loss did not improve from 2.80759\n",
      "703/703 [==============================] - 47s 66ms/step - loss: 2.3836 - accuracy: 0.3884 - val_loss: 2.9794 - val_accuracy: 0.3220 - lr: 0.1000\n",
      "Epoch 5/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 2.1586 - accuracy: 0.4381\n",
      "Epoch 5: val_loss improved from 2.80759 to 2.70707, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 2.1586 - accuracy: 0.4381 - val_loss: 2.7071 - val_accuracy: 0.3704 - lr: 0.1000\n",
      "Epoch 6/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.9966 - accuracy: 0.4782\n",
      "Epoch 6: val_loss improved from 2.70707 to 2.41894, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 1.9966 - accuracy: 0.4782 - val_loss: 2.4189 - val_accuracy: 0.4264 - lr: 0.1000\n",
      "Epoch 7/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.8655 - accuracy: 0.5110\n",
      "Epoch 7: val_loss did not improve from 2.41894\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.8655 - accuracy: 0.5110 - val_loss: 2.8485 - val_accuracy: 0.3810 - lr: 0.1000\n",
      "Epoch 8/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.7523 - accuracy: 0.5379\n",
      "Epoch 8: val_loss did not improve from 2.41894\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.7523 - accuracy: 0.5379 - val_loss: 2.4469 - val_accuracy: 0.4518 - lr: 0.1000\n",
      "Epoch 9/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.6633 - accuracy: 0.5616\n",
      "Epoch 9: val_loss improved from 2.41894 to 2.05636, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 1.6633 - accuracy: 0.5616 - val_loss: 2.0564 - val_accuracy: 0.5000 - lr: 0.1000\n",
      "Epoch 10/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.5801 - accuracy: 0.5813\n",
      "Epoch 10: val_loss did not improve from 2.05636\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.5801 - accuracy: 0.5813 - val_loss: 2.0888 - val_accuracy: 0.5008 - lr: 0.1000\n",
      "Epoch 11/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.5161 - accuracy: 0.5994\n",
      "Epoch 11: val_loss did not improve from 2.05636\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.5161 - accuracy: 0.5994 - val_loss: 2.0777 - val_accuracy: 0.5122 - lr: 0.1000\n",
      "Epoch 12/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.4468 - accuracy: 0.6192\n",
      "Epoch 12: val_loss did not improve from 2.05636\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.4468 - accuracy: 0.6192 - val_loss: 2.1488 - val_accuracy: 0.5018 - lr: 0.1000\n",
      "Epoch 13/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.4064 - accuracy: 0.6309\n",
      "Epoch 13: val_loss improved from 2.05636 to 1.92145, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.4064 - accuracy: 0.6309 - val_loss: 1.9215 - val_accuracy: 0.5382 - lr: 0.1000\n",
      "Epoch 14/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.3513 - accuracy: 0.6424\n",
      "Epoch 14: val_loss improved from 1.92145 to 1.87530, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.3513 - accuracy: 0.6424 - val_loss: 1.8753 - val_accuracy: 0.5416 - lr: 0.1000\n",
      "Epoch 15/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.3059 - accuracy: 0.6565\n",
      "Epoch 15: val_loss did not improve from 1.87530\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 1.3059 - accuracy: 0.6565 - val_loss: 2.4423 - val_accuracy: 0.4938 - lr: 0.1000\n",
      "Epoch 16/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.2684 - accuracy: 0.6671\n",
      "Epoch 16: val_loss improved from 1.87530 to 1.81390, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.2684 - accuracy: 0.6671 - val_loss: 1.8139 - val_accuracy: 0.5654 - lr: 0.1000\n",
      "Epoch 17/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.2393 - accuracy: 0.6753\n",
      "Epoch 17: val_loss did not improve from 1.81390\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.2393 - accuracy: 0.6753 - val_loss: 2.2431 - val_accuracy: 0.5342 - lr: 0.1000\n",
      "Epoch 18/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.2007 - accuracy: 0.6853\n",
      "Epoch 18: val_loss did not improve from 1.81390\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 1.2007 - accuracy: 0.6853 - val_loss: 1.9467 - val_accuracy: 0.5526 - lr: 0.1000\n",
      "Epoch 19/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.1703 - accuracy: 0.6961\n",
      "Epoch 19: val_loss did not improve from 1.81390\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.1703 - accuracy: 0.6961 - val_loss: 1.8689 - val_accuracy: 0.5662 - lr: 0.1000\n",
      "Epoch 20/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.1366 - accuracy: 0.7041\n",
      "Epoch 20: val_loss did not improve from 1.81390\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.1366 - accuracy: 0.7041 - val_loss: 1.9845 - val_accuracy: 0.5528 - lr: 0.1000\n",
      "Epoch 21/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.1165 - accuracy: 0.7083\n",
      "Epoch 21: val_loss did not improve from 1.81390\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.1165 - accuracy: 0.7083 - val_loss: 2.1018 - val_accuracy: 0.5432 - lr: 0.1000\n",
      "Epoch 22/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0895 - accuracy: 0.7180\n",
      "Epoch 22: val_loss did not improve from 1.81390\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.0895 - accuracy: 0.7180 - val_loss: 1.8903 - val_accuracy: 0.5632 - lr: 0.1000\n",
      "Epoch 23/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0644 - accuracy: 0.7260\n",
      "Epoch 23: val_loss did not improve from 1.81390\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 1.0644 - accuracy: 0.7260 - val_loss: 2.0871 - val_accuracy: 0.5610 - lr: 0.1000\n",
      "Epoch 24/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0499 - accuracy: 0.7283\n",
      "Epoch 24: val_loss did not improve from 1.81390\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.0499 - accuracy: 0.7283 - val_loss: 1.9007 - val_accuracy: 0.5730 - lr: 0.1000\n",
      "Epoch 25/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0262 - accuracy: 0.7373\n",
      "Epoch 25: val_loss improved from 1.81390 to 1.61222, saving model to model_dense_c100_best_2.hdf5\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.0262 - accuracy: 0.7373 - val_loss: 1.6122 - val_accuracy: 0.6266 - lr: 0.1000\n",
      "Epoch 26/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0039 - accuracy: 0.7432\n",
      "Epoch 26: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.0039 - accuracy: 0.7432 - val_loss: 1.7886 - val_accuracy: 0.6050 - lr: 0.1000\n",
      "Epoch 27/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9844 - accuracy: 0.7512\n",
      "Epoch 27: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.9844 - accuracy: 0.7512 - val_loss: 2.1693 - val_accuracy: 0.5520 - lr: 0.1000\n",
      "Epoch 28/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9654 - accuracy: 0.7539\n",
      "Epoch 28: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.9654 - accuracy: 0.7539 - val_loss: 1.7906 - val_accuracy: 0.5974 - lr: 0.1000\n",
      "Epoch 29/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9444 - accuracy: 0.7591\n",
      "Epoch 29: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.9444 - accuracy: 0.7591 - val_loss: 1.7791 - val_accuracy: 0.6046 - lr: 0.1000\n",
      "Epoch 30/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9325 - accuracy: 0.7624\n",
      "Epoch 30: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.9325 - accuracy: 0.7624 - val_loss: 1.9783 - val_accuracy: 0.5748 - lr: 0.1000\n",
      "Epoch 31/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9274 - accuracy: 0.7628\n",
      "Epoch 31: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.9274 - accuracy: 0.7628 - val_loss: 1.7871 - val_accuracy: 0.6082 - lr: 0.1000\n",
      "Epoch 32/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8984 - accuracy: 0.7744\n",
      "Epoch 32: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.8984 - accuracy: 0.7744 - val_loss: 1.8159 - val_accuracy: 0.6134 - lr: 0.1000\n",
      "Epoch 33/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8885 - accuracy: 0.7761\n",
      "Epoch 33: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.8885 - accuracy: 0.7761 - val_loss: 1.8343 - val_accuracy: 0.6100 - lr: 0.1000\n",
      "Epoch 34/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8760 - accuracy: 0.7817\n",
      "Epoch 34: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.8760 - accuracy: 0.7817 - val_loss: 2.0643 - val_accuracy: 0.5940 - lr: 0.1000\n",
      "Epoch 35/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8631 - accuracy: 0.7842\n",
      "Epoch 35: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 66ms/step - loss: 0.8631 - accuracy: 0.7842 - val_loss: 1.9768 - val_accuracy: 0.6056 - lr: 0.1000\n",
      "Epoch 36/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8627 - accuracy: 0.7837\n",
      "Epoch 36: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.8627 - accuracy: 0.7837 - val_loss: 1.7714 - val_accuracy: 0.6222 - lr: 0.1000\n",
      "Epoch 37/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8325 - accuracy: 0.7945\n",
      "Epoch 37: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.8325 - accuracy: 0.7945 - val_loss: 2.0932 - val_accuracy: 0.5940 - lr: 0.1000\n",
      "Epoch 38/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8370 - accuracy: 0.7910\n",
      "Epoch 38: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.8370 - accuracy: 0.7910 - val_loss: 2.1147 - val_accuracy: 0.5668 - lr: 0.1000\n",
      "Epoch 39/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8128 - accuracy: 0.7992\n",
      "Epoch 39: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.8128 - accuracy: 0.7992 - val_loss: 1.8665 - val_accuracy: 0.6182 - lr: 0.1000\n",
      "Epoch 40/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8043 - accuracy: 0.8033\n",
      "Epoch 40: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.8043 - accuracy: 0.8033 - val_loss: 1.9270 - val_accuracy: 0.6080 - lr: 0.1000\n",
      "Epoch 41/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7952 - accuracy: 0.8056\n",
      "Epoch 41: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.7952 - accuracy: 0.8056 - val_loss: 1.8949 - val_accuracy: 0.6048 - lr: 0.1000\n",
      "Epoch 42/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7890 - accuracy: 0.8060\n",
      "Epoch 42: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.7890 - accuracy: 0.8060 - val_loss: 2.0060 - val_accuracy: 0.6180 - lr: 0.1000\n",
      "Epoch 43/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7739 - accuracy: 0.8110\n",
      "Epoch 43: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.7739 - accuracy: 0.8110 - val_loss: 1.9300 - val_accuracy: 0.6172 - lr: 0.1000\n",
      "Epoch 44/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7726 - accuracy: 0.8122\n",
      "Epoch 44: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.7726 - accuracy: 0.8122 - val_loss: 2.0220 - val_accuracy: 0.6144 - lr: 0.1000\n",
      "Epoch 45/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7494 - accuracy: 0.8196\n",
      "Epoch 45: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.7494 - accuracy: 0.8196 - val_loss: 1.9107 - val_accuracy: 0.6134 - lr: 0.1000\n",
      "Epoch 46/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7425 - accuracy: 0.8212\n",
      "Epoch 46: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.7425 - accuracy: 0.8212 - val_loss: 1.9622 - val_accuracy: 0.6116 - lr: 0.1000\n",
      "Epoch 47/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7346 - accuracy: 0.8226\n",
      "Epoch 47: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.7346 - accuracy: 0.8226 - val_loss: 1.8653 - val_accuracy: 0.6248 - lr: 0.1000\n",
      "Epoch 48/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7272 - accuracy: 0.8243\n",
      "Epoch 48: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.7272 - accuracy: 0.8243 - val_loss: 2.0148 - val_accuracy: 0.6068 - lr: 0.1000\n",
      "Epoch 49/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7238 - accuracy: 0.8262\n",
      "Epoch 49: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.7238 - accuracy: 0.8262 - val_loss: 1.9776 - val_accuracy: 0.6198 - lr: 0.1000\n",
      "Epoch 50/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7096 - accuracy: 0.8312\n",
      "Epoch 50: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.7096 - accuracy: 0.8312 - val_loss: 1.9508 - val_accuracy: 0.6156 - lr: 0.1000\n",
      "Epoch 51/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7114 - accuracy: 0.8302\n",
      "Epoch 51: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.7114 - accuracy: 0.8302 - val_loss: 2.2439 - val_accuracy: 0.5990 - lr: 0.1000\n",
      "Epoch 52/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.8368\n",
      "Epoch 52: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.6874 - accuracy: 0.8368 - val_loss: 1.9483 - val_accuracy: 0.6218 - lr: 0.1000\n",
      "Epoch 53/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.8350\n",
      "Epoch 53: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.6960 - accuracy: 0.8350 - val_loss: 1.8231 - val_accuracy: 0.6304 - lr: 0.1000\n",
      "Epoch 54/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6829 - accuracy: 0.8397\n",
      "Epoch 54: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.6829 - accuracy: 0.8397 - val_loss: 1.9684 - val_accuracy: 0.6234 - lr: 0.1000\n",
      "Epoch 55/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.8382\n",
      "Epoch 55: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.6878 - accuracy: 0.8382 - val_loss: 2.0530 - val_accuracy: 0.6190 - lr: 0.1000\n",
      "Epoch 56/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6644 - accuracy: 0.8447\n",
      "Epoch 56: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.6644 - accuracy: 0.8447 - val_loss: 2.1740 - val_accuracy: 0.5956 - lr: 0.1000\n",
      "Epoch 57/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6554 - accuracy: 0.8481\n",
      "Epoch 57: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.6554 - accuracy: 0.8481 - val_loss: 2.0393 - val_accuracy: 0.6184 - lr: 0.1000\n",
      "Epoch 58/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6600 - accuracy: 0.8454\n",
      "Epoch 58: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.6600 - accuracy: 0.8454 - val_loss: 1.9440 - val_accuracy: 0.6314 - lr: 0.1000\n",
      "Epoch 59/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6597 - accuracy: 0.8487\n",
      "Epoch 59: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.6597 - accuracy: 0.8487 - val_loss: 1.9623 - val_accuracy: 0.6312 - lr: 0.1000\n",
      "Epoch 60/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.8501\n",
      "Epoch 60: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.6481 - accuracy: 0.8501 - val_loss: 2.0146 - val_accuracy: 0.6266 - lr: 0.1000\n",
      "Epoch 61/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6480 - accuracy: 0.8503\n",
      "Epoch 61: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.6480 - accuracy: 0.8503 - val_loss: 2.0898 - val_accuracy: 0.6246 - lr: 0.1000\n",
      "Epoch 62/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6376 - accuracy: 0.8521\n",
      "Epoch 62: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.6376 - accuracy: 0.8521 - val_loss: 2.1341 - val_accuracy: 0.6172 - lr: 0.1000\n",
      "Epoch 63/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6405 - accuracy: 0.8542\n",
      "Epoch 63: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.6405 - accuracy: 0.8542 - val_loss: 1.8934 - val_accuracy: 0.6518 - lr: 0.1000\n",
      "Epoch 64/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6147 - accuracy: 0.8617\n",
      "Epoch 64: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.6147 - accuracy: 0.8617 - val_loss: 2.3594 - val_accuracy: 0.5946 - lr: 0.1000\n",
      "Epoch 65/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.8583\n",
      "Epoch 65: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.6222 - accuracy: 0.8583 - val_loss: 2.0651 - val_accuracy: 0.6302 - lr: 0.1000\n",
      "Epoch 66/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.8617\n",
      "Epoch 66: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.6074 - accuracy: 0.8617 - val_loss: 2.1044 - val_accuracy: 0.6174 - lr: 0.1000\n",
      "Epoch 67/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6318 - accuracy: 0.8574\n",
      "Epoch 67: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.6318 - accuracy: 0.8574 - val_loss: 1.9044 - val_accuracy: 0.6384 - lr: 0.1000\n",
      "Epoch 68/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.8667\n",
      "Epoch 68: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5984 - accuracy: 0.8667 - val_loss: 1.8463 - val_accuracy: 0.6524 - lr: 0.1000\n",
      "Epoch 69/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5944 - accuracy: 0.8665\n",
      "Epoch 69: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5944 - accuracy: 0.8665 - val_loss: 2.0771 - val_accuracy: 0.6268 - lr: 0.1000\n",
      "Epoch 70/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.8639\n",
      "Epoch 70: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.6028 - accuracy: 0.8639 - val_loss: 2.0746 - val_accuracy: 0.6314 - lr: 0.1000\n",
      "Epoch 71/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.8708\n",
      "Epoch 71: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5816 - accuracy: 0.8708 - val_loss: 2.1019 - val_accuracy: 0.6348 - lr: 0.1000\n",
      "Epoch 72/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.8675\n",
      "Epoch 72: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5905 - accuracy: 0.8675 - val_loss: 2.0405 - val_accuracy: 0.6238 - lr: 0.1000\n",
      "Epoch 73/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5837 - accuracy: 0.8707\n",
      "Epoch 73: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.5837 - accuracy: 0.8707 - val_loss: 1.9385 - val_accuracy: 0.6542 - lr: 0.1000\n",
      "Epoch 74/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5708 - accuracy: 0.8745\n",
      "Epoch 74: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5708 - accuracy: 0.8745 - val_loss: 1.9626 - val_accuracy: 0.6392 - lr: 0.1000\n",
      "Epoch 75/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5675 - accuracy: 0.8739\n",
      "Epoch 75: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5675 - accuracy: 0.8739 - val_loss: 2.1450 - val_accuracy: 0.6282 - lr: 0.1000\n",
      "Epoch 76/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5785 - accuracy: 0.8734\n",
      "Epoch 76: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.5785 - accuracy: 0.8734 - val_loss: 2.1178 - val_accuracy: 0.6364 - lr: 0.1000\n",
      "Epoch 77/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5797 - accuracy: 0.8722\n",
      "Epoch 77: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5797 - accuracy: 0.8722 - val_loss: 2.0035 - val_accuracy: 0.6372 - lr: 0.1000\n",
      "Epoch 78/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.8785\n",
      "Epoch 78: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5593 - accuracy: 0.8785 - val_loss: 2.3871 - val_accuracy: 0.6160 - lr: 0.1000\n",
      "Epoch 79/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5670 - accuracy: 0.8765\n",
      "Epoch 79: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.5670 - accuracy: 0.8765 - val_loss: 2.0891 - val_accuracy: 0.6334 - lr: 0.1000\n",
      "Epoch 80/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5490 - accuracy: 0.8823\n",
      "Epoch 80: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5490 - accuracy: 0.8823 - val_loss: 2.1864 - val_accuracy: 0.6162 - lr: 0.1000\n",
      "Epoch 81/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.8784\n",
      "Epoch 81: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5572 - accuracy: 0.8784 - val_loss: 2.0879 - val_accuracy: 0.6368 - lr: 0.1000\n",
      "Epoch 82/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.8843\n",
      "Epoch 82: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.5416 - accuracy: 0.8843 - val_loss: 2.0265 - val_accuracy: 0.6464 - lr: 0.1000\n",
      "Epoch 83/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.8864\n",
      "Epoch 83: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.5344 - accuracy: 0.8864 - val_loss: 2.1095 - val_accuracy: 0.6430 - lr: 0.1000\n",
      "Epoch 84/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5390 - accuracy: 0.8846\n",
      "Epoch 84: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.5390 - accuracy: 0.8846 - val_loss: 2.2044 - val_accuracy: 0.6308 - lr: 0.1000\n",
      "Epoch 85/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.8841\n",
      "Epoch 85: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.5447 - accuracy: 0.8841 - val_loss: 2.9039 - val_accuracy: 0.5806 - lr: 0.1000\n",
      "Epoch 86/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5324 - accuracy: 0.8874\n",
      "Epoch 86: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.5324 - accuracy: 0.8874 - val_loss: 2.1691 - val_accuracy: 0.6410 - lr: 0.1000\n",
      "Epoch 87/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.8868\n",
      "Epoch 87: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5335 - accuracy: 0.8868 - val_loss: 2.2141 - val_accuracy: 0.6292 - lr: 0.1000\n",
      "Epoch 88/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5353 - accuracy: 0.8863\n",
      "Epoch 88: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.5353 - accuracy: 0.8863 - val_loss: 2.0617 - val_accuracy: 0.6414 - lr: 0.1000\n",
      "Epoch 89/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5272 - accuracy: 0.8890\n",
      "Epoch 89: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.5272 - accuracy: 0.8890 - val_loss: 2.3297 - val_accuracy: 0.6186 - lr: 0.1000\n",
      "Epoch 90/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5196 - accuracy: 0.8899\n",
      "Epoch 90: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5196 - accuracy: 0.8899 - val_loss: 2.1046 - val_accuracy: 0.6474 - lr: 0.1000\n",
      "Epoch 91/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.8896\n",
      "Epoch 91: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.5221 - accuracy: 0.8896 - val_loss: 2.2014 - val_accuracy: 0.6278 - lr: 0.1000\n",
      "Epoch 92/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.8941\n",
      "Epoch 92: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5091 - accuracy: 0.8941 - val_loss: 2.2802 - val_accuracy: 0.6252 - lr: 0.1000\n",
      "Epoch 93/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.8928\n",
      "Epoch 93: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5108 - accuracy: 0.8928 - val_loss: 2.2727 - val_accuracy: 0.6292 - lr: 0.1000\n",
      "Epoch 94/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5177 - accuracy: 0.8919\n",
      "Epoch 94: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.5177 - accuracy: 0.8919 - val_loss: 2.1206 - val_accuracy: 0.6314 - lr: 0.1000\n",
      "Epoch 95/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5167 - accuracy: 0.8910\n",
      "Epoch 95: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5167 - accuracy: 0.8910 - val_loss: 2.1579 - val_accuracy: 0.6488 - lr: 0.1000\n",
      "Epoch 96/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.8953\n",
      "Epoch 96: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.5094 - accuracy: 0.8953 - val_loss: 2.2830 - val_accuracy: 0.6228 - lr: 0.1000\n",
      "Epoch 97/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.8968\n",
      "Epoch 97: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4985 - accuracy: 0.8968 - val_loss: 2.1991 - val_accuracy: 0.6440 - lr: 0.1000\n",
      "Epoch 98/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.8953\n",
      "Epoch 98: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5063 - accuracy: 0.8953 - val_loss: 2.2475 - val_accuracy: 0.6434 - lr: 0.1000\n",
      "Epoch 99/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4975 - accuracy: 0.8981\n",
      "Epoch 99: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4975 - accuracy: 0.8981 - val_loss: 2.1005 - val_accuracy: 0.6470 - lr: 0.1000\n",
      "Epoch 100/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.8992\n",
      "Epoch 100: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4932 - accuracy: 0.8992 - val_loss: 2.3744 - val_accuracy: 0.6178 - lr: 0.1000\n",
      "Epoch 101/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.8949\n",
      "Epoch 101: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.5034 - accuracy: 0.8949 - val_loss: 2.2794 - val_accuracy: 0.6196 - lr: 0.1000\n",
      "Epoch 102/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4761 - accuracy: 0.9040\n",
      "Epoch 102: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.4761 - accuracy: 0.9040 - val_loss: 2.3221 - val_accuracy: 0.6364 - lr: 0.1000\n",
      "Epoch 103/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.9015\n",
      "Epoch 103: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 66ms/step - loss: 0.4834 - accuracy: 0.9015 - val_loss: 2.3528 - val_accuracy: 0.6416 - lr: 0.1000\n",
      "Epoch 104/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4949 - accuracy: 0.8977\n",
      "Epoch 104: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4949 - accuracy: 0.8977 - val_loss: 2.2909 - val_accuracy: 0.6364 - lr: 0.1000\n",
      "Epoch 105/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4774 - accuracy: 0.9040\n",
      "Epoch 105: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4774 - accuracy: 0.9040 - val_loss: 2.3563 - val_accuracy: 0.6286 - lr: 0.1000\n",
      "Epoch 106/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4796 - accuracy: 0.9036\n",
      "Epoch 106: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4796 - accuracy: 0.9036 - val_loss: 2.2427 - val_accuracy: 0.6472 - lr: 0.1000\n",
      "Epoch 107/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.9098\n",
      "Epoch 107: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4602 - accuracy: 0.9098 - val_loss: 2.2240 - val_accuracy: 0.6484 - lr: 0.1000\n",
      "Epoch 108/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.8971\n",
      "Epoch 108: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4993 - accuracy: 0.8971 - val_loss: 2.1904 - val_accuracy: 0.6442 - lr: 0.1000\n",
      "Epoch 109/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4652 - accuracy: 0.9078\n",
      "Epoch 109: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.4652 - accuracy: 0.9078 - val_loss: 2.1142 - val_accuracy: 0.6572 - lr: 0.1000\n",
      "Epoch 110/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4711 - accuracy: 0.9059\n",
      "Epoch 110: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4711 - accuracy: 0.9059 - val_loss: 2.3746 - val_accuracy: 0.6300 - lr: 0.1000\n",
      "Epoch 111/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4721 - accuracy: 0.9056\n",
      "Epoch 111: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4721 - accuracy: 0.9056 - val_loss: 2.4498 - val_accuracy: 0.6258 - lr: 0.1000\n",
      "Epoch 112/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4802 - accuracy: 0.9035\n",
      "Epoch 112: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4802 - accuracy: 0.9035 - val_loss: 2.1315 - val_accuracy: 0.6538 - lr: 0.1000\n",
      "Epoch 113/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4717 - accuracy: 0.9062\n",
      "Epoch 113: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4717 - accuracy: 0.9062 - val_loss: 2.2736 - val_accuracy: 0.6374 - lr: 0.1000\n",
      "Epoch 114/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.9065\n",
      "Epoch 114: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4747 - accuracy: 0.9065 - val_loss: 2.0036 - val_accuracy: 0.6594 - lr: 0.1000\n",
      "Epoch 115/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4549 - accuracy: 0.9112\n",
      "Epoch 115: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.4549 - accuracy: 0.9112 - val_loss: 2.2277 - val_accuracy: 0.6446 - lr: 0.1000\n",
      "Epoch 116/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.9134\n",
      "Epoch 116: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4451 - accuracy: 0.9134 - val_loss: 2.2309 - val_accuracy: 0.6466 - lr: 0.1000\n",
      "Epoch 117/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.9080\n",
      "Epoch 117: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4619 - accuracy: 0.9080 - val_loss: 2.3669 - val_accuracy: 0.6348 - lr: 0.1000\n",
      "Epoch 118/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.9093\n",
      "Epoch 118: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4624 - accuracy: 0.9093 - val_loss: 2.3972 - val_accuracy: 0.6190 - lr: 0.1000\n",
      "Epoch 119/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4702 - accuracy: 0.9069\n",
      "Epoch 119: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4702 - accuracy: 0.9069 - val_loss: 2.1708 - val_accuracy: 0.6424 - lr: 0.1000\n",
      "Epoch 120/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.9154\n",
      "Epoch 120: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.4469 - accuracy: 0.9154 - val_loss: 2.2689 - val_accuracy: 0.6376 - lr: 0.1000\n",
      "Epoch 121/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4518 - accuracy: 0.9113\n",
      "Epoch 121: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.4518 - accuracy: 0.9113 - val_loss: 2.2407 - val_accuracy: 0.6424 - lr: 0.1000\n",
      "Epoch 122/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4437 - accuracy: 0.9134\n",
      "Epoch 122: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 70ms/step - loss: 0.4437 - accuracy: 0.9134 - val_loss: 2.9275 - val_accuracy: 0.5842 - lr: 0.1000\n",
      "Epoch 123/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.9072\n",
      "Epoch 123: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4704 - accuracy: 0.9072 - val_loss: 2.3825 - val_accuracy: 0.6354 - lr: 0.1000\n",
      "Epoch 124/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.9163\n",
      "Epoch 124: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.4407 - accuracy: 0.9163 - val_loss: 2.3536 - val_accuracy: 0.6374 - lr: 0.1000\n",
      "Epoch 125/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4458 - accuracy: 0.9139\n",
      "Epoch 125: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 51s 72ms/step - loss: 0.4458 - accuracy: 0.9139 - val_loss: 2.2113 - val_accuracy: 0.6456 - lr: 0.1000\n",
      "Epoch 126/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.9170\n",
      "Epoch 126: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 51s 72ms/step - loss: 0.4352 - accuracy: 0.9170 - val_loss: 2.7297 - val_accuracy: 0.5948 - lr: 0.1000\n",
      "Epoch 127/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4446 - accuracy: 0.9137\n",
      "Epoch 127: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4446 - accuracy: 0.9137 - val_loss: 2.5974 - val_accuracy: 0.6254 - lr: 0.1000\n",
      "Epoch 128/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4519 - accuracy: 0.9109\n",
      "Epoch 128: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4519 - accuracy: 0.9109 - val_loss: 2.2215 - val_accuracy: 0.6544 - lr: 0.1000\n",
      "Epoch 129/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.9150\n",
      "Epoch 129: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4481 - accuracy: 0.9150 - val_loss: 2.1873 - val_accuracy: 0.6448 - lr: 0.1000\n",
      "Epoch 130/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.9195\n",
      "Epoch 130: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 72ms/step - loss: 0.4270 - accuracy: 0.9195 - val_loss: 2.4990 - val_accuracy: 0.6176 - lr: 0.1000\n",
      "Epoch 131/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.9192\n",
      "Epoch 131: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4277 - accuracy: 0.9192 - val_loss: 2.4929 - val_accuracy: 0.6340 - lr: 0.1000\n",
      "Epoch 132/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4360 - accuracy: 0.9184\n",
      "Epoch 132: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 51s 72ms/step - loss: 0.4360 - accuracy: 0.9184 - val_loss: 2.3654 - val_accuracy: 0.6552 - lr: 0.1000\n",
      "Epoch 133/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.9197\n",
      "Epoch 133: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4265 - accuracy: 0.9197 - val_loss: 2.4331 - val_accuracy: 0.6376 - lr: 0.1000\n",
      "Epoch 134/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.9197\n",
      "Epoch 134: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4241 - accuracy: 0.9197 - val_loss: 2.4051 - val_accuracy: 0.6384 - lr: 0.1000\n",
      "Epoch 135/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.9209\n",
      "Epoch 135: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.4189 - accuracy: 0.9209 - val_loss: 2.1833 - val_accuracy: 0.6556 - lr: 0.1000\n",
      "Epoch 136/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.9187\n",
      "Epoch 136: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.4279 - accuracy: 0.9187 - val_loss: 2.5625 - val_accuracy: 0.6248 - lr: 0.1000\n",
      "Epoch 137/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.9148\n",
      "Epoch 137: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4427 - accuracy: 0.9148 - val_loss: 2.4205 - val_accuracy: 0.6274 - lr: 0.1000\n",
      "Epoch 138/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.9232\n",
      "Epoch 138: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4144 - accuracy: 0.9232 - val_loss: 2.5452 - val_accuracy: 0.6278 - lr: 0.1000\n",
      "Epoch 139/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.9211\n",
      "Epoch 139: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.4270 - accuracy: 0.9211 - val_loss: 2.2989 - val_accuracy: 0.6474 - lr: 0.1000\n",
      "Epoch 140/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4209 - accuracy: 0.9208\n",
      "Epoch 140: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4209 - accuracy: 0.9208 - val_loss: 2.4416 - val_accuracy: 0.6458 - lr: 0.1000\n",
      "Epoch 141/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4147 - accuracy: 0.9235\n",
      "Epoch 141: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.4147 - accuracy: 0.9235 - val_loss: 2.3929 - val_accuracy: 0.6304 - lr: 0.1000\n",
      "Epoch 142/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 0.9160\n",
      "Epoch 142: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 72ms/step - loss: 0.4377 - accuracy: 0.9160 - val_loss: 2.1637 - val_accuracy: 0.6636 - lr: 0.1000\n",
      "Epoch 143/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.9246\n",
      "Epoch 143: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4129 - accuracy: 0.9246 - val_loss: 2.2774 - val_accuracy: 0.6502 - lr: 0.1000\n",
      "Epoch 144/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4209 - accuracy: 0.9214\n",
      "Epoch 144: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4209 - accuracy: 0.9214 - val_loss: 2.2996 - val_accuracy: 0.6594 - lr: 0.1000\n",
      "Epoch 145/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.9227\n",
      "Epoch 145: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 46s 66ms/step - loss: 0.4183 - accuracy: 0.9227 - val_loss: 2.2867 - val_accuracy: 0.6502 - lr: 0.1000\n",
      "Epoch 146/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4013 - accuracy: 0.9262\n",
      "Epoch 146: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 72ms/step - loss: 0.4013 - accuracy: 0.9262 - val_loss: 2.3275 - val_accuracy: 0.6464 - lr: 0.1000\n",
      "Epoch 147/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4163 - accuracy: 0.9218\n",
      "Epoch 147: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 66ms/step - loss: 0.4163 - accuracy: 0.9218 - val_loss: 2.4596 - val_accuracy: 0.6378 - lr: 0.1000\n",
      "Epoch 148/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4106 - accuracy: 0.9243\n",
      "Epoch 148: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4106 - accuracy: 0.9243 - val_loss: 2.4296 - val_accuracy: 0.6306 - lr: 0.1000\n",
      "Epoch 149/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.9162\n",
      "Epoch 149: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4362 - accuracy: 0.9162 - val_loss: 2.3270 - val_accuracy: 0.6394 - lr: 0.1000\n",
      "Epoch 150/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.9233\n",
      "Epoch 150: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4196 - accuracy: 0.9233 - val_loss: 2.1716 - val_accuracy: 0.6526 - lr: 0.1000\n",
      "Epoch 151/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3134 - accuracy: 0.9599\n",
      "Epoch 151: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.3134 - accuracy: 0.9599 - val_loss: 1.8878 - val_accuracy: 0.7030 - lr: 0.0100\n",
      "Epoch 152/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9765\n",
      "Epoch 152: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.2654 - accuracy: 0.9765 - val_loss: 1.8491 - val_accuracy: 0.7070 - lr: 0.0100\n",
      "Epoch 153/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9802\n",
      "Epoch 153: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.2489 - accuracy: 0.9802 - val_loss: 1.8601 - val_accuracy: 0.7058 - lr: 0.0100\n",
      "Epoch 154/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 0.9834\n",
      "Epoch 154: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.2357 - accuracy: 0.9834 - val_loss: 1.8577 - val_accuracy: 0.7090 - lr: 0.0100\n",
      "Epoch 155/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2264 - accuracy: 0.9846\n",
      "Epoch 155: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.2264 - accuracy: 0.9846 - val_loss: 1.8553 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 156/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2144 - accuracy: 0.9874\n",
      "Epoch 156: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.2144 - accuracy: 0.9874 - val_loss: 1.8433 - val_accuracy: 0.7108 - lr: 0.0100\n",
      "Epoch 157/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.9875\n",
      "Epoch 157: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.2094 - accuracy: 0.9875 - val_loss: 1.8710 - val_accuracy: 0.7100 - lr: 0.0100\n",
      "Epoch 158/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9884\n",
      "Epoch 158: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.2020 - accuracy: 0.9884 - val_loss: 1.8588 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 159/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9891\n",
      "Epoch 159: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1950 - accuracy: 0.9891 - val_loss: 1.8626 - val_accuracy: 0.7088 - lr: 0.0100\n",
      "Epoch 160/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.9906\n",
      "Epoch 160: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1883 - accuracy: 0.9906 - val_loss: 1.8542 - val_accuracy: 0.7136 - lr: 0.0100\n",
      "Epoch 161/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9902\n",
      "Epoch 161: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.1828 - accuracy: 0.9902 - val_loss: 1.8694 - val_accuracy: 0.7122 - lr: 0.0100\n",
      "Epoch 162/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9908\n",
      "Epoch 162: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.1786 - accuracy: 0.9908 - val_loss: 1.8501 - val_accuracy: 0.7104 - lr: 0.0100\n",
      "Epoch 163/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9913\n",
      "Epoch 163: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.1731 - accuracy: 0.9913 - val_loss: 1.8583 - val_accuracy: 0.7126 - lr: 0.0100\n",
      "Epoch 164/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9918\n",
      "Epoch 164: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1680 - accuracy: 0.9918 - val_loss: 1.8519 - val_accuracy: 0.7110 - lr: 0.0100\n",
      "Epoch 165/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1636 - accuracy: 0.9923\n",
      "Epoch 165: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.1636 - accuracy: 0.9923 - val_loss: 1.8576 - val_accuracy: 0.7142 - lr: 0.0100\n",
      "Epoch 166/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9914\n",
      "Epoch 166: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.1601 - accuracy: 0.9914 - val_loss: 1.8677 - val_accuracy: 0.7144 - lr: 0.0100\n",
      "Epoch 167/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9916\n",
      "Epoch 167: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1559 - accuracy: 0.9916 - val_loss: 1.8820 - val_accuracy: 0.7154 - lr: 0.0100\n",
      "Epoch 168/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1523 - accuracy: 0.9915\n",
      "Epoch 168: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.1523 - accuracy: 0.9915 - val_loss: 1.8787 - val_accuracy: 0.7128 - lr: 0.0100\n",
      "Epoch 169/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.9923\n",
      "Epoch 169: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1477 - accuracy: 0.9923 - val_loss: 1.8855 - val_accuracy: 0.7136 - lr: 0.0100\n",
      "Epoch 170/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9928\n",
      "Epoch 170: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.1440 - accuracy: 0.9928 - val_loss: 1.8895 - val_accuracy: 0.7134 - lr: 0.0100\n",
      "Epoch 171/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9929\n",
      "Epoch 171: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1403 - accuracy: 0.9929 - val_loss: 1.8792 - val_accuracy: 0.7152 - lr: 0.0100\n",
      "Epoch 172/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9926\n",
      "Epoch 172: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.1375 - accuracy: 0.9926 - val_loss: 1.8895 - val_accuracy: 0.7130 - lr: 0.0100\n",
      "Epoch 173/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9938\n",
      "Epoch 173: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 46s 66ms/step - loss: 0.1334 - accuracy: 0.9938 - val_loss: 1.8768 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 174/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9937\n",
      "Epoch 174: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.1291 - accuracy: 0.9937 - val_loss: 1.8926 - val_accuracy: 0.7132 - lr: 0.0100\n",
      "Epoch 175/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9939\n",
      "Epoch 175: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 46s 66ms/step - loss: 0.1276 - accuracy: 0.9939 - val_loss: 1.8890 - val_accuracy: 0.7152 - lr: 0.0100\n",
      "Epoch 176/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9933\n",
      "Epoch 176: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.1246 - accuracy: 0.9933 - val_loss: 1.8993 - val_accuracy: 0.7168 - lr: 0.0100\n",
      "Epoch 177/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9934\n",
      "Epoch 177: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.1212 - accuracy: 0.9934 - val_loss: 1.9304 - val_accuracy: 0.7086 - lr: 0.0100\n",
      "Epoch 178/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9936\n",
      "Epoch 178: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1185 - accuracy: 0.9936 - val_loss: 1.8989 - val_accuracy: 0.7142 - lr: 0.0100\n",
      "Epoch 179/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9935\n",
      "Epoch 179: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1164 - accuracy: 0.9935 - val_loss: 1.8970 - val_accuracy: 0.7150 - lr: 0.0100\n",
      "Epoch 180/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9937\n",
      "Epoch 180: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1140 - accuracy: 0.9937 - val_loss: 1.9052 - val_accuracy: 0.7132 - lr: 0.0100\n",
      "Epoch 1/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9957\n",
      "Epoch 1: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 36s 47ms/step - loss: 0.1059 - accuracy: 0.9957 - val_loss: 1.9041 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "Epoch 2/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1050 - accuracy: 0.9962\n",
      "Epoch 2: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1050 - accuracy: 0.9962 - val_loss: 1.9105 - val_accuracy: 0.7120 - lr: 0.0100\n",
      "Epoch 3/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9961\n",
      "Epoch 3: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1051 - accuracy: 0.9961 - val_loss: 1.9066 - val_accuracy: 0.7120 - lr: 0.0100\n",
      "Epoch 4/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9962\n",
      "Epoch 4: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.1047 - accuracy: 0.9962 - val_loss: 1.9102 - val_accuracy: 0.7120 - lr: 0.0100\n",
      "Epoch 5/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9962\n",
      "Epoch 5: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1050 - accuracy: 0.9962 - val_loss: 1.9171 - val_accuracy: 0.7126 - lr: 0.0100\n",
      "Epoch 6/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9967\n",
      "Epoch 6: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1034 - accuracy: 0.9967 - val_loss: 1.9206 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "Epoch 7/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9963\n",
      "Epoch 7: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.1038 - accuracy: 0.9964 - val_loss: 1.9179 - val_accuracy: 0.7140 - lr: 0.0100\n",
      "Epoch 8/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9963\n",
      "Epoch 8: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1043 - accuracy: 0.9963 - val_loss: 1.9245 - val_accuracy: 0.7144 - lr: 0.0100\n",
      "Epoch 9/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9962\n",
      "Epoch 9: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.1038 - accuracy: 0.9962 - val_loss: 1.9351 - val_accuracy: 0.7122 - lr: 0.0100\n",
      "Epoch 10/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9963\n",
      "Epoch 10: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1042 - accuracy: 0.9963 - val_loss: 1.9306 - val_accuracy: 0.7120 - lr: 0.0100\n",
      "Epoch 11/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9959\n",
      "Epoch 11: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1041 - accuracy: 0.9959 - val_loss: 1.9338 - val_accuracy: 0.7132 - lr: 0.0100\n",
      "Epoch 12/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9965\n",
      "Epoch 12: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1031 - accuracy: 0.9966 - val_loss: 1.9353 - val_accuracy: 0.7124 - lr: 0.0100\n",
      "Epoch 13/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9963\n",
      "Epoch 13: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.1032 - accuracy: 0.9963 - val_loss: 1.9386 - val_accuracy: 0.7126 - lr: 0.0100\n",
      "Epoch 14/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9966\n",
      "Epoch 14: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1035 - accuracy: 0.9966 - val_loss: 1.9416 - val_accuracy: 0.7124 - lr: 0.0100\n",
      "Epoch 15/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9968\n",
      "Epoch 15: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1028 - accuracy: 0.9968 - val_loss: 1.9426 - val_accuracy: 0.7138 - lr: 0.0100\n",
      "Epoch 16/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1029 - accuracy: 0.9966\n",
      "Epoch 16: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1030 - accuracy: 0.9966 - val_loss: 1.9417 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 17/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9959\n",
      "Epoch 17: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1038 - accuracy: 0.9959 - val_loss: 1.9529 - val_accuracy: 0.7132 - lr: 0.0100\n",
      "Epoch 18/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9966\n",
      "Epoch 18: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.1031 - accuracy: 0.9966 - val_loss: 1.9533 - val_accuracy: 0.7134 - lr: 0.0100\n",
      "Epoch 19/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.9961\n",
      "Epoch 19: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1043 - accuracy: 0.9961 - val_loss: 1.9534 - val_accuracy: 0.7136 - lr: 0.0100\n",
      "Epoch 20/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9968\n",
      "Epoch 20: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1020 - accuracy: 0.9968 - val_loss: 1.9614 - val_accuracy: 0.7126 - lr: 0.0100\n",
      "Epoch 21/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9964\n",
      "Epoch 21: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.1033 - accuracy: 0.9964 - val_loss: 1.9664 - val_accuracy: 0.7132 - lr: 0.0100\n",
      "Epoch 22/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1033 - accuracy: 0.9961\n",
      "Epoch 22: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1033 - accuracy: 0.9961 - val_loss: 1.9630 - val_accuracy: 0.7132 - lr: 0.0100\n",
      "Epoch 23/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9967\n",
      "Epoch 23: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1022 - accuracy: 0.9967 - val_loss: 1.9642 - val_accuracy: 0.7114 - lr: 0.0100\n",
      "Epoch 24/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9961\n",
      "Epoch 24: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1038 - accuracy: 0.9961 - val_loss: 1.9704 - val_accuracy: 0.7126 - lr: 0.0100\n",
      "Epoch 25/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9970\n",
      "Epoch 25: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1021 - accuracy: 0.9970 - val_loss: 1.9730 - val_accuracy: 0.7132 - lr: 0.0100\n",
      "Epoch 26/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1023 - accuracy: 0.9968\n",
      "Epoch 26: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1023 - accuracy: 0.9968 - val_loss: 1.9711 - val_accuracy: 0.7126 - lr: 0.0100\n",
      "Epoch 27/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9963\n",
      "Epoch 27: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1026 - accuracy: 0.9963 - val_loss: 1.9799 - val_accuracy: 0.7106 - lr: 0.0100\n",
      "Epoch 28/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9962\n",
      "Epoch 28: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1037 - accuracy: 0.9962 - val_loss: 1.9791 - val_accuracy: 0.7098 - lr: 0.0100\n",
      "Epoch 29/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9967\n",
      "Epoch 29: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.1024 - accuracy: 0.9967 - val_loss: 1.9856 - val_accuracy: 0.7122 - lr: 0.0100\n",
      "Epoch 30/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9967\n",
      "Epoch 30: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1019 - accuracy: 0.9967 - val_loss: 1.9861 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 31/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9964\n",
      "Epoch 31: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1024 - accuracy: 0.9964 - val_loss: 1.9857 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 32/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9960\n",
      "Epoch 32: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.1028 - accuracy: 0.9960 - val_loss: 1.9906 - val_accuracy: 0.7094 - lr: 0.0100\n",
      "Epoch 33/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9971\n",
      "Epoch 33: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.1016 - accuracy: 0.9971 - val_loss: 1.9908 - val_accuracy: 0.7104 - lr: 0.0100\n",
      "Epoch 34/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1021 - accuracy: 0.9966\n",
      "Epoch 34: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1021 - accuracy: 0.9966 - val_loss: 1.9934 - val_accuracy: 0.7114 - lr: 0.0100\n",
      "Epoch 35/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1022 - accuracy: 0.9965\n",
      "Epoch 35: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1022 - accuracy: 0.9965 - val_loss: 1.9984 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 36/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9961\n",
      "Epoch 36: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1022 - accuracy: 0.9961 - val_loss: 1.9988 - val_accuracy: 0.7120 - lr: 0.0100\n",
      "Epoch 37/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9969\n",
      "Epoch 37: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.1010 - accuracy: 0.9969 - val_loss: 2.0005 - val_accuracy: 0.7124 - lr: 0.0100\n",
      "Epoch 38/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9969\n",
      "Epoch 38: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1015 - accuracy: 0.9969 - val_loss: 2.0012 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "Epoch 39/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9964\n",
      "Epoch 39: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1020 - accuracy: 0.9964 - val_loss: 2.0044 - val_accuracy: 0.7124 - lr: 0.0100\n",
      "Epoch 40/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9966\n",
      "Epoch 40: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1023 - accuracy: 0.9966 - val_loss: 2.0094 - val_accuracy: 0.7114 - lr: 0.0100\n",
      "Epoch 41/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9969\n",
      "Epoch 41: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.1002 - accuracy: 0.9969 - val_loss: 2.0103 - val_accuracy: 0.7132 - lr: 0.0100\n",
      "Epoch 42/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1023 - accuracy: 0.9963\n",
      "Epoch 42: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.1023 - accuracy: 0.9963 - val_loss: 2.0150 - val_accuracy: 0.7106 - lr: 0.0100\n",
      "Epoch 43/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9964\n",
      "Epoch 43: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1024 - accuracy: 0.9964 - val_loss: 2.0149 - val_accuracy: 0.7126 - lr: 0.0100\n",
      "Epoch 44/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9965\n",
      "Epoch 44: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.1015 - accuracy: 0.9965 - val_loss: 2.0168 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 45/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9969\n",
      "Epoch 45: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1013 - accuracy: 0.9969 - val_loss: 2.0195 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 46/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9969\n",
      "Epoch 46: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1003 - accuracy: 0.9969 - val_loss: 2.0147 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 47/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1008 - accuracy: 0.9967\n",
      "Epoch 47: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1009 - accuracy: 0.9967 - val_loss: 2.0202 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 48/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9969\n",
      "Epoch 48: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.1004 - accuracy: 0.9969 - val_loss: 2.0107 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 49/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9963\n",
      "Epoch 49: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1016 - accuracy: 0.9963 - val_loss: 2.0173 - val_accuracy: 0.7110 - lr: 0.0010\n",
      "Epoch 50/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9966\n",
      "Epoch 50: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1013 - accuracy: 0.9966 - val_loss: 2.0197 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 51/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9970\n",
      "Epoch 51: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1008 - accuracy: 0.9970 - val_loss: 2.0198 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 52/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9970\n",
      "Epoch 52: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.1006 - accuracy: 0.9970 - val_loss: 2.0160 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 53/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9964\n",
      "Epoch 53: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1014 - accuracy: 0.9964 - val_loss: 2.0181 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 54/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9968\n",
      "Epoch 54: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1013 - accuracy: 0.9968 - val_loss: 2.0217 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 55/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9968\n",
      "Epoch 55: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1010 - accuracy: 0.9968 - val_loss: 2.0184 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 56/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9971\n",
      "Epoch 56: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.1006 - accuracy: 0.9971 - val_loss: 2.0170 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 57/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9967\n",
      "Epoch 57: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1005 - accuracy: 0.9967 - val_loss: 2.0201 - val_accuracy: 0.7128 - lr: 0.0010\n",
      "Epoch 58/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1018 - accuracy: 0.9966\n",
      "Epoch 58: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1018 - accuracy: 0.9966 - val_loss: 2.0226 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 59/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9965\n",
      "Epoch 59: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1014 - accuracy: 0.9965 - val_loss: 2.0143 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 60/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9967\n",
      "Epoch 60: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.1011 - accuracy: 0.9967 - val_loss: 2.0225 - val_accuracy: 0.7128 - lr: 0.0010\n",
      "Epoch 61/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9964\n",
      "Epoch 61: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1016 - accuracy: 0.9964 - val_loss: 2.0193 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 62/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9966\n",
      "Epoch 62: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.1013 - accuracy: 0.9966 - val_loss: 2.0176 - val_accuracy: 0.7142 - lr: 0.0010\n",
      "Epoch 63/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9966\n",
      "Epoch 63: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.1017 - accuracy: 0.9966 - val_loss: 2.0296 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 64/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9966\n",
      "Epoch 64: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1008 - accuracy: 0.9966 - val_loss: 2.0180 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 65/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9968\n",
      "Epoch 65: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1010 - accuracy: 0.9968 - val_loss: 2.0224 - val_accuracy: 0.7128 - lr: 0.0010\n",
      "Epoch 66/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9969\n",
      "Epoch 66: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1003 - accuracy: 0.9969 - val_loss: 2.0189 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 67/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9967\n",
      "Epoch 67: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.1008 - accuracy: 0.9967 - val_loss: 2.0200 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 68/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0996 - accuracy: 0.9975\n",
      "Epoch 68: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.0996 - accuracy: 0.9975 - val_loss: 2.0199 - val_accuracy: 0.7146 - lr: 0.0010\n",
      "Epoch 69/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9969\n",
      "Epoch 69: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1006 - accuracy: 0.9969 - val_loss: 2.0180 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 70/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9968\n",
      "Epoch 70: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1009 - accuracy: 0.9968 - val_loss: 2.0176 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 71/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9965\n",
      "Epoch 71: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 34s 49ms/step - loss: 0.1014 - accuracy: 0.9965 - val_loss: 2.0223 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 72/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9970\n",
      "Epoch 72: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1012 - accuracy: 0.9970 - val_loss: 2.0207 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 73/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9964\n",
      "Epoch 73: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1017 - accuracy: 0.9964 - val_loss: 2.0217 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 74/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9968\n",
      "Epoch 74: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1010 - accuracy: 0.9968 - val_loss: 2.0256 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 75/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9972\n",
      "Epoch 75: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 35s 49ms/step - loss: 0.1002 - accuracy: 0.9972 - val_loss: 2.0238 - val_accuracy: 0.7128 - lr: 0.0010\n",
      "Epoch 76/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9966\n",
      "Epoch 76: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1014 - accuracy: 0.9966 - val_loss: 2.0247 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 77/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9967\n",
      "Epoch 77: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1012 - accuracy: 0.9967 - val_loss: 2.0225 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 78/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9967\n",
      "Epoch 78: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 26s 38ms/step - loss: 0.1009 - accuracy: 0.9967 - val_loss: 2.0237 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 79/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1002 - accuracy: 0.9970\n",
      "Epoch 79: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.1002 - accuracy: 0.9970 - val_loss: 2.0221 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 80/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9970\n",
      "Epoch 80: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.1004 - accuracy: 0.9970 - val_loss: 2.0246 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 81/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9966\n",
      "Epoch 81: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1012 - accuracy: 0.9966 - val_loss: 2.0232 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 82/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9970\n",
      "Epoch 82: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1007 - accuracy: 0.9970 - val_loss: 2.0214 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 83/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1019 - accuracy: 0.9963\n",
      "Epoch 83: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.1019 - accuracy: 0.9963 - val_loss: 2.0231 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 84/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9968\n",
      "Epoch 84: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1006 - accuracy: 0.9968 - val_loss: 2.0231 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 85/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9966\n",
      "Epoch 85: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1003 - accuracy: 0.9966 - val_loss: 2.0243 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 86/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9969\n",
      "Epoch 86: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1008 - accuracy: 0.9969 - val_loss: 2.0291 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 87/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1004 - accuracy: 0.9971\n",
      "Epoch 87: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.1004 - accuracy: 0.9971 - val_loss: 2.0239 - val_accuracy: 0.7146 - lr: 0.0010\n",
      "Epoch 88/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9970\n",
      "Epoch 88: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1004 - accuracy: 0.9970 - val_loss: 2.0206 - val_accuracy: 0.7142 - lr: 0.0010\n",
      "Epoch 89/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9968\n",
      "Epoch 89: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1003 - accuracy: 0.9968 - val_loss: 2.0274 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 90/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9970\n",
      "Epoch 90: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1007 - accuracy: 0.9970 - val_loss: 2.0282 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 91/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9965\n",
      "Epoch 91: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1011 - accuracy: 0.9965 - val_loss: 2.0236 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 92/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9972\n",
      "Epoch 92: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1003 - accuracy: 0.9972 - val_loss: 2.0224 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 93/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9972\n",
      "Epoch 93: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.1007 - accuracy: 0.9972 - val_loss: 2.0268 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 94/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9967\n",
      "Epoch 94: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.1006 - accuracy: 0.9967 - val_loss: 2.0255 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 95/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9970\n",
      "Epoch 95: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.1006 - accuracy: 0.9970 - val_loss: 2.0280 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 96/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9968\n",
      "Epoch 96: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1003 - accuracy: 0.9968 - val_loss: 2.0265 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 97/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1008 - accuracy: 0.9968\n",
      "Epoch 97: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.1008 - accuracy: 0.9968 - val_loss: 2.0266 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 98/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1014 - accuracy: 0.9964\n",
      "Epoch 98: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1014 - accuracy: 0.9964 - val_loss: 2.0271 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 99/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9968\n",
      "Epoch 99: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.1011 - accuracy: 0.9968 - val_loss: 2.0247 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 100/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9969\n",
      "Epoch 100: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1006 - accuracy: 0.9969 - val_loss: 2.0280 - val_accuracy: 0.7144 - lr: 0.0010\n",
      "Epoch 101/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9967\n",
      "Epoch 101: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1009 - accuracy: 0.9967 - val_loss: 2.0252 - val_accuracy: 0.7144 - lr: 0.0010\n",
      "Epoch 102/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1015 - accuracy: 0.9964\n",
      "Epoch 102: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.1015 - accuracy: 0.9964 - val_loss: 2.0265 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 103/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9965\n",
      "Epoch 103: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1012 - accuracy: 0.9965 - val_loss: 2.0287 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 104/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9969\n",
      "Epoch 104: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1006 - accuracy: 0.9969 - val_loss: 2.0310 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 105/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9964\n",
      "Epoch 105: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.1011 - accuracy: 0.9964 - val_loss: 2.0284 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 106/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9969\n",
      "Epoch 106: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 35s 49ms/step - loss: 0.1003 - accuracy: 0.9969 - val_loss: 2.0306 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 107/108\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9969\n",
      "Epoch 107: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.1006 - accuracy: 0.9969 - val_loss: 2.0325 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 108/108\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9968\n",
      "Epoch 108: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1014 - accuracy: 0.9968 - val_loss: 2.0291 - val_accuracy: 0.7128 - lr: 0.0010\n",
      "Current:  142\n",
      "Epoch 1/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0949 - accuracy: 0.9982\n",
      "Epoch 1: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 30s 38ms/step - loss: 0.0949 - accuracy: 0.9982 - val_loss: 2.0291 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 2/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0951 - accuracy: 0.9986\n",
      "Epoch 2: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0951 - accuracy: 0.9986 - val_loss: 2.0292 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 3/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9984\n",
      "Epoch 3: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.0950 - accuracy: 0.9984 - val_loss: 2.0294 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 4/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9983\n",
      "Epoch 4: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0953 - accuracy: 0.9983 - val_loss: 2.0292 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 5/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.9981\n",
      "Epoch 5: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 31s 45ms/step - loss: 0.0954 - accuracy: 0.9981 - val_loss: 2.0294 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 6/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9980\n",
      "Epoch 6: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0956 - accuracy: 0.9980 - val_loss: 2.0294 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 7/12\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0951 - accuracy: 0.9985\n",
      "Epoch 7: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0951 - accuracy: 0.9985 - val_loss: 2.0294 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 8/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9985\n",
      "Epoch 8: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0952 - accuracy: 0.9985 - val_loss: 2.0295 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 9/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9982\n",
      "Epoch 9: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0955 - accuracy: 0.9982 - val_loss: 2.0296 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 10/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9980\n",
      "Epoch 10: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.0959 - accuracy: 0.9980 - val_loss: 2.0294 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 11/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9981\n",
      "Epoch 11: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0952 - accuracy: 0.9981 - val_loss: 2.0294 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 12/12\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9980\n",
      "Epoch 12: val_loss did not improve from 1.61222\n",
      "703/703 [==============================] - 26s 38ms/step - loss: 0.0959 - accuracy: 0.9980 - val_loss: 2.0293 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Current:  154\n",
      "313/313 [==============================] - 5s 11ms/step\n",
      "Accuracy: 70.13000000000001\n",
      "Error: 29.86999999999999\n",
      "ECE: 0.21113096915334467\n",
      "MCE: 0.49416474651951364\n",
      "Loss: 1.9960648885787395\n",
      "brier: 0.2746932363577274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[29.86999999999999,\n",
       " 0.21113096915334467,\n",
       " 0.49416474651951364,\n",
       " 1.9960648885787395,\n",
       " 0.2746932363577274]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, img_gen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list,batch_size=batch_size,lr_schedule = [[0, 0.1],[nb_epoch*0.5,0.01],[nb_epoch*0.75,0.001]], cbks=[checkpointer],name='dense_cifar100_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16204.846911,
   "end_time": "2023-04-24T19:38:48.515285",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-24T15:08:43.668374",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
