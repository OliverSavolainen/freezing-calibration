{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5098339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T09:34:51.143186Z",
     "iopub.status.busy": "2023-04-22T09:34:51.142130Z",
     "iopub.status.idle": "2023-04-22T09:34:58.879570Z",
     "shell.execute_reply": "2023-04-22T09:34:58.878422Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1682109576846,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "bpCwuvLginNe",
    "papermill": {
     "duration": 7.745734,
     "end_time": "2023-04-22T09:34:58.882296",
     "exception": false,
     "start_time": "2023-04-22T09:34:51.136562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import Input\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3c8590",
   "metadata": {
    "_cell_guid": "d714c626-b790-420e-b76b-1f7aab627dc7",
    "_uuid": "46386508-5d86-48fc-86f1-04e2626bb2f5",
    "execution": {
     "iopub.execute_input": "2023-04-22T09:34:58.890779Z",
     "iopub.status.busy": "2023-04-22T09:34:58.890188Z",
     "iopub.status.idle": "2023-04-22T09:34:58.935882Z",
     "shell.execute_reply": "2023-04-22T09:34:58.934968Z"
    },
    "executionInfo": {
     "elapsed": 6101,
     "status": "ok",
     "timestamp": 1682109576845,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "4vOgQ5fvinNh",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0524,
     "end_time": "2023-04-22T09:34:58.937990",
     "exception": false,
     "start_time": "2023-04-22T09:34:58.885590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import freezing\n",
    "import wrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a025f15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T09:34:58.980501Z",
     "iopub.status.busy": "2023-04-22T09:34:58.980211Z",
     "iopub.status.idle": "2023-04-22T09:34:58.986890Z",
     "shell.execute_reply": "2023-04-22T09:34:58.985885Z"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682109576847,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "eFW7ePv8inNn",
    "papermill": {
     "duration": 0.012703,
     "end_time": "2023-04-22T09:34:58.988948",
     "exception": false,
     "start_time": "2023-04-22T09:34:58.976245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "depth              = 34\n",
    "growth_rate        = 10  # Growth factor\n",
    "n                  = (depth-4)//6  # N, number of blocks in one group\n",
    "num_classes        = 100\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels       = 3\n",
    "batch_size         = 128\n",
    "epochs             = 200\n",
    "iterations         = 45000 // batch_size\n",
    "weight_decay       = 0.0005\n",
    "seed = 333\n",
    "def color_preprocessing(x_train, x_val, x_test):\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')    \n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    mean = np.mean(x_train, axis=(0,1,2))  # Per channel mean\n",
    "    std = np.std(x_train, axis=(0,1,2))\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_val = (x_val - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "    \n",
    "    return x_train, x_val, x_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d42f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T09:34:58.996114Z",
     "iopub.status.busy": "2023-04-22T09:34:58.995524Z",
     "iopub.status.idle": "2023-04-22T09:35:08.903078Z",
     "shell.execute_reply": "2023-04-22T09:35:08.901859Z"
    },
    "executionInfo": {
     "elapsed": 10712,
     "status": "ok",
     "timestamp": 1682109777593,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "oghEV45AinNo",
    "outputId": "a00812e9-f46c-4548-dec2-5af843f1ec31",
    "papermill": {
     "duration": 9.913484,
     "end_time": "2023-04-22T09:35:08.905350",
     "exception": false,
     "start_time": "2023-04-22T09:34:58.991866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# color preprocessing\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "x_train45, x_val, x_test = color_preprocessing(x_train45, x_val, x_test)\n",
    "\n",
    "y_train45 = keras.utils.to_categorical(y_train45, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# build network\n",
    "img_input = Input(shape=(img_rows,img_cols,img_channels))    \n",
    "model = wrn.create_wide_residual_network(img_input, nb_classes=num_classes, N=n, k=growth_rate, dropout=0.0)\n",
    "print(model.summary())\n",
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "\n",
    "# set callback\n",
    "checkpointer = ModelCheckpoint('model_wide_28_10_c100_best.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "# set data augmentation\n",
    "print('Using real-time data augmentation.')\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "        width_shift_range=0.125,height_shift_range=0.125,fill_mode='reflect') # Missing pixels replaced with reflections\n",
    "\n",
    "datagen.fit(x_train45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d20cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T09:35:08.948273Z",
     "iopub.status.busy": "2023-04-22T09:35:08.947427Z",
     "iopub.status.idle": "2023-04-22T09:35:08.975113Z",
     "shell.execute_reply": "2023-04-22T09:35:08.973722Z"
    },
    "papermill": {
     "duration": 0.053052,
     "end_time": "2023-04-22T09:35:08.979195",
     "exception": false,
     "start_time": "2023-04-22T09:35:08.926143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.8:\n",
    "    freezing_list.append(int(epochs*0.6))\n",
    "freezing_list.append(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c002a3",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-04-22T09:35:09.092631Z",
     "iopub.status.busy": "2023-04-22T09:35:09.091965Z",
     "iopub.status.idle": "2023-04-22T15:35:09.081342Z",
     "shell.execute_reply": "2023-04-22T15:35:09.080187Z"
    },
    "papermill": {
     "duration": 21600.049479,
     "end_time": "2023-04-22T15:35:09.083642",
     "exception": false,
     "start_time": "2023-04-22T09:35:09.034163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-22 09:35:09        53368\n",
      "variables.h5                                   2023-04-22 09:35:09    185249224\n",
      "metadata.json                                  2023-04-22 09:35:09           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-22 09:35:08        53368\n",
      "variables.h5                                   2023-04-22 09:35:08    185249224\n",
      "metadata.json                                  2023-04-22 09:35:08           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.0225 - accuracy: 0.0802\n",
      "Epoch 1: val_loss improved from inf to 3.52313, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 136s 364ms/step - loss: 4.0225 - accuracy: 0.0802 - val_loss: 3.5231 - val_accuracy: 0.1454 - lr: 0.1000\n",
      "Epoch 2/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1029 - accuracy: 0.2173\n",
      "Epoch 2: val_loss improved from 3.52313 to 2.64885, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 3.1029 - accuracy: 0.2173 - val_loss: 2.6489 - val_accuracy: 0.3024 - lr: 0.1000\n",
      "Epoch 3/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4494 - accuracy: 0.3456\n",
      "Epoch 3: val_loss improved from 2.64885 to 2.21338, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 2.4494 - accuracy: 0.3456 - val_loss: 2.2134 - val_accuracy: 0.4090 - lr: 0.1000\n",
      "Epoch 4/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0508 - accuracy: 0.4341\n",
      "Epoch 4: val_loss improved from 2.21338 to 1.93971, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 2.0508 - accuracy: 0.4341 - val_loss: 1.9397 - val_accuracy: 0.4614 - lr: 0.1000\n",
      "Epoch 5/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7860 - accuracy: 0.4984\n",
      "Epoch 5: val_loss improved from 1.93971 to 1.74278, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 1.7860 - accuracy: 0.4984 - val_loss: 1.7428 - val_accuracy: 0.5138 - lr: 0.1000\n",
      "Epoch 6/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5689 - accuracy: 0.5555\n",
      "Epoch 6: val_loss improved from 1.74278 to 1.62116, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 1.5689 - accuracy: 0.5555 - val_loss: 1.6212 - val_accuracy: 0.5546 - lr: 0.1000\n",
      "Epoch 7/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4084 - accuracy: 0.5946\n",
      "Epoch 7: val_loss improved from 1.62116 to 1.44109, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 1.4084 - accuracy: 0.5946 - val_loss: 1.4411 - val_accuracy: 0.6000 - lr: 0.1000\n",
      "Epoch 8/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2574 - accuracy: 0.6356\n",
      "Epoch 8: val_loss improved from 1.44109 to 1.41818, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 1.2574 - accuracy: 0.6356 - val_loss: 1.4182 - val_accuracy: 0.6056 - lr: 0.1000\n",
      "Epoch 9/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1346 - accuracy: 0.6653\n",
      "Epoch 9: val_loss improved from 1.41818 to 1.36724, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 1.1346 - accuracy: 0.6653 - val_loss: 1.3672 - val_accuracy: 0.6250 - lr: 0.1000\n",
      "Epoch 10/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0302 - accuracy: 0.6939\n",
      "Epoch 10: val_loss did not improve from 1.36724\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 1.0302 - accuracy: 0.6939 - val_loss: 1.3787 - val_accuracy: 0.6232 - lr: 0.1000\n",
      "Epoch 11/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9328 - accuracy: 0.7180\n",
      "Epoch 11: val_loss improved from 1.36724 to 1.27578, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.9328 - accuracy: 0.7180 - val_loss: 1.2758 - val_accuracy: 0.6476 - lr: 0.1000\n",
      "Epoch 12/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8496 - accuracy: 0.7439\n",
      "Epoch 12: val_loss did not improve from 1.27578\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.8496 - accuracy: 0.7439 - val_loss: 1.2969 - val_accuracy: 0.6632 - lr: 0.1000\n",
      "Epoch 13/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7642 - accuracy: 0.7660\n",
      "Epoch 13: val_loss did not improve from 1.27578\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.7642 - accuracy: 0.7660 - val_loss: 1.3105 - val_accuracy: 0.6578 - lr: 0.1000\n",
      "Epoch 14/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.7847\n",
      "Epoch 14: val_loss did not improve from 1.27578\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.6951 - accuracy: 0.7847 - val_loss: 1.3028 - val_accuracy: 0.6650 - lr: 0.1000\n",
      "Epoch 15/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6259 - accuracy: 0.8037\n",
      "Epoch 15: val_loss did not improve from 1.27578\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.6259 - accuracy: 0.8037 - val_loss: 1.2906 - val_accuracy: 0.6710 - lr: 0.1000\n",
      "Epoch 16/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5594 - accuracy: 0.8241\n",
      "Epoch 16: val_loss improved from 1.27578 to 1.27439, saving model to model_wide_28_10_c100_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.5594 - accuracy: 0.8241 - val_loss: 1.2744 - val_accuracy: 0.6870 - lr: 0.1000\n",
      "Epoch 17/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5049 - accuracy: 0.8390\n",
      "Epoch 17: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.5049 - accuracy: 0.8390 - val_loss: 1.2985 - val_accuracy: 0.6772 - lr: 0.1000\n",
      "Epoch 18/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4583 - accuracy: 0.8535\n",
      "Epoch 18: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.4583 - accuracy: 0.8535 - val_loss: 1.3079 - val_accuracy: 0.6894 - lr: 0.1000\n",
      "Epoch 19/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4050 - accuracy: 0.8686\n",
      "Epoch 19: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.4050 - accuracy: 0.8686 - val_loss: 1.3625 - val_accuracy: 0.6820 - lr: 0.1000\n",
      "Epoch 20/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3652 - accuracy: 0.8801\n",
      "Epoch 20: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.3652 - accuracy: 0.8801 - val_loss: 1.3782 - val_accuracy: 0.6886 - lr: 0.1000\n",
      "Epoch 21/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3267 - accuracy: 0.8940\n",
      "Epoch 21: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.3267 - accuracy: 0.8940 - val_loss: 1.4439 - val_accuracy: 0.6826 - lr: 0.1000\n",
      "Epoch 22/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.9027\n",
      "Epoch 22: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.2985 - accuracy: 0.9027 - val_loss: 1.4570 - val_accuracy: 0.6872 - lr: 0.1000\n",
      "Epoch 23/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.9144\n",
      "Epoch 23: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.2597 - accuracy: 0.9144 - val_loss: 1.4071 - val_accuracy: 0.6994 - lr: 0.1000\n",
      "Epoch 24/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9247\n",
      "Epoch 24: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.2282 - accuracy: 0.9247 - val_loss: 1.4689 - val_accuracy: 0.6890 - lr: 0.1000\n",
      "Epoch 25/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2088 - accuracy: 0.9309\n",
      "Epoch 25: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.2088 - accuracy: 0.9309 - val_loss: 1.5288 - val_accuracy: 0.6950 - lr: 0.1000\n",
      "Epoch 26/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9344\n",
      "Epoch 26: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1973 - accuracy: 0.9344 - val_loss: 1.4998 - val_accuracy: 0.6926 - lr: 0.1000\n",
      "Epoch 27/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9441\n",
      "Epoch 27: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.1667 - accuracy: 0.9441 - val_loss: 1.5676 - val_accuracy: 0.6920 - lr: 0.1000\n",
      "Epoch 28/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1457 - accuracy: 0.9525\n",
      "Epoch 28: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.1457 - accuracy: 0.9525 - val_loss: 1.5390 - val_accuracy: 0.7080 - lr: 0.1000\n",
      "Epoch 29/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9534\n",
      "Epoch 29: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.1433 - accuracy: 0.9534 - val_loss: 1.5878 - val_accuracy: 0.7028 - lr: 0.1000\n",
      "Epoch 30/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9542\n",
      "Epoch 30: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1388 - accuracy: 0.9542 - val_loss: 1.5932 - val_accuracy: 0.7004 - lr: 0.1000\n",
      "Epoch 31/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9613\n",
      "Epoch 31: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1209 - accuracy: 0.9613 - val_loss: 1.5675 - val_accuracy: 0.7122 - lr: 0.1000\n",
      "Epoch 32/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9652\n",
      "Epoch 32: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1076 - accuracy: 0.9652 - val_loss: 1.6940 - val_accuracy: 0.7000 - lr: 0.1000\n",
      "Epoch 33/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9668\n",
      "Epoch 33: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.1027 - accuracy: 0.9668 - val_loss: 1.6429 - val_accuracy: 0.7044 - lr: 0.1000\n",
      "Epoch 34/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9690\n",
      "Epoch 34: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0920 - accuracy: 0.9690 - val_loss: 1.6272 - val_accuracy: 0.7058 - lr: 0.1000\n",
      "Epoch 35/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9703\n",
      "Epoch 35: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0896 - accuracy: 0.9703 - val_loss: 1.7048 - val_accuracy: 0.6952 - lr: 0.1000\n",
      "Epoch 36/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9750\n",
      "Epoch 36: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0765 - accuracy: 0.9750 - val_loss: 1.6976 - val_accuracy: 0.7000 - lr: 0.1000\n",
      "Epoch 37/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9743\n",
      "Epoch 37: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0794 - accuracy: 0.9743 - val_loss: 1.7352 - val_accuracy: 0.7066 - lr: 0.1000\n",
      "Epoch 38/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9763\n",
      "Epoch 38: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0730 - accuracy: 0.9763 - val_loss: 1.6777 - val_accuracy: 0.7110 - lr: 0.1000\n",
      "Epoch 39/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9763\n",
      "Epoch 39: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0732 - accuracy: 0.9763 - val_loss: 1.6705 - val_accuracy: 0.7114 - lr: 0.1000\n",
      "Epoch 40/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9793\n",
      "Epoch 40: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0636 - accuracy: 0.9793 - val_loss: 1.6584 - val_accuracy: 0.7126 - lr: 0.1000\n",
      "Epoch 41/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9815\n",
      "Epoch 41: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 128s 363ms/step - loss: 0.0581 - accuracy: 0.9815 - val_loss: 1.7069 - val_accuracy: 0.7150 - lr: 0.1000\n",
      "Epoch 42/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9802\n",
      "Epoch 42: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0619 - accuracy: 0.9802 - val_loss: 1.7343 - val_accuracy: 0.7098 - lr: 0.1000\n",
      "Epoch 43/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9813\n",
      "Epoch 43: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0589 - accuracy: 0.9813 - val_loss: 1.6636 - val_accuracy: 0.7266 - lr: 0.1000\n",
      "Epoch 44/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9863\n",
      "Epoch 44: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0453 - accuracy: 0.9863 - val_loss: 1.7680 - val_accuracy: 0.7216 - lr: 0.1000\n",
      "Epoch 45/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9855\n",
      "Epoch 45: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0454 - accuracy: 0.9855 - val_loss: 1.8318 - val_accuracy: 0.7120 - lr: 0.1000\n",
      "Epoch 46/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9867\n",
      "Epoch 46: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0428 - accuracy: 0.9867 - val_loss: 1.7799 - val_accuracy: 0.7096 - lr: 0.1000\n",
      "Epoch 47/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9849\n",
      "Epoch 47: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0468 - accuracy: 0.9849 - val_loss: 1.7255 - val_accuracy: 0.7236 - lr: 0.1000\n",
      "Epoch 48/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9879\n",
      "Epoch 48: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0374 - accuracy: 0.9879 - val_loss: 1.7567 - val_accuracy: 0.7218 - lr: 0.1000\n",
      "Epoch 49/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9894\n",
      "Epoch 49: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0339 - accuracy: 0.9894 - val_loss: 1.8052 - val_accuracy: 0.7106 - lr: 0.1000\n",
      "Epoch 50/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9867\n",
      "Epoch 50: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0418 - accuracy: 0.9867 - val_loss: 1.8680 - val_accuracy: 0.7116 - lr: 0.1000\n",
      "Epoch 51/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9865\n",
      "Epoch 51: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0413 - accuracy: 0.9865 - val_loss: 1.8214 - val_accuracy: 0.7102 - lr: 0.1000\n",
      "Epoch 52/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9879\n",
      "Epoch 52: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0384 - accuracy: 0.9879 - val_loss: 1.8298 - val_accuracy: 0.7088 - lr: 0.1000\n",
      "Epoch 53/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9914\n",
      "Epoch 53: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0276 - accuracy: 0.9914 - val_loss: 1.8136 - val_accuracy: 0.7172 - lr: 0.1000\n",
      "Epoch 54/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9920\n",
      "Epoch 54: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0264 - accuracy: 0.9920 - val_loss: 1.8764 - val_accuracy: 0.7108 - lr: 0.1000\n",
      "Epoch 55/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9906\n",
      "Epoch 55: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0302 - accuracy: 0.9906 - val_loss: 1.8537 - val_accuracy: 0.7146 - lr: 0.1000\n",
      "Epoch 56/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9923\n",
      "Epoch 56: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 1.8024 - val_accuracy: 0.7192 - lr: 0.1000\n",
      "Epoch 57/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9923\n",
      "Epoch 57: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 1.8489 - val_accuracy: 0.7132 - lr: 0.1000\n",
      "Epoch 58/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9908\n",
      "Epoch 58: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 1.8093 - val_accuracy: 0.7192 - lr: 0.1000\n",
      "Epoch 59/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9908\n",
      "Epoch 59: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0277 - accuracy: 0.9908 - val_loss: 1.9300 - val_accuracy: 0.7158 - lr: 0.1000\n",
      "Epoch 60/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9916\n",
      "Epoch 60: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 1.8594 - val_accuracy: 0.7192 - lr: 0.1000\n",
      "Epoch 61/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9960\n",
      "Epoch 61: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 1.7228 - val_accuracy: 0.7358 - lr: 0.0200\n",
      "Epoch 62/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9984\n",
      "Epoch 62: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0061 - accuracy: 0.9984 - val_loss: 1.7640 - val_accuracy: 0.7356 - lr: 0.0200\n",
      "Epoch 63/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9986\n",
      "Epoch 63: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 1.7135 - val_accuracy: 0.7362 - lr: 0.0200\n",
      "Epoch 64/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9989\n",
      "Epoch 64: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 1.7267 - val_accuracy: 0.7410 - lr: 0.0200\n",
      "Epoch 65/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9988\n",
      "Epoch 65: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 1.7236 - val_accuracy: 0.7386 - lr: 0.0200\n",
      "Epoch 66/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 66: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 1.7273 - val_accuracy: 0.7412 - lr: 0.0200\n",
      "Epoch 67/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9990\n",
      "Epoch 67: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 1.7121 - val_accuracy: 0.7400 - lr: 0.0200\n",
      "Epoch 68/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 68: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 1.6947 - val_accuracy: 0.7440 - lr: 0.0200\n",
      "Epoch 69/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 69: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 1.7638 - val_accuracy: 0.7312 - lr: 0.0200\n",
      "Epoch 70/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9996\n",
      "Epoch 70: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 1.7596 - val_accuracy: 0.7422 - lr: 0.0200\n",
      "Epoch 71/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 71: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 1.7734 - val_accuracy: 0.7378 - lr: 0.0200\n",
      "Epoch 72/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 72: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 1.7533 - val_accuracy: 0.7314 - lr: 0.0200\n",
      "Epoch 73/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 73: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 1.7415 - val_accuracy: 0.7374 - lr: 0.0200\n",
      "Epoch 74/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 74: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 1.7157 - val_accuracy: 0.7458 - lr: 0.0200\n",
      "Epoch 75/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 75: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 1.7513 - val_accuracy: 0.7412 - lr: 0.0200\n",
      "Epoch 76/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 76: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 1.7528 - val_accuracy: 0.7376 - lr: 0.0200\n",
      "Epoch 77/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9995\n",
      "Epoch 77: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 1.7169 - val_accuracy: 0.7398 - lr: 0.0200\n",
      "Epoch 78/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 78: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 1.7320 - val_accuracy: 0.7366 - lr: 0.0200\n",
      "Epoch 79/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 79: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 1.7487 - val_accuracy: 0.7450 - lr: 0.0200\n",
      "Epoch 80/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 80: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 1.7455 - val_accuracy: 0.7442 - lr: 0.0200\n",
      "Epoch 81/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 81: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.7315 - val_accuracy: 0.7412 - lr: 0.0200\n",
      "Epoch 82/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 82: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.7288 - val_accuracy: 0.7400 - lr: 0.0200\n",
      "Epoch 83/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 83: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 1.7249 - val_accuracy: 0.7422 - lr: 0.0200\n",
      "Epoch 84/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 84: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.7496 - val_accuracy: 0.7438 - lr: 0.0200\n",
      "Epoch 85/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 85: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 1.7295 - val_accuracy: 0.7390 - lr: 0.0200\n",
      "Epoch 86/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 86: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 1.7470 - val_accuracy: 0.7370 - lr: 0.0200\n",
      "Epoch 87/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 87: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.7729 - val_accuracy: 0.7402 - lr: 0.0200\n",
      "Epoch 88/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 88: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.7599 - val_accuracy: 0.7400 - lr: 0.0200\n",
      "Epoch 89/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 89: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.7197 - val_accuracy: 0.7434 - lr: 0.0200\n",
      "Epoch 90/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 90: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.7597 - val_accuracy: 0.7402 - lr: 0.0200\n",
      "Epoch 91/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 91: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 128s 363ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 1.7249 - val_accuracy: 0.7422 - lr: 0.0200\n",
      "Epoch 92/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 92: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.7983 - val_accuracy: 0.7366 - lr: 0.0200\n",
      "Epoch 93/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 93: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 1.7734 - val_accuracy: 0.7392 - lr: 0.0200\n",
      "Epoch 94/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 94: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.7334 - val_accuracy: 0.7418 - lr: 0.0200\n",
      "Epoch 95/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9994\n",
      "Epoch 95: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 1.7136 - val_accuracy: 0.7446 - lr: 0.0200\n",
      "Epoch 96/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 96: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 1.7590 - val_accuracy: 0.7396 - lr: 0.0200\n",
      "Epoch 97/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 97: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 1.7298 - val_accuracy: 0.7428 - lr: 0.0200\n",
      "Epoch 98/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 98: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.7709 - val_accuracy: 0.7398 - lr: 0.0200\n",
      "Epoch 99/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 99: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 1.7583 - val_accuracy: 0.7404 - lr: 0.0200\n",
      "Epoch 100/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 100: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 1.7842 - val_accuracy: 0.7378 - lr: 0.0200\n",
      "Epoch 101/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 101: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.7539 - val_accuracy: 0.7384 - lr: 0.0200\n",
      "Epoch 102/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 102: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.7861 - val_accuracy: 0.7426 - lr: 0.0200\n",
      "Epoch 103/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 103: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.7692 - val_accuracy: 0.7388 - lr: 0.0200\n",
      "Epoch 104/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 104: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.7258 - val_accuracy: 0.7448 - lr: 0.0200\n",
      "Epoch 105/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 105: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.7558 - val_accuracy: 0.7392 - lr: 0.0200\n",
      "Epoch 106/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 106: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.7856 - val_accuracy: 0.7398 - lr: 0.0200\n",
      "Epoch 107/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 107: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.7701 - val_accuracy: 0.7386 - lr: 0.0200\n",
      "Epoch 108/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 108: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.7651 - val_accuracy: 0.7412 - lr: 0.0200\n",
      "Epoch 109/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 109: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.7149 - val_accuracy: 0.7436 - lr: 0.0200\n",
      "Epoch 110/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 110: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.7711 - val_accuracy: 0.7394 - lr: 0.0200\n",
      "Epoch 111/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 111: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 128s 363ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.7459 - val_accuracy: 0.7448 - lr: 0.0200\n",
      "Epoch 112/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 112: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.7785 - val_accuracy: 0.7366 - lr: 0.0200\n",
      "Epoch 113/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 113: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 1.8021 - val_accuracy: 0.7386 - lr: 0.0200\n",
      "Epoch 114/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 114: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.8012 - val_accuracy: 0.7418 - lr: 0.0200\n",
      "Epoch 115/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 115: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.7446 - val_accuracy: 0.7406 - lr: 0.0200\n",
      "Epoch 116/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 116: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.7715 - val_accuracy: 0.7436 - lr: 0.0200\n",
      "Epoch 117/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 117: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.7680 - val_accuracy: 0.7418 - lr: 0.0200\n",
      "Epoch 118/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 118: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.7493 - val_accuracy: 0.7434 - lr: 0.0200\n",
      "Epoch 119/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 119: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.7767 - val_accuracy: 0.7414 - lr: 0.0200\n",
      "Epoch 120/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 120: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.7676 - val_accuracy: 0.7448 - lr: 0.0200\n",
      "Epoch 1/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.3667e-04 - accuracy: 0.9998\n",
      "Epoch 1: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 63s 175ms/step - loss: 8.3667e-04 - accuracy: 0.9998 - val_loss: 1.7781 - val_accuracy: 0.7460 - lr: 0.0040\n",
      "Epoch 2/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7252e-04 - accuracy: 0.9998\n",
      "Epoch 2: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 56s 159ms/step - loss: 7.7252e-04 - accuracy: 0.9998 - val_loss: 1.7273 - val_accuracy: 0.7434 - lr: 0.0040\n",
      "Epoch 3/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.9043e-04 - accuracy: 0.9999\n",
      "Epoch 3: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 6.9043e-04 - accuracy: 0.9999 - val_loss: 1.7632 - val_accuracy: 0.7428 - lr: 0.0040\n",
      "Epoch 4/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8509e-04 - accuracy: 0.9998\n",
      "Epoch 4: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 56s 159ms/step - loss: 7.8509e-04 - accuracy: 0.9998 - val_loss: 1.7409 - val_accuracy: 0.7434 - lr: 0.0040\n",
      "Epoch 5/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.2653e-04 - accuracy: 0.9998\n",
      "Epoch 5: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 8.2653e-04 - accuracy: 0.9998 - val_loss: 1.7297 - val_accuracy: 0.7432 - lr: 0.0040\n",
      "Epoch 6/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.2875e-04 - accuracy: 0.9998\n",
      "Epoch 6: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 9.2875e-04 - accuracy: 0.9998 - val_loss: 1.7840 - val_accuracy: 0.7412 - lr: 0.0040\n",
      "Epoch 7/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.6490e-04 - accuracy: 0.9999\n",
      "Epoch 7: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 6.6490e-04 - accuracy: 0.9999 - val_loss: 1.8097 - val_accuracy: 0.7396 - lr: 0.0040\n",
      "Epoch 8/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.1232e-04 - accuracy: 0.9997\n",
      "Epoch 8: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 9.1232e-04 - accuracy: 0.9997 - val_loss: 1.7723 - val_accuracy: 0.7388 - lr: 0.0040\n",
      "Epoch 9/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6809e-04 - accuracy: 0.9998\n",
      "Epoch 9: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.6809e-04 - accuracy: 0.9998 - val_loss: 1.7541 - val_accuracy: 0.7426 - lr: 0.0040\n",
      "Epoch 10/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6068e-04 - accuracy: 0.9998\n",
      "Epoch 10: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 56s 158ms/step - loss: 8.6068e-04 - accuracy: 0.9998 - val_loss: 1.7408 - val_accuracy: 0.7418 - lr: 0.0040\n",
      "Epoch 11/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6921e-04 - accuracy: 0.9998\n",
      "Epoch 11: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 8.6921e-04 - accuracy: 0.9998 - val_loss: 1.7275 - val_accuracy: 0.7436 - lr: 0.0040\n",
      "Epoch 12/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6551e-04 - accuracy: 0.9997\n",
      "Epoch 12: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 8.6551e-04 - accuracy: 0.9997 - val_loss: 1.7462 - val_accuracy: 0.7422 - lr: 0.0040\n",
      "Epoch 13/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.7147e-04 - accuracy: 0.9997\n",
      "Epoch 13: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.7147e-04 - accuracy: 0.9997 - val_loss: 1.7509 - val_accuracy: 0.7436 - lr: 0.0040\n",
      "Epoch 14/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.0430e-04 - accuracy: 0.9997\n",
      "Epoch 14: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 9.0430e-04 - accuracy: 0.9997 - val_loss: 1.7713 - val_accuracy: 0.7398 - lr: 0.0040\n",
      "Epoch 15/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0260e-04 - accuracy: 0.9997\n",
      "Epoch 15: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 8.0260e-04 - accuracy: 0.9997 - val_loss: 1.7566 - val_accuracy: 0.7414 - lr: 0.0040\n",
      "Epoch 16/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.7601e-04 - accuracy: 0.9997\n",
      "Epoch 16: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.7601e-04 - accuracy: 0.9997 - val_loss: 1.7730 - val_accuracy: 0.7446 - lr: 0.0040\n",
      "Epoch 17/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9353e-04 - accuracy: 0.9998\n",
      "Epoch 17: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 7.9353e-04 - accuracy: 0.9998 - val_loss: 1.7730 - val_accuracy: 0.7416 - lr: 0.0040\n",
      "Epoch 18/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.8948e-04 - accuracy: 0.9997\n",
      "Epoch 18: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.8948e-04 - accuracy: 0.9997 - val_loss: 1.7594 - val_accuracy: 0.7462 - lr: 0.0040\n",
      "Epoch 19/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2929e-04 - accuracy: 0.9998\n",
      "Epoch 19: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.2929e-04 - accuracy: 0.9998 - val_loss: 1.7733 - val_accuracy: 0.7394 - lr: 0.0040\n",
      "Epoch 20/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.1665e-04 - accuracy: 0.9997\n",
      "Epoch 20: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 9.1665e-04 - accuracy: 0.9997 - val_loss: 1.7430 - val_accuracy: 0.7434 - lr: 0.0040\n",
      "Epoch 21/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6014e-04 - accuracy: 0.9997\n",
      "Epoch 21: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 56s 158ms/step - loss: 8.6014e-04 - accuracy: 0.9997 - val_loss: 1.7789 - val_accuracy: 0.7390 - lr: 0.0040\n",
      "Epoch 22/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.0803e-04 - accuracy: 0.9996\n",
      "Epoch 22: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 9.0803e-04 - accuracy: 0.9996 - val_loss: 1.7531 - val_accuracy: 0.7398 - lr: 0.0040\n",
      "Epoch 23/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.3059e-04 - accuracy: 0.9998\n",
      "Epoch 23: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.3059e-04 - accuracy: 0.9998 - val_loss: 1.8070 - val_accuracy: 0.7386 - lr: 0.0040\n",
      "Epoch 24/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.6440e-04 - accuracy: 0.9998\n",
      "Epoch 24: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.6440e-04 - accuracy: 0.9998 - val_loss: 1.7617 - val_accuracy: 0.7438 - lr: 0.0040\n",
      "Epoch 25/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 25: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 1.7616 - val_accuracy: 0.7404 - lr: 0.0040\n",
      "Epoch 26/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8512e-04 - accuracy: 0.9998\n",
      "Epoch 26: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 7.8512e-04 - accuracy: 0.9998 - val_loss: 1.7501 - val_accuracy: 0.7406 - lr: 0.0040\n",
      "Epoch 27/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0637e-04 - accuracy: 0.9998\n",
      "Epoch 27: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 8.0637e-04 - accuracy: 0.9998 - val_loss: 1.7553 - val_accuracy: 0.7408 - lr: 0.0040\n",
      "Epoch 28/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6619e-04 - accuracy: 0.9997\n",
      "Epoch 28: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.6619e-04 - accuracy: 0.9997 - val_loss: 1.8007 - val_accuracy: 0.7400 - lr: 0.0040\n",
      "Epoch 29/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.6384e-04 - accuracy: 0.9998\n",
      "Epoch 29: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 9.6384e-04 - accuracy: 0.9998 - val_loss: 1.7654 - val_accuracy: 0.7406 - lr: 0.0040\n",
      "Epoch 30/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.8697e-04 - accuracy: 0.9998\n",
      "Epoch 30: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 8.8697e-04 - accuracy: 0.9998 - val_loss: 1.7810 - val_accuracy: 0.7408 - lr: 0.0040\n",
      "Epoch 31/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.3540e-04 - accuracy: 0.9998\n",
      "Epoch 31: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 56s 158ms/step - loss: 7.3540e-04 - accuracy: 0.9998 - val_loss: 1.7812 - val_accuracy: 0.7422 - lr: 0.0040\n",
      "Epoch 32/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2906e-04 - accuracy: 0.9998\n",
      "Epoch 32: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.2906e-04 - accuracy: 0.9998 - val_loss: 1.7833 - val_accuracy: 0.7412 - lr: 0.0040\n",
      "Epoch 33/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.2900e-04 - accuracy: 0.9998\n",
      "Epoch 33: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 8.2900e-04 - accuracy: 0.9998 - val_loss: 1.7691 - val_accuracy: 0.7424 - lr: 0.0040\n",
      "Epoch 34/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.4056e-04 - accuracy: 0.9997\n",
      "Epoch 34: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 8.4056e-04 - accuracy: 0.9997 - val_loss: 1.7589 - val_accuracy: 0.7398 - lr: 0.0040\n",
      "Epoch 35/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7542e-04 - accuracy: 0.9998\n",
      "Epoch 35: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.7542e-04 - accuracy: 0.9998 - val_loss: 1.7496 - val_accuracy: 0.7422 - lr: 0.0040\n",
      "Epoch 36/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.2266e-04 - accuracy: 0.9998\n",
      "Epoch 36: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.2266e-04 - accuracy: 0.9998 - val_loss: 1.7867 - val_accuracy: 0.7422 - lr: 0.0040\n",
      "Epoch 37/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2141e-04 - accuracy: 0.9998\n",
      "Epoch 37: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 7.2141e-04 - accuracy: 0.9998 - val_loss: 1.7379 - val_accuracy: 0.7444 - lr: 0.0040\n",
      "Epoch 38/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.6509e-04 - accuracy: 0.9998\n",
      "Epoch 38: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 7.6509e-04 - accuracy: 0.9998 - val_loss: 1.7595 - val_accuracy: 0.7414 - lr: 0.0040\n",
      "Epoch 39/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.7680e-04 - accuracy: 0.9997\n",
      "Epoch 39: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.7680e-04 - accuracy: 0.9997 - val_loss: 1.7750 - val_accuracy: 0.7416 - lr: 0.0040\n",
      "Epoch 40/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.4439e-04 - accuracy: 0.9998\n",
      "Epoch 40: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 8.4439e-04 - accuracy: 0.9998 - val_loss: 1.7488 - val_accuracy: 0.7422 - lr: 0.0040\n",
      "Epoch 41/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.6812e-04 - accuracy: 0.9997\n",
      "Epoch 41: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 7.6812e-04 - accuracy: 0.9997 - val_loss: 1.7617 - val_accuracy: 0.7408 - lr: 8.0000e-04\n",
      "Epoch 42/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.3138e-04 - accuracy: 0.9998\n",
      "Epoch 42: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 8.3138e-04 - accuracy: 0.9998 - val_loss: 1.7385 - val_accuracy: 0.7430 - lr: 8.0000e-04\n",
      "Epoch 43/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0915e-04 - accuracy: 0.9997\n",
      "Epoch 43: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.0915e-04 - accuracy: 0.9997 - val_loss: 1.7839 - val_accuracy: 0.7420 - lr: 8.0000e-04\n",
      "Epoch 44/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.1840e-04 - accuracy: 0.9997\n",
      "Epoch 44: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 9.1840e-04 - accuracy: 0.9997 - val_loss: 1.7220 - val_accuracy: 0.7446 - lr: 8.0000e-04\n",
      "Epoch 45/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.3247e-04 - accuracy: 0.9997\n",
      "Epoch 45: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 9.3247e-04 - accuracy: 0.9997 - val_loss: 1.7395 - val_accuracy: 0.7420 - lr: 8.0000e-04\n",
      "Epoch 46/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.3039e-04 - accuracy: 0.9998\n",
      "Epoch 46: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 8.3039e-04 - accuracy: 0.9998 - val_loss: 1.7412 - val_accuracy: 0.7420 - lr: 8.0000e-04\n",
      "Epoch 47/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.4046e-04 - accuracy: 0.9997\n",
      "Epoch 47: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.4046e-04 - accuracy: 0.9997 - val_loss: 1.7647 - val_accuracy: 0.7422 - lr: 8.0000e-04\n",
      "Epoch 48/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9334e-04 - accuracy: 0.9997\n",
      "Epoch 48: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.9334e-04 - accuracy: 0.9997 - val_loss: 1.8134 - val_accuracy: 0.7394 - lr: 8.0000e-04\n",
      "Epoch 49/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.4662e-04 - accuracy: 0.9997\n",
      "Epoch 49: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 9.4662e-04 - accuracy: 0.9997 - val_loss: 1.7774 - val_accuracy: 0.7410 - lr: 8.0000e-04\n",
      "Epoch 50/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.5456e-04 - accuracy: 0.9998\n",
      "Epoch 50: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.5456e-04 - accuracy: 0.9998 - val_loss: 1.7127 - val_accuracy: 0.7444 - lr: 8.0000e-04\n",
      "Epoch 51/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.4798e-04 - accuracy: 0.9998\n",
      "Epoch 51: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 56s 160ms/step - loss: 8.4798e-04 - accuracy: 0.9998 - val_loss: 1.7701 - val_accuracy: 0.7424 - lr: 8.0000e-04\n",
      "Epoch 52/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.5134e-04 - accuracy: 0.9997\n",
      "Epoch 52: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.5134e-04 - accuracy: 0.9997 - val_loss: 1.7931 - val_accuracy: 0.7402 - lr: 8.0000e-04\n",
      "Epoch 53/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2902e-04 - accuracy: 0.9998\n",
      "Epoch 53: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.2902e-04 - accuracy: 0.9998 - val_loss: 1.7914 - val_accuracy: 0.7394 - lr: 8.0000e-04\n",
      "Epoch 54/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2455e-04 - accuracy: 0.9998\n",
      "Epoch 54: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.2455e-04 - accuracy: 0.9998 - val_loss: 1.7702 - val_accuracy: 0.7420 - lr: 8.0000e-04\n",
      "Epoch 55/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8355e-04 - accuracy: 0.9997\n",
      "Epoch 55: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.8355e-04 - accuracy: 0.9997 - val_loss: 1.7333 - val_accuracy: 0.7420 - lr: 8.0000e-04\n",
      "Epoch 56/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.5683e-04 - accuracy: 0.9998\n",
      "Epoch 56: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.5683e-04 - accuracy: 0.9998 - val_loss: 1.7695 - val_accuracy: 0.7414 - lr: 8.0000e-04\n",
      "Epoch 57/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.1685e-04 - accuracy: 0.9999\n",
      "Epoch 57: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 7.1685e-04 - accuracy: 0.9999 - val_loss: 1.7570 - val_accuracy: 0.7424 - lr: 8.0000e-04\n",
      "Epoch 58/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.8876e-04 - accuracy: 0.9996\n",
      "Epoch 58: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 9.8876e-04 - accuracy: 0.9996 - val_loss: 1.7870 - val_accuracy: 0.7428 - lr: 8.0000e-04\n",
      "Epoch 59/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.5900e-04 - accuracy: 0.9998\n",
      "Epoch 59: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.5900e-04 - accuracy: 0.9998 - val_loss: 1.7448 - val_accuracy: 0.7436 - lr: 8.0000e-04\n",
      "Epoch 60/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.7953e-04 - accuracy: 0.9999\n",
      "Epoch 60: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 172ms/step - loss: 6.7953e-04 - accuracy: 0.9999 - val_loss: 1.7606 - val_accuracy: 0.7402 - lr: 8.0000e-04\n",
      "Epoch 61/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.9101e-04 - accuracy: 0.9998\n",
      "Epoch 61: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 56s 160ms/step - loss: 6.9101e-04 - accuracy: 0.9998 - val_loss: 1.7928 - val_accuracy: 0.7386 - lr: 8.0000e-04\n",
      "Epoch 62/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9707e-04 - accuracy: 0.9998\n",
      "Epoch 62: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.9707e-04 - accuracy: 0.9998 - val_loss: 1.7498 - val_accuracy: 0.7394 - lr: 8.0000e-04\n",
      "Epoch 63/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8068e-04 - accuracy: 0.9998\n",
      "Epoch 63: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 7.8068e-04 - accuracy: 0.9998 - val_loss: 1.7525 - val_accuracy: 0.7446 - lr: 8.0000e-04\n",
      "Epoch 64/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9609e-04 - accuracy: 0.9998\n",
      "Epoch 64: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.9609e-04 - accuracy: 0.9998 - val_loss: 1.7833 - val_accuracy: 0.7430 - lr: 8.0000e-04\n",
      "Epoch 65/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0221e-04 - accuracy: 0.9998\n",
      "Epoch 65: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 8.0221e-04 - accuracy: 0.9998 - val_loss: 1.7323 - val_accuracy: 0.7412 - lr: 8.0000e-04\n",
      "Epoch 66/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7469e-04 - accuracy: 0.9997\n",
      "Epoch 66: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 7.7469e-04 - accuracy: 0.9997 - val_loss: 1.7553 - val_accuracy: 0.7438 - lr: 8.0000e-04\n",
      "Epoch 67/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7758e-04 - accuracy: 0.9998\n",
      "Epoch 67: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 7.7758e-04 - accuracy: 0.9998 - val_loss: 1.8007 - val_accuracy: 0.7452 - lr: 8.0000e-04\n",
      "Epoch 68/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.1884e-04 - accuracy: 0.9998\n",
      "Epoch 68: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.1884e-04 - accuracy: 0.9998 - val_loss: 1.8192 - val_accuracy: 0.7408 - lr: 8.0000e-04\n",
      "Epoch 69/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8966e-04 - accuracy: 0.9998\n",
      "Epoch 69: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.8966e-04 - accuracy: 0.9998 - val_loss: 1.8016 - val_accuracy: 0.7414 - lr: 8.0000e-04\n",
      "Epoch 70/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9962e-04 - accuracy: 0.9998\n",
      "Epoch 70: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.9962e-04 - accuracy: 0.9998 - val_loss: 1.7697 - val_accuracy: 0.7406 - lr: 8.0000e-04\n",
      "Epoch 71/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.5111e-04 - accuracy: 0.9998\n",
      "Epoch 71: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 8.5111e-04 - accuracy: 0.9998 - val_loss: 1.7703 - val_accuracy: 0.7414 - lr: 8.0000e-04\n",
      "Epoch 72/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.4720e-04 - accuracy: 0.9998\n",
      "Epoch 72: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.4720e-04 - accuracy: 0.9998 - val_loss: 1.7552 - val_accuracy: 0.7414 - lr: 8.0000e-04\n",
      "Epoch 73/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2430e-04 - accuracy: 0.9998\n",
      "Epoch 73: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 7.2430e-04 - accuracy: 0.9998 - val_loss: 1.7517 - val_accuracy: 0.7424 - lr: 8.0000e-04\n",
      "Epoch 74/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0938e-04 - accuracy: 0.9998\n",
      "Epoch 74: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 8.0938e-04 - accuracy: 0.9998 - val_loss: 1.7425 - val_accuracy: 0.7484 - lr: 8.0000e-04\n",
      "Epoch 75/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.7090e-04 - accuracy: 0.9998\n",
      "Epoch 75: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 6.7090e-04 - accuracy: 0.9998 - val_loss: 1.7662 - val_accuracy: 0.7420 - lr: 8.0000e-04\n",
      "Epoch 76/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0644e-04 - accuracy: 0.9998\n",
      "Epoch 76: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.0644e-04 - accuracy: 0.9998 - val_loss: 1.7505 - val_accuracy: 0.7432 - lr: 8.0000e-04\n",
      "Epoch 77/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.1366e-04 - accuracy: 0.9997\n",
      "Epoch 77: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.1366e-04 - accuracy: 0.9997 - val_loss: 1.7819 - val_accuracy: 0.7432 - lr: 8.0000e-04\n",
      "Epoch 78/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8855e-04 - accuracy: 0.9998\n",
      "Epoch 78: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 7.8855e-04 - accuracy: 0.9998 - val_loss: 1.7213 - val_accuracy: 0.7422 - lr: 8.0000e-04\n",
      "Epoch 79/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.4085e-04 - accuracy: 0.9997\n",
      "Epoch 79: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.4085e-04 - accuracy: 0.9997 - val_loss: 1.7543 - val_accuracy: 0.7408 - lr: 8.0000e-04\n",
      "Epoch 80/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8694e-04 - accuracy: 0.9998\n",
      "Epoch 80: val_loss did not improve from 1.27439\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 7.8694e-04 - accuracy: 0.9998 - val_loss: 1.7071 - val_accuracy: 0.7434 - lr: 8.0000e-04\n",
      "Current:  91\n",
      "313/313 [==============================] - 11s 33ms/step\n",
      "Accuracy: 74.28\n",
      "Error: 25.72\n",
      "ECE: 0.18379888268411154\n",
      "MCE: 0.4234702744225199\n",
      "Loss: 1.8007572489687467\n",
      "brier: 0.23835868281587044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[25.72,\n",
       " 0.18379888268411154,\n",
       " 0.4234702744225199,\n",
       " 1.8007572489687467,\n",
       " 0.23835868281587044]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, datagen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list,lr_schedule = [[0, 0.1],[epochs*0.3,0.02],[epochs*0.6,0.004],[epochs*0.8,0.0008]],cbks=[checkpointer], name='resnet_wide_cifar100')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21634.68547,
   "end_time": "2023-04-22T15:35:16.707969",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-22T09:34:42.022499",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
