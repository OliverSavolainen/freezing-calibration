{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbf69be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:52:47.618071Z",
     "iopub.status.busy": "2023-04-25T09:52:47.617704Z",
     "iopub.status.idle": "2023-04-25T09:52:55.528978Z",
     "shell.execute_reply": "2023-04-25T09:52:55.527873Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1682109576846,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "bpCwuvLginNe",
    "papermill": {
     "duration": 7.918999,
     "end_time": "2023-04-25T09:52:55.531921",
     "exception": false,
     "start_time": "2023-04-25T09:52:47.612922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import Input\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import freezing\n",
    "import wrn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "depth = 34\n",
    "growth_rate = 10  # Growth factor\n",
    "n = (depth - 4) // 6  # N, number of blocks in one group\n",
    "num_classes = 100\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "iterations = 45000 // batch_size\n",
    "weight_decay = 0.0005\n",
    "seed = 333\n",
    "\n",
    "\n",
    "def color_preprocessing(x_train, x_val, x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    mean = np.mean(x_train, axis=(0, 1, 2))  # Per channel mean\n",
    "    std = np.std(x_train, axis=(0, 1, 2))\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_val = (x_val - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    return x_train, x_val, x_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# color preprocessing\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "x_train45, x_val, x_test = color_preprocessing(x_train45, x_val, x_test)\n",
    "\n",
    "y_train45 = keras.utils.to_categorical(y_train45, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# build network\n",
    "img_input = Input(shape=(img_rows,img_cols,img_channels))    \n",
    "model = wrn.create_wide_residual_network(img_input, nb_classes=num_classes, N=n, k=growth_rate, dropout=0.0)\n",
    "print(model.summary())\n",
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "\n",
    "# set callback\n",
    "checkpointer = ModelCheckpoint('model_wide_28_10_c100_best_2.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "# set data augmentation\n",
    "print('Using real-time data augmentation.')\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "        width_shift_range=0.125,height_shift_range=0.125,fill_mode='reflect') # Missing pixels replaced with reflections\n",
    "\n",
    "datagen.fit(x_train45)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5cb1559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T09:53:07.073139Z",
     "iopub.status.busy": "2023-04-25T09:53:07.072776Z",
     "iopub.status.idle": "2023-04-25T09:53:07.096224Z",
     "shell.execute_reply": "2023-04-25T09:53:07.095351Z"
    },
    "papermill": {
     "duration": 0.047705,
     "end_time": "2023-04-25T09:53:07.098281",
     "exception": false,
     "start_time": "2023-04-25T09:53:07.050576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "    if i < len(model.layers) * 0.9:\n",
    "        freezing_list.append(int(epochs*0.6))\n",
    "    elif i < len(model.layers) * 0.98:\n",
    "        freezing_list.append(int(epochs*0.96))\n",
    "freezing_list.append(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25cf7c6",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-04-25T09:53:07.140840Z",
     "iopub.status.busy": "2023-04-25T09:53:07.140563Z",
     "iopub.status.idle": "2023-04-25T15:52:30.582023Z",
     "shell.execute_reply": "2023-04-25T15:52:30.580927Z"
    },
    "papermill": {
     "duration": 21563.465512,
     "end_time": "2023-04-25T15:52:30.584213",
     "exception": false,
     "start_time": "2023-04-25T09:53:07.118701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-25 09:53:07        53368\n",
      "variables.h5                                   2023-04-25 09:53:07    185249224\n",
      "metadata.json                                  2023-04-25 09:53:07           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-25 09:53:06        53368\n",
      "variables.h5                                   2023-04-25 09:53:06    185249224\n",
      "metadata.json                                  2023-04-25 09:53:06           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.9782 - accuracy: 0.0854\n",
      "Epoch 1: val_loss improved from inf to 3.56925, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 137s 368ms/step - loss: 3.9782 - accuracy: 0.0854 - val_loss: 3.5693 - val_accuracy: 0.1386 - lr: 0.1000\n",
      "Epoch 2/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.0380 - accuracy: 0.2309\n",
      "Epoch 2: val_loss improved from 3.56925 to 2.61170, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 3.0380 - accuracy: 0.2309 - val_loss: 2.6117 - val_accuracy: 0.3204 - lr: 0.1000\n",
      "Epoch 3/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4152 - accuracy: 0.3535\n",
      "Epoch 3: val_loss improved from 2.61170 to 2.14281, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 2.4152 - accuracy: 0.3535 - val_loss: 2.1428 - val_accuracy: 0.4194 - lr: 0.1000\n",
      "Epoch 4/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0256 - accuracy: 0.4437\n",
      "Epoch 4: val_loss improved from 2.14281 to 1.95406, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 2.0256 - accuracy: 0.4437 - val_loss: 1.9541 - val_accuracy: 0.4684 - lr: 0.1000\n",
      "Epoch 5/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7496 - accuracy: 0.5076\n",
      "Epoch 5: val_loss improved from 1.95406 to 1.70433, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 1.7496 - accuracy: 0.5076 - val_loss: 1.7043 - val_accuracy: 0.5208 - lr: 0.1000\n",
      "Epoch 6/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5530 - accuracy: 0.5550\n",
      "Epoch 6: val_loss improved from 1.70433 to 1.59521, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 128s 363ms/step - loss: 1.5530 - accuracy: 0.5550 - val_loss: 1.5952 - val_accuracy: 0.5530 - lr: 0.1000\n",
      "Epoch 7/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3858 - accuracy: 0.6000\n",
      "Epoch 7: val_loss improved from 1.59521 to 1.46672, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 1.3858 - accuracy: 0.6000 - val_loss: 1.4667 - val_accuracy: 0.5922 - lr: 0.1000\n",
      "Epoch 8/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2456 - accuracy: 0.6333\n",
      "Epoch 8: val_loss improved from 1.46672 to 1.43192, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 1.2456 - accuracy: 0.6333 - val_loss: 1.4319 - val_accuracy: 0.5978 - lr: 0.1000\n",
      "Epoch 9/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1261 - accuracy: 0.6678\n",
      "Epoch 9: val_loss improved from 1.43192 to 1.33333, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 1.1261 - accuracy: 0.6678 - val_loss: 1.3333 - val_accuracy: 0.6240 - lr: 0.1000\n",
      "Epoch 10/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0226 - accuracy: 0.6942\n",
      "Epoch 10: val_loss improved from 1.33333 to 1.29889, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 1.0226 - accuracy: 0.6942 - val_loss: 1.2989 - val_accuracy: 0.6386 - lr: 0.1000\n",
      "Epoch 11/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9283 - accuracy: 0.7207\n",
      "Epoch 11: val_loss improved from 1.29889 to 1.27332, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.9283 - accuracy: 0.7207 - val_loss: 1.2733 - val_accuracy: 0.6540 - lr: 0.1000\n",
      "Epoch 12/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8393 - accuracy: 0.7458\n",
      "Epoch 12: val_loss improved from 1.27332 to 1.23811, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.8393 - accuracy: 0.7458 - val_loss: 1.2381 - val_accuracy: 0.6556 - lr: 0.1000\n",
      "Epoch 13/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7619 - accuracy: 0.7642\n",
      "Epoch 13: val_loss improved from 1.23811 to 1.21779, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.7619 - accuracy: 0.7642 - val_loss: 1.2178 - val_accuracy: 0.6724 - lr: 0.1000\n",
      "Epoch 14/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.7871\n",
      "Epoch 14: val_loss did not improve from 1.21779\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.6885 - accuracy: 0.7871 - val_loss: 1.3151 - val_accuracy: 0.6598 - lr: 0.1000\n",
      "Epoch 15/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.8055\n",
      "Epoch 15: val_loss improved from 1.21779 to 1.18856, saving model to model_wide_28_10_c100_best_2.hdf5\n",
      "351/351 [==============================] - 128s 363ms/step - loss: 0.6246 - accuracy: 0.8055 - val_loss: 1.1886 - val_accuracy: 0.6846 - lr: 0.1000\n",
      "Epoch 16/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5520 - accuracy: 0.8264\n",
      "Epoch 16: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.5520 - accuracy: 0.8264 - val_loss: 1.3040 - val_accuracy: 0.6778 - lr: 0.1000\n",
      "Epoch 17/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4920 - accuracy: 0.8438\n",
      "Epoch 17: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.4920 - accuracy: 0.8438 - val_loss: 1.2502 - val_accuracy: 0.6820 - lr: 0.1000\n",
      "Epoch 18/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4556 - accuracy: 0.8530\n",
      "Epoch 18: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.4556 - accuracy: 0.8530 - val_loss: 1.3652 - val_accuracy: 0.6676 - lr: 0.1000\n",
      "Epoch 19/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3987 - accuracy: 0.8713\n",
      "Epoch 19: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.3987 - accuracy: 0.8713 - val_loss: 1.3665 - val_accuracy: 0.6884 - lr: 0.1000\n",
      "Epoch 20/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.8849\n",
      "Epoch 20: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.3584 - accuracy: 0.8849 - val_loss: 1.3124 - val_accuracy: 0.6916 - lr: 0.1000\n",
      "Epoch 21/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.8949\n",
      "Epoch 21: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.3228 - accuracy: 0.8949 - val_loss: 1.2898 - val_accuracy: 0.7120 - lr: 0.1000\n",
      "Epoch 22/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9074\n",
      "Epoch 22: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.2863 - accuracy: 0.9074 - val_loss: 1.3643 - val_accuracy: 0.6946 - lr: 0.1000\n",
      "Epoch 23/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9130\n",
      "Epoch 23: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.2648 - accuracy: 0.9130 - val_loss: 1.4421 - val_accuracy: 0.6928 - lr: 0.1000\n",
      "Epoch 24/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.9245\n",
      "Epoch 24: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.2327 - accuracy: 0.9245 - val_loss: 1.4410 - val_accuracy: 0.6954 - lr: 0.1000\n",
      "Epoch 25/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9296\n",
      "Epoch 25: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.2109 - accuracy: 0.9296 - val_loss: 1.4021 - val_accuracy: 0.7018 - lr: 0.1000\n",
      "Epoch 26/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1841 - accuracy: 0.9388\n",
      "Epoch 26: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1841 - accuracy: 0.9388 - val_loss: 1.4325 - val_accuracy: 0.7056 - lr: 0.1000\n",
      "Epoch 27/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1695 - accuracy: 0.9435\n",
      "Epoch 27: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.1695 - accuracy: 0.9435 - val_loss: 1.5184 - val_accuracy: 0.6948 - lr: 0.1000\n",
      "Epoch 28/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9460\n",
      "Epoch 28: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.1621 - accuracy: 0.9460 - val_loss: 1.4617 - val_accuracy: 0.7112 - lr: 0.1000\n",
      "Epoch 29/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9569\n",
      "Epoch 29: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.1335 - accuracy: 0.9569 - val_loss: 1.4382 - val_accuracy: 0.7144 - lr: 0.1000\n",
      "Epoch 30/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9581\n",
      "Epoch 30: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.1256 - accuracy: 0.9581 - val_loss: 1.5618 - val_accuracy: 0.7032 - lr: 0.1000\n",
      "Epoch 31/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9601\n",
      "Epoch 31: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.1223 - accuracy: 0.9601 - val_loss: 1.4920 - val_accuracy: 0.7116 - lr: 0.1000\n",
      "Epoch 32/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9647\n",
      "Epoch 32: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.1088 - accuracy: 0.9647 - val_loss: 1.5026 - val_accuracy: 0.7124 - lr: 0.1000\n",
      "Epoch 33/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9678\n",
      "Epoch 33: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0991 - accuracy: 0.9678 - val_loss: 1.6117 - val_accuracy: 0.7018 - lr: 0.1000\n",
      "Epoch 34/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9658\n",
      "Epoch 34: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1019 - accuracy: 0.9658 - val_loss: 1.5144 - val_accuracy: 0.7142 - lr: 0.1000\n",
      "Epoch 35/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9731\n",
      "Epoch 35: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0830 - accuracy: 0.9731 - val_loss: 1.6407 - val_accuracy: 0.7076 - lr: 0.1000\n",
      "Epoch 36/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9725\n",
      "Epoch 36: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0828 - accuracy: 0.9725 - val_loss: 1.6203 - val_accuracy: 0.7116 - lr: 0.1000\n",
      "Epoch 37/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9756\n",
      "Epoch 37: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0757 - accuracy: 0.9756 - val_loss: 1.6220 - val_accuracy: 0.7176 - lr: 0.1000\n",
      "Epoch 38/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9756\n",
      "Epoch 38: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0747 - accuracy: 0.9756 - val_loss: 1.5844 - val_accuracy: 0.7252 - lr: 0.1000\n",
      "Epoch 39/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9773\n",
      "Epoch 39: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0708 - accuracy: 0.9773 - val_loss: 1.7125 - val_accuracy: 0.7122 - lr: 0.1000\n",
      "Epoch 40/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9790\n",
      "Epoch 40: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0652 - accuracy: 0.9790 - val_loss: 1.6323 - val_accuracy: 0.7178 - lr: 0.1000\n",
      "Epoch 41/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9821\n",
      "Epoch 41: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0565 - accuracy: 0.9821 - val_loss: 1.6741 - val_accuracy: 0.7234 - lr: 0.1000\n",
      "Epoch 42/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9825\n",
      "Epoch 42: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0532 - accuracy: 0.9825 - val_loss: 1.6523 - val_accuracy: 0.7264 - lr: 0.1000\n",
      "Epoch 43/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9850\n",
      "Epoch 43: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0470 - accuracy: 0.9850 - val_loss: 1.7144 - val_accuracy: 0.7228 - lr: 0.1000\n",
      "Epoch 44/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9866\n",
      "Epoch 44: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0430 - accuracy: 0.9866 - val_loss: 1.7043 - val_accuracy: 0.7172 - lr: 0.1000\n",
      "Epoch 45/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9868\n",
      "Epoch 45: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0410 - accuracy: 0.9868 - val_loss: 1.7374 - val_accuracy: 0.7216 - lr: 0.1000\n",
      "Epoch 46/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9862\n",
      "Epoch 46: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0436 - accuracy: 0.9862 - val_loss: 1.6211 - val_accuracy: 0.7228 - lr: 0.1000\n",
      "Epoch 47/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9876\n",
      "Epoch 47: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0396 - accuracy: 0.9876 - val_loss: 1.6909 - val_accuracy: 0.7156 - lr: 0.1000\n",
      "Epoch 48/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9857\n",
      "Epoch 48: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0433 - accuracy: 0.9857 - val_loss: 1.7656 - val_accuracy: 0.7130 - lr: 0.1000\n",
      "Epoch 49/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9868\n",
      "Epoch 49: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0411 - accuracy: 0.9868 - val_loss: 1.7517 - val_accuracy: 0.7158 - lr: 0.1000\n",
      "Epoch 50/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9898\n",
      "Epoch 50: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0337 - accuracy: 0.9898 - val_loss: 1.7400 - val_accuracy: 0.7324 - lr: 0.1000\n",
      "Epoch 51/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9883\n",
      "Epoch 51: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0357 - accuracy: 0.9883 - val_loss: 1.7758 - val_accuracy: 0.7234 - lr: 0.1000\n",
      "Epoch 52/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9880\n",
      "Epoch 52: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0344 - accuracy: 0.9880 - val_loss: 1.7694 - val_accuracy: 0.7208 - lr: 0.1000\n",
      "Epoch 53/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9897\n",
      "Epoch 53: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0342 - accuracy: 0.9897 - val_loss: 1.7467 - val_accuracy: 0.7268 - lr: 0.1000\n",
      "Epoch 54/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9887\n",
      "Epoch 54: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0351 - accuracy: 0.9887 - val_loss: 1.8493 - val_accuracy: 0.7180 - lr: 0.1000\n",
      "Epoch 55/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9908\n",
      "Epoch 55: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0306 - accuracy: 0.9908 - val_loss: 1.7507 - val_accuracy: 0.7284 - lr: 0.1000\n",
      "Epoch 56/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9914\n",
      "Epoch 56: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0292 - accuracy: 0.9914 - val_loss: 1.7871 - val_accuracy: 0.7232 - lr: 0.1000\n",
      "Epoch 57/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9908\n",
      "Epoch 57: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0312 - accuracy: 0.9908 - val_loss: 1.7425 - val_accuracy: 0.7190 - lr: 0.1000\n",
      "Epoch 58/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9904\n",
      "Epoch 58: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0284 - accuracy: 0.9904 - val_loss: 1.7746 - val_accuracy: 0.7240 - lr: 0.1000\n",
      "Epoch 59/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9894\n",
      "Epoch 59: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0327 - accuracy: 0.9894 - val_loss: 1.8654 - val_accuracy: 0.7130 - lr: 0.1000\n",
      "Epoch 60/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9919\n",
      "Epoch 60: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0276 - accuracy: 0.9919 - val_loss: 1.8054 - val_accuracy: 0.7260 - lr: 0.1000\n",
      "Epoch 61/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9959\n",
      "Epoch 61: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0135 - accuracy: 0.9959 - val_loss: 1.6776 - val_accuracy: 0.7378 - lr: 0.0200\n",
      "Epoch 62/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9983\n",
      "Epoch 62: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 1.6226 - val_accuracy: 0.7466 - lr: 0.0200\n",
      "Epoch 63/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9986\n",
      "Epoch 63: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 1.6428 - val_accuracy: 0.7452 - lr: 0.0200\n",
      "Epoch 64/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9991\n",
      "Epoch 64: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 1.6219 - val_accuracy: 0.7476 - lr: 0.0200\n",
      "Epoch 65/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 65: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 1.6952 - val_accuracy: 0.7428 - lr: 0.0200\n",
      "Epoch 66/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 66: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 1.6491 - val_accuracy: 0.7464 - lr: 0.0200\n",
      "Epoch 67/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 67: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 363ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 1.5728 - val_accuracy: 0.7506 - lr: 0.0200\n",
      "Epoch 68/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.9991\n",
      "Epoch 68: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 1.6441 - val_accuracy: 0.7464 - lr: 0.0200\n",
      "Epoch 69/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 69: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 1.5988 - val_accuracy: 0.7462 - lr: 0.0200\n",
      "Epoch 70/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 70: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 1.6615 - val_accuracy: 0.7498 - lr: 0.0200\n",
      "Epoch 71/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 71: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 129s 367ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 1.5942 - val_accuracy: 0.7506 - lr: 0.0200\n",
      "Epoch 72/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 72: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 1.6560 - val_accuracy: 0.7428 - lr: 0.0200\n",
      "Epoch 73/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 73: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 1.6928 - val_accuracy: 0.7466 - lr: 0.0200\n",
      "Epoch 74/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 74: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 1.6452 - val_accuracy: 0.7494 - lr: 0.0200\n",
      "Epoch 75/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 75: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 1.6186 - val_accuracy: 0.7496 - lr: 0.0200\n",
      "Epoch 76/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 76: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 1.6416 - val_accuracy: 0.7474 - lr: 0.0200\n",
      "Epoch 77/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 77: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 1.6402 - val_accuracy: 0.7498 - lr: 0.0200\n",
      "Epoch 78/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 78: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 365ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 1.6223 - val_accuracy: 0.7466 - lr: 0.0200\n",
      "Epoch 79/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 79: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 365ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 1.6050 - val_accuracy: 0.7494 - lr: 0.0200\n",
      "Epoch 80/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 80: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 1.6718 - val_accuracy: 0.7520 - lr: 0.0200\n",
      "Epoch 81/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 81: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 129s 367ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 1.6543 - val_accuracy: 0.7506 - lr: 0.0200\n",
      "Epoch 82/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 82: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 1.6247 - val_accuracy: 0.7488 - lr: 0.0200\n",
      "Epoch 83/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 83: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 1.6391 - val_accuracy: 0.7538 - lr: 0.0200\n",
      "Epoch 84/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 84: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 365ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 1.6516 - val_accuracy: 0.7512 - lr: 0.0200\n",
      "Epoch 85/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 85: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 365ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 1.5920 - val_accuracy: 0.7540 - lr: 0.0200\n",
      "Epoch 86/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 86: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 1.5798 - val_accuracy: 0.7530 - lr: 0.0200\n",
      "Epoch 87/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 87: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 1.6808 - val_accuracy: 0.7472 - lr: 0.0200\n",
      "Epoch 88/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 88: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.5956 - val_accuracy: 0.7560 - lr: 0.0200\n",
      "Epoch 89/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 89: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 1.6313 - val_accuracy: 0.7508 - lr: 0.0200\n",
      "Epoch 90/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 90: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.6421 - val_accuracy: 0.7508 - lr: 0.0200\n",
      "Epoch 91/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 91: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 365ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.6625 - val_accuracy: 0.7458 - lr: 0.0200\n",
      "Epoch 92/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 92: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 363ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 1.6151 - val_accuracy: 0.7506 - lr: 0.0200\n",
      "Epoch 93/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 93: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.6142 - val_accuracy: 0.7482 - lr: 0.0200\n",
      "Epoch 94/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 94: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.6747 - val_accuracy: 0.7466 - lr: 0.0200\n",
      "Epoch 95/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 95: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 1.6247 - val_accuracy: 0.7516 - lr: 0.0200\n",
      "Epoch 96/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 96: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 1.6336 - val_accuracy: 0.7516 - lr: 0.0200\n",
      "Epoch 97/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 97: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.6368 - val_accuracy: 0.7494 - lr: 0.0200\n",
      "Epoch 98/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 98: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.7001 - val_accuracy: 0.7440 - lr: 0.0200\n",
      "Epoch 99/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 99: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.6094 - val_accuracy: 0.7520 - lr: 0.0200\n",
      "Epoch 100/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 100: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.6443 - val_accuracy: 0.7516 - lr: 0.0200\n",
      "Epoch 101/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 101: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 365ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.6444 - val_accuracy: 0.7514 - lr: 0.0200\n",
      "Epoch 102/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 102: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 1.6035 - val_accuracy: 0.7560 - lr: 0.0200\n",
      "Epoch 103/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 103: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 1.6117 - val_accuracy: 0.7574 - lr: 0.0200\n",
      "Epoch 104/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 104: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.7051 - val_accuracy: 0.7498 - lr: 0.0200\n",
      "Epoch 105/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 105: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.6786 - val_accuracy: 0.7542 - lr: 0.0200\n",
      "Epoch 106/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 106: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.6415 - val_accuracy: 0.7524 - lr: 0.0200\n",
      "Epoch 107/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 107: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 1.6596 - val_accuracy: 0.7506 - lr: 0.0200\n",
      "Epoch 108/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 108: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.6235 - val_accuracy: 0.7542 - lr: 0.0200\n",
      "Epoch 109/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 109: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.6960 - val_accuracy: 0.7490 - lr: 0.0200\n",
      "Epoch 110/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 110: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.6249 - val_accuracy: 0.7526 - lr: 0.0200\n",
      "Epoch 111/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 111: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 128s 365ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.6391 - val_accuracy: 0.7508 - lr: 0.0200\n",
      "Epoch 112/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 112: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.6645 - val_accuracy: 0.7494 - lr: 0.0200\n",
      "Epoch 113/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 113: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.6607 - val_accuracy: 0.7534 - lr: 0.0200\n",
      "Epoch 114/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 114: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.6626 - val_accuracy: 0.7510 - lr: 0.0200\n",
      "Epoch 115/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 115: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 1.7238 - val_accuracy: 0.7480 - lr: 0.0200\n",
      "Epoch 116/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 116: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.7055 - val_accuracy: 0.7498 - lr: 0.0200\n",
      "Epoch 117/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 117: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 1.6280 - val_accuracy: 0.7574 - lr: 0.0200\n",
      "Epoch 118/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 118: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.6866 - val_accuracy: 0.7530 - lr: 0.0200\n",
      "Epoch 119/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.5703e-04 - accuracy: 0.9997\n",
      "Epoch 119: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 9.5703e-04 - accuracy: 0.9997 - val_loss: 1.6609 - val_accuracy: 0.7524 - lr: 0.0200\n",
      "Epoch 120/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 120: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.6220 - val_accuracy: 0.7554 - lr: 0.0200\n",
      "Epoch 1/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 1: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 54s 145ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 1.6340 - val_accuracy: 0.7540 - lr: 0.0040\n",
      "Epoch 2/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 2: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.6165 - val_accuracy: 0.7556 - lr: 0.0040\n",
      "Epoch 3/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.5790e-04 - accuracy: 0.9998\n",
      "Epoch 3: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 9.5790e-04 - accuracy: 0.9998 - val_loss: 1.6297 - val_accuracy: 0.7596 - lr: 0.0040\n",
      "Epoch 4/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.5358e-04 - accuracy: 0.9998\n",
      "Epoch 4: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 7.5358e-04 - accuracy: 0.9998 - val_loss: 1.6339 - val_accuracy: 0.7550 - lr: 0.0040\n",
      "Epoch 5/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.8129e-04 - accuracy: 0.9998\n",
      "Epoch 5: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 8.8129e-04 - accuracy: 0.9998 - val_loss: 1.6743 - val_accuracy: 0.7538 - lr: 0.0040\n",
      "Epoch 6/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.8006e-04 - accuracy: 0.9998\n",
      "Epoch 6: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 8.8006e-04 - accuracy: 0.9998 - val_loss: 1.6619 - val_accuracy: 0.7566 - lr: 0.0040\n",
      "Epoch 7/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0269e-04 - accuracy: 0.9998\n",
      "Epoch 7: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 8.0269e-04 - accuracy: 0.9998 - val_loss: 1.6222 - val_accuracy: 0.7568 - lr: 0.0040\n",
      "Epoch 8/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.1334e-04 - accuracy: 0.9997\n",
      "Epoch 8: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 9.1334e-04 - accuracy: 0.9997 - val_loss: 1.6330 - val_accuracy: 0.7600 - lr: 0.0040\n",
      "Epoch 9/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 9: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.6846 - val_accuracy: 0.7582 - lr: 0.0040\n",
      "Epoch 10/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.7368e-04 - accuracy: 0.9998\n",
      "Epoch 10: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 8.7368e-04 - accuracy: 0.9998 - val_loss: 1.6258 - val_accuracy: 0.7558 - lr: 0.0040\n",
      "Epoch 11/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2016e-04 - accuracy: 0.9998\n",
      "Epoch 11: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 7.2016e-04 - accuracy: 0.9998 - val_loss: 1.6040 - val_accuracy: 0.7542 - lr: 0.0040\n",
      "Epoch 12/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.0115e-04 - accuracy: 0.9998\n",
      "Epoch 12: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 9.0115e-04 - accuracy: 0.9998 - val_loss: 1.6258 - val_accuracy: 0.7540 - lr: 0.0040\n",
      "Epoch 13/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9996\n",
      "Epoch 13: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.6254 - val_accuracy: 0.7532 - lr: 0.0040\n",
      "Epoch 14/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.0722e-04 - accuracy: 0.9998\n",
      "Epoch 14: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 7.0722e-04 - accuracy: 0.9998 - val_loss: 1.6369 - val_accuracy: 0.7560 - lr: 0.0040\n",
      "Epoch 15/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9474e-04 - accuracy: 0.9998\n",
      "Epoch 15: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 7.9474e-04 - accuracy: 0.9998 - val_loss: 1.6646 - val_accuracy: 0.7548 - lr: 0.0040\n",
      "Epoch 16/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.1927e-04 - accuracy: 0.9997\n",
      "Epoch 16: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 50s 142ms/step - loss: 8.1927e-04 - accuracy: 0.9997 - val_loss: 1.7066 - val_accuracy: 0.7544 - lr: 0.0040\n",
      "Epoch 17/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.1578e-04 - accuracy: 0.9997\n",
      "Epoch 17: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 9.1578e-04 - accuracy: 0.9997 - val_loss: 1.6150 - val_accuracy: 0.7522 - lr: 0.0040\n",
      "Epoch 18/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.0253e-04 - accuracy: 0.9998\n",
      "Epoch 18: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 140ms/step - loss: 7.0253e-04 - accuracy: 0.9998 - val_loss: 1.6497 - val_accuracy: 0.7554 - lr: 0.0040\n",
      "Epoch 19/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.5251e-04 - accuracy: 0.9997\n",
      "Epoch 19: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 8.5251e-04 - accuracy: 0.9997 - val_loss: 1.6314 - val_accuracy: 0.7556 - lr: 0.0040\n",
      "Epoch 20/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.5052e-04 - accuracy: 0.9997\n",
      "Epoch 20: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 8.5052e-04 - accuracy: 0.9997 - val_loss: 1.6154 - val_accuracy: 0.7580 - lr: 0.0040\n",
      "Epoch 21/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.4747e-04 - accuracy: 0.9999\n",
      "Epoch 21: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 7.4747e-04 - accuracy: 0.9999 - val_loss: 1.6170 - val_accuracy: 0.7548 - lr: 0.0040\n",
      "Epoch 22/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.4543e-04 - accuracy: 0.9998\n",
      "Epoch 22: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 140ms/step - loss: 7.4543e-04 - accuracy: 0.9998 - val_loss: 1.6332 - val_accuracy: 0.7540 - lr: 0.0040\n",
      "Epoch 23/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6133e-04 - accuracy: 0.9998\n",
      "Epoch 23: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 140ms/step - loss: 8.6133e-04 - accuracy: 0.9998 - val_loss: 1.6488 - val_accuracy: 0.7578 - lr: 0.0040\n",
      "Epoch 24/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.0720e-04 - accuracy: 0.9998\n",
      "Epoch 24: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 53s 151ms/step - loss: 9.0720e-04 - accuracy: 0.9998 - val_loss: 1.6221 - val_accuracy: 0.7566 - lr: 0.0040\n",
      "Epoch 25/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.9251e-04 - accuracy: 0.9998\n",
      "Epoch 25: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 8.9251e-04 - accuracy: 0.9998 - val_loss: 1.5979 - val_accuracy: 0.7574 - lr: 0.0040\n",
      "Epoch 26/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9827e-04 - accuracy: 0.9998\n",
      "Epoch 26: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 7.9827e-04 - accuracy: 0.9998 - val_loss: 1.6265 - val_accuracy: 0.7554 - lr: 0.0040\n",
      "Epoch 27/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9372e-04 - accuracy: 0.9998\n",
      "Epoch 27: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 7.9372e-04 - accuracy: 0.9998 - val_loss: 1.6486 - val_accuracy: 0.7538 - lr: 0.0040\n",
      "Epoch 28/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.1560e-04 - accuracy: 0.9998\n",
      "Epoch 28: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 8.1560e-04 - accuracy: 0.9998 - val_loss: 1.6409 - val_accuracy: 0.7552 - lr: 0.0040\n",
      "Epoch 29/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.2614e-04 - accuracy: 0.9998\n",
      "Epoch 29: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 8.2614e-04 - accuracy: 0.9998 - val_loss: 1.6364 - val_accuracy: 0.7556 - lr: 0.0040\n",
      "Epoch 30/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.1551e-04 - accuracy: 0.9999\n",
      "Epoch 30: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 7.1551e-04 - accuracy: 0.9999 - val_loss: 1.6595 - val_accuracy: 0.7582 - lr: 0.0040\n",
      "Epoch 31/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.9964e-04 - accuracy: 0.9997\n",
      "Epoch 31: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 54s 152ms/step - loss: 8.9964e-04 - accuracy: 0.9997 - val_loss: 1.6320 - val_accuracy: 0.7550 - lr: 0.0040\n",
      "Epoch 32/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2577e-04 - accuracy: 0.9998\n",
      "Epoch 32: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 7.2577e-04 - accuracy: 0.9998 - val_loss: 1.6121 - val_accuracy: 0.7532 - lr: 0.0040\n",
      "Epoch 33/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2632e-04 - accuracy: 0.9998\n",
      "Epoch 33: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 7.2632e-04 - accuracy: 0.9998 - val_loss: 1.6545 - val_accuracy: 0.7564 - lr: 0.0040\n",
      "Epoch 34/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.4684e-04 - accuracy: 0.9997\n",
      "Epoch 34: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 9.4684e-04 - accuracy: 0.9997 - val_loss: 1.6121 - val_accuracy: 0.7570 - lr: 0.0040\n",
      "Epoch 35/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7447e-04 - accuracy: 0.9998\n",
      "Epoch 35: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 7.7447e-04 - accuracy: 0.9998 - val_loss: 1.6619 - val_accuracy: 0.7572 - lr: 0.0040\n",
      "Epoch 36/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.1159e-04 - accuracy: 0.9998\n",
      "Epoch 36: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 7.1159e-04 - accuracy: 0.9998 - val_loss: 1.6197 - val_accuracy: 0.7558 - lr: 0.0040\n",
      "Epoch 37/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.0666e-04 - accuracy: 0.9997\n",
      "Epoch 37: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 9.0666e-04 - accuracy: 0.9997 - val_loss: 1.6350 - val_accuracy: 0.7552 - lr: 0.0040\n",
      "Epoch 38/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7751e-04 - accuracy: 0.9998\n",
      "Epoch 38: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 7.7751e-04 - accuracy: 0.9998 - val_loss: 1.6652 - val_accuracy: 0.7556 - lr: 0.0040\n",
      "Epoch 39/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0758e-04 - accuracy: 0.9998\n",
      "Epoch 39: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 8.0758e-04 - accuracy: 0.9998 - val_loss: 1.6476 - val_accuracy: 0.7574 - lr: 0.0040\n",
      "Epoch 40/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.5343e-04 - accuracy: 0.9998\n",
      "Epoch 40: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 7.5343e-04 - accuracy: 0.9998 - val_loss: 1.6547 - val_accuracy: 0.7582 - lr: 0.0040\n",
      "Epoch 41/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.0421e-04 - accuracy: 0.9997\n",
      "Epoch 41: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 140ms/step - loss: 9.0421e-04 - accuracy: 0.9997 - val_loss: 1.6455 - val_accuracy: 0.7544 - lr: 8.0000e-04\n",
      "Epoch 42/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.4826e-04 - accuracy: 0.9998\n",
      "Epoch 42: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 6.4826e-04 - accuracy: 0.9998 - val_loss: 1.6356 - val_accuracy: 0.7560 - lr: 8.0000e-04\n",
      "Epoch 43/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6402e-04 - accuracy: 0.9998\n",
      "Epoch 43: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 8.6402e-04 - accuracy: 0.9998 - val_loss: 1.6543 - val_accuracy: 0.7542 - lr: 8.0000e-04\n",
      "Epoch 44/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.8091e-04 - accuracy: 0.9998\n",
      "Epoch 44: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 8.8091e-04 - accuracy: 0.9998 - val_loss: 1.6570 - val_accuracy: 0.7554 - lr: 8.0000e-04\n",
      "Epoch 45/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7036e-04 - accuracy: 0.9998\n",
      "Epoch 45: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 7.7036e-04 - accuracy: 0.9998 - val_loss: 1.6292 - val_accuracy: 0.7554 - lr: 8.0000e-04\n",
      "Epoch 46/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.5082e-04 - accuracy: 0.9998\n",
      "Epoch 46: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 7.5082e-04 - accuracy: 0.9998 - val_loss: 1.6373 - val_accuracy: 0.7554 - lr: 8.0000e-04\n",
      "Epoch 47/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0922e-04 - accuracy: 0.9998\n",
      "Epoch 47: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 8.0922e-04 - accuracy: 0.9998 - val_loss: 1.6396 - val_accuracy: 0.7526 - lr: 8.0000e-04\n",
      "Epoch 48/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.1311e-04 - accuracy: 0.9998\n",
      "Epoch 48: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 53s 151ms/step - loss: 9.1311e-04 - accuracy: 0.9998 - val_loss: 1.6198 - val_accuracy: 0.7570 - lr: 8.0000e-04\n",
      "Epoch 49/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.3030e-04 - accuracy: 0.9998\n",
      "Epoch 49: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 8.3030e-04 - accuracy: 0.9998 - val_loss: 1.6357 - val_accuracy: 0.7548 - lr: 8.0000e-04\n",
      "Epoch 50/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.5200e-04 - accuracy: 0.9997\n",
      "Epoch 50: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 9.5200e-04 - accuracy: 0.9997 - val_loss: 1.6038 - val_accuracy: 0.7562 - lr: 8.0000e-04\n",
      "Epoch 51/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6036e-04 - accuracy: 0.9998\n",
      "Epoch 51: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 8.6036e-04 - accuracy: 0.9998 - val_loss: 1.6366 - val_accuracy: 0.7558 - lr: 8.0000e-04\n",
      "Epoch 52/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.1848e-04 - accuracy: 0.9997\n",
      "Epoch 52: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 8.1848e-04 - accuracy: 0.9997 - val_loss: 1.6316 - val_accuracy: 0.7548 - lr: 8.0000e-04\n",
      "Epoch 53/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.1702e-04 - accuracy: 0.9998\n",
      "Epoch 53: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 53s 151ms/step - loss: 9.1702e-04 - accuracy: 0.9998 - val_loss: 1.6351 - val_accuracy: 0.7548 - lr: 8.0000e-04\n",
      "Epoch 54/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6448e-04 - accuracy: 0.9997\n",
      "Epoch 54: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 8.6448e-04 - accuracy: 0.9997 - val_loss: 1.6616 - val_accuracy: 0.7566 - lr: 8.0000e-04\n",
      "Epoch 55/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.5647e-04 - accuracy: 0.9998\n",
      "Epoch 55: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 7.5647e-04 - accuracy: 0.9998 - val_loss: 1.6265 - val_accuracy: 0.7564 - lr: 8.0000e-04\n",
      "Epoch 56/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.3793e-04 - accuracy: 0.9998\n",
      "Epoch 56: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 8.3793e-04 - accuracy: 0.9998 - val_loss: 1.6735 - val_accuracy: 0.7550 - lr: 8.0000e-04\n",
      "Epoch 57/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.5524e-04 - accuracy: 0.9997\n",
      "Epoch 57: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 7.5524e-04 - accuracy: 0.9997 - val_loss: 1.6233 - val_accuracy: 0.7548 - lr: 8.0000e-04\n",
      "Epoch 58/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.9576e-04 - accuracy: 0.9998\n",
      "Epoch 58: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 6.9576e-04 - accuracy: 0.9998 - val_loss: 1.6759 - val_accuracy: 0.7590 - lr: 8.0000e-04\n",
      "Epoch 59/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7959e-04 - accuracy: 0.9998\n",
      "Epoch 59: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 7.7959e-04 - accuracy: 0.9998 - val_loss: 1.6686 - val_accuracy: 0.7542 - lr: 8.0000e-04\n",
      "Epoch 60/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9703e-04 - accuracy: 0.9997\n",
      "Epoch 60: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 7.9703e-04 - accuracy: 0.9997 - val_loss: 1.6166 - val_accuracy: 0.7576 - lr: 8.0000e-04\n",
      "Epoch 61/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.6681e-04 - accuracy: 0.9998\n",
      "Epoch 61: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 7.6681e-04 - accuracy: 0.9998 - val_loss: 1.6272 - val_accuracy: 0.7578 - lr: 8.0000e-04\n",
      "Epoch 62/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.6919e-04 - accuracy: 0.9998\n",
      "Epoch 62: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 7.6919e-04 - accuracy: 0.9998 - val_loss: 1.6243 - val_accuracy: 0.7560 - lr: 8.0000e-04\n",
      "Epoch 63/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.8767e-04 - accuracy: 0.9997\n",
      "Epoch 63: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 8.8767e-04 - accuracy: 0.9997 - val_loss: 1.6075 - val_accuracy: 0.7552 - lr: 8.0000e-04\n",
      "Epoch 64/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.5467e-04 - accuracy: 0.9998\n",
      "Epoch 64: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 7.5467e-04 - accuracy: 0.9998 - val_loss: 1.6524 - val_accuracy: 0.7564 - lr: 8.0000e-04\n",
      "Epoch 65/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.2270e-04 - accuracy: 0.9997\n",
      "Epoch 65: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 8.2270e-04 - accuracy: 0.9997 - val_loss: 1.6366 - val_accuracy: 0.7556 - lr: 8.0000e-04\n",
      "Epoch 66/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.1378e-04 - accuracy: 0.9998\n",
      "Epoch 66: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 7.1378e-04 - accuracy: 0.9998 - val_loss: 1.6259 - val_accuracy: 0.7562 - lr: 8.0000e-04\n",
      "Epoch 67/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.2807e-04 - accuracy: 0.9999\n",
      "Epoch 67: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 7.2807e-04 - accuracy: 0.9999 - val_loss: 1.6455 - val_accuracy: 0.7586 - lr: 8.0000e-04\n",
      "Epoch 68/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.3338e-04 - accuracy: 0.9998\n",
      "Epoch 68: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 7.3338e-04 - accuracy: 0.9998 - val_loss: 1.6675 - val_accuracy: 0.7576 - lr: 8.0000e-04\n",
      "Epoch 69/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.4821e-04 - accuracy: 0.9997\n",
      "Epoch 69: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 9.4821e-04 - accuracy: 0.9997 - val_loss: 1.6653 - val_accuracy: 0.7548 - lr: 8.0000e-04\n",
      "Epoch 70/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7872e-04 - accuracy: 0.9998\n",
      "Epoch 70: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 7.7872e-04 - accuracy: 0.9998 - val_loss: 1.6169 - val_accuracy: 0.7576 - lr: 8.0000e-04\n",
      "Epoch 71/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8861e-04 - accuracy: 0.9998\n",
      "Epoch 71: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 7.8861e-04 - accuracy: 0.9998 - val_loss: 1.6531 - val_accuracy: 0.7574 - lr: 8.0000e-04\n",
      "Epoch 72/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.4101e-04 - accuracy: 0.9998\n",
      "Epoch 72: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 8.4101e-04 - accuracy: 0.9998 - val_loss: 1.6575 - val_accuracy: 0.7538 - lr: 8.0000e-04\n",
      "Current:  103\n",
      "Epoch 1/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.0507e-04 - accuracy: 0.9998\n",
      "Epoch 1: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 49s 131ms/step - loss: 7.0507e-04 - accuracy: 0.9998 - val_loss: 1.6575 - val_accuracy: 0.7538 - lr: 8.0000e-04\n",
      "Epoch 2/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.0459e-04 - accuracy: 0.9998\n",
      "Epoch 2: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 7.0459e-04 - accuracy: 0.9998 - val_loss: 1.6574 - val_accuracy: 0.7538 - lr: 8.0000e-04\n",
      "Epoch 3/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.2953e-04 - accuracy: 0.9997\n",
      "Epoch 3: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 8.2953e-04 - accuracy: 0.9997 - val_loss: 1.6574 - val_accuracy: 0.7540 - lr: 8.0000e-04\n",
      "Epoch 4/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.9686e-04 - accuracy: 0.9997\n",
      "Epoch 4: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 6.9686e-04 - accuracy: 0.9997 - val_loss: 1.6573 - val_accuracy: 0.7540 - lr: 8.0000e-04\n",
      "Epoch 5/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7913e-04 - accuracy: 0.9998\n",
      "Epoch 5: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 50s 142ms/step - loss: 7.7913e-04 - accuracy: 0.9998 - val_loss: 1.6572 - val_accuracy: 0.7540 - lr: 8.0000e-04\n",
      "Epoch 6/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.1618e-04 - accuracy: 0.9997\n",
      "Epoch 6: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 8.1618e-04 - accuracy: 0.9997 - val_loss: 1.6571 - val_accuracy: 0.7540 - lr: 8.0000e-04\n",
      "Epoch 7/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.4554e-04 - accuracy: 0.9998\n",
      "Epoch 7: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 7.4554e-04 - accuracy: 0.9998 - val_loss: 1.6571 - val_accuracy: 0.7540 - lr: 8.0000e-04\n",
      "Epoch 8/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.6474e-04 - accuracy: 0.9998\n",
      "Epoch 8: val_loss did not improve from 1.18856\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 6.6474e-04 - accuracy: 0.9998 - val_loss: 1.6570 - val_accuracy: 0.7540 - lr: 8.0000e-04\n",
      "Current:  112\n",
      "313/313 [==============================] - 11s 32ms/step\n",
      "Accuracy: 74.45\n",
      "Error: 25.549999999999997\n",
      "ECE: 0.1824167468354106\n",
      "MCE: 0.42276842810679227\n",
      "Loss: 1.780017128846754\n",
      "brier: 0.23437533662312643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[25.549999999999997,\n",
       " 0.1824167468354106,\n",
       " 0.42276842810679227,\n",
       " 1.780017128846754,\n",
       " 0.23437533662312643]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, datagen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list,lr_schedule = [[0, 0.1],[epochs*0.3,0.02],[epochs*0.6,0.004],[epochs*0.8,0.0008]],cbks=[checkpointer], name='resnet_wide_cifar100_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21599.503541,
   "end_time": "2023-04-25T15:52:37.784877",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-25T09:52:38.281336",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
