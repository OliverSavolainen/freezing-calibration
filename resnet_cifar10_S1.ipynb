{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b09f8b-c683-4b1e-be79-3b82149ddb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import Input, Model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import (add,\n",
    "                          Conv2D, GlobalAveragePooling2D)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import freezing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ee168-6429-44d6-9edd-e37b74dcefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training procedure for CIFAR-10 using ResNet 110.\n",
    "# ResNet model from https://github.com/BIGBALLON/cifar-10-cnn/blob/master/4_Residual_Network/ResNet_keras.py\n",
    "stack_n = 18\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "iterations = 45000 // batch_size\n",
    "weight_decay = 0.0001\n",
    "seed = 333\n",
    "\n",
    "\n",
    "def residual_network(img_input, classes_num=10, stack_n=5):\n",
    "    def residual_block(intput, out_channel, increase=False):\n",
    "        if increase:\n",
    "            stride = (2, 2)\n",
    "        else:\n",
    "            stride = (1, 1)\n",
    "\n",
    "        pre_bn = BatchNormalization()(intput)\n",
    "        pre_relu = Activation('relu')(pre_bn)\n",
    "\n",
    "        conv_1 = Conv2D(out_channel, kernel_size=(3, 3), strides=stride, padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(pre_relu)\n",
    "        bn_1 = BatchNormalization()(conv_1)\n",
    "        relu1 = Activation('relu')(bn_1)\n",
    "        conv_2 = Conv2D(out_channel, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(relu1)\n",
    "        if increase:\n",
    "            projection = Conv2D(out_channel,\n",
    "                                kernel_size=(1, 1),\n",
    "                                strides=(2, 2),\n",
    "                                padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(weight_decay))(intput)\n",
    "            block = add([conv_2, projection])\n",
    "        else:\n",
    "            block = add([intput, conv_2])\n",
    "        return block\n",
    "\n",
    "    # build model\n",
    "    # total layers = stack_n * 3 * 2 + 2\n",
    "    # stack_n = 5 by default, total layers = 32\n",
    "    # input: 32x32x3 output: 32x32x16\n",
    "    x = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "\n",
    "    # input: 32x32x16 output: 32x32x16\n",
    "    for _ in range(stack_n):\n",
    "        x = residual_block(x, 16, False)\n",
    "\n",
    "    # input: 32x32x16 output: 16x16x32\n",
    "    x = residual_block(x, 32, True)\n",
    "    for _ in range(1, stack_n):\n",
    "        x = residual_block(x, 32, False)\n",
    "\n",
    "    # input: 16x16x32 output: 8x8x64\n",
    "    x = residual_block(x, 64, True)\n",
    "    for _ in range(1, stack_n):\n",
    "        x = residual_block(x, 64, False)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # input: 64 output: 10\n",
    "    x = Dense(classes_num, activation='softmax',\n",
    "              kernel_initializer=\"he_normal\",\n",
    "              kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1,\n",
    "                                                      random_state=seed)  # random_state = seed\n",
    "\n",
    "img_mean = x_train45.mean(axis=0)  # per-pixel mean, what about std?\n",
    "img_std = x_train45.std(axis=0)\n",
    "x_train45 = (x_train45 - img_mean) / img_std\n",
    "x_val = (x_val - img_mean) / img_std\n",
    "x_test = (x_test - img_mean) / img_std\n",
    "\n",
    "# build network\n",
    "img_input = Input(shape=(img_rows, img_cols, img_channels))\n",
    "output = residual_network(img_input, num_classes, stack_n)\n",
    "model = Model(img_input, output)\n",
    "print(model.summary())\n",
    "\n",
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant', cval=0.)\n",
    "\n",
    "datagen.fit(x_train45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e63caf-1499-43a5-900e-5c0b3403a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "    if i < len(model.layers) * 0.8:\n",
    "        freezing_list.append(int(epochs * 0.6))\n",
    "freezing_list.append(epochs)\n",
    "checkpointer = ModelCheckpoint('model_resnet_c10_best.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b037ac09-8587-47b6-a127-af3bf6837085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_100\n",
      ".........vars\n",
      "......activation_101\n",
      ".........vars\n",
      "......activation_102\n",
      ".........vars\n",
      "......activation_103\n",
      ".........vars\n",
      "......activation_104\n",
      ".........vars\n",
      "......activation_105\n",
      ".........vars\n",
      "......activation_106\n",
      ".........vars\n",
      "......activation_107\n",
      ".........vars\n",
      "......activation_108\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_55\n",
      ".........vars\n",
      "......activation_56\n",
      ".........vars\n",
      "......activation_57\n",
      ".........vars\n",
      "......activation_58\n",
      ".........vars\n",
      "......activation_59\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_60\n",
      ".........vars\n",
      "......activation_61\n",
      ".........vars\n",
      "......activation_62\n",
      ".........vars\n",
      "......activation_63\n",
      ".........vars\n",
      "......activation_64\n",
      ".........vars\n",
      "......activation_65\n",
      ".........vars\n",
      "......activation_66\n",
      ".........vars\n",
      "......activation_67\n",
      ".........vars\n",
      "......activation_68\n",
      ".........vars\n",
      "......activation_69\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_70\n",
      ".........vars\n",
      "......activation_71\n",
      ".........vars\n",
      "......activation_72\n",
      ".........vars\n",
      "......activation_73\n",
      ".........vars\n",
      "......activation_74\n",
      ".........vars\n",
      "......activation_75\n",
      ".........vars\n",
      "......activation_76\n",
      ".........vars\n",
      "......activation_77\n",
      ".........vars\n",
      "......activation_78\n",
      ".........vars\n",
      "......activation_79\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_80\n",
      ".........vars\n",
      "......activation_81\n",
      ".........vars\n",
      "......activation_82\n",
      ".........vars\n",
      "......activation_83\n",
      ".........vars\n",
      "......activation_84\n",
      ".........vars\n",
      "......activation_85\n",
      ".........vars\n",
      "......activation_86\n",
      ".........vars\n",
      "......activation_87\n",
      ".........vars\n",
      "......activation_88\n",
      ".........vars\n",
      "......activation_89\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......activation_90\n",
      ".........vars\n",
      "......activation_91\n",
      ".........vars\n",
      "......activation_92\n",
      ".........vars\n",
      "......activation_93\n",
      ".........vars\n",
      "......activation_94\n",
      ".........vars\n",
      "......activation_95\n",
      ".........vars\n",
      "......activation_96\n",
      ".........vars\n",
      "......activation_97\n",
      ".........vars\n",
      "......activation_98\n",
      ".........vars\n",
      "......activation_99\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_109\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_110\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-21 10:23:18           64\n",
      "config.json                                    2023-04-21 10:23:18       179538\n",
      "variables.h5                                   2023-04-21 10:23:19      7938896\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-21 10:23:18           64\n",
      "config.json                                    2023-04-21 10:23:18       179538\n",
      "variables.h5                                   2023-04-21 10:23:18      7938896\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_100\n",
      ".........vars\n",
      "......activation_101\n",
      ".........vars\n",
      "......activation_102\n",
      ".........vars\n",
      "......activation_103\n",
      ".........vars\n",
      "......activation_104\n",
      ".........vars\n",
      "......activation_105\n",
      ".........vars\n",
      "......activation_106\n",
      ".........vars\n",
      "......activation_107\n",
      ".........vars\n",
      "......activation_108\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_55\n",
      ".........vars\n",
      "......activation_56\n",
      ".........vars\n",
      "......activation_57\n",
      ".........vars\n",
      "......activation_58\n",
      ".........vars\n",
      "......activation_59\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_60\n",
      ".........vars\n",
      "......activation_61\n",
      ".........vars\n",
      "......activation_62\n",
      ".........vars\n",
      "......activation_63\n",
      ".........vars\n",
      "......activation_64\n",
      ".........vars\n",
      "......activation_65\n",
      ".........vars\n",
      "......activation_66\n",
      ".........vars\n",
      "......activation_67\n",
      ".........vars\n",
      "......activation_68\n",
      ".........vars\n",
      "......activation_69\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_70\n",
      ".........vars\n",
      "......activation_71\n",
      ".........vars\n",
      "......activation_72\n",
      ".........vars\n",
      "......activation_73\n",
      ".........vars\n",
      "......activation_74\n",
      ".........vars\n",
      "......activation_75\n",
      ".........vars\n",
      "......activation_76\n",
      ".........vars\n",
      "......activation_77\n",
      ".........vars\n",
      "......activation_78\n",
      ".........vars\n",
      "......activation_79\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_80\n",
      ".........vars\n",
      "......activation_81\n",
      ".........vars\n",
      "......activation_82\n",
      ".........vars\n",
      "......activation_83\n",
      ".........vars\n",
      "......activation_84\n",
      ".........vars\n",
      "......activation_85\n",
      ".........vars\n",
      "......activation_86\n",
      ".........vars\n",
      "......activation_87\n",
      ".........vars\n",
      "......activation_88\n",
      ".........vars\n",
      "......activation_89\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......activation_90\n",
      ".........vars\n",
      "......activation_91\n",
      ".........vars\n",
      "......activation_92\n",
      ".........vars\n",
      "......activation_93\n",
      ".........vars\n",
      "......activation_94\n",
      ".........vars\n",
      "......activation_95\n",
      ".........vars\n",
      "......activation_96\n",
      ".........vars\n",
      "......activation_97\n",
      ".........vars\n",
      "......activation_98\n",
      ".........vars\n",
      "......activation_99\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_109\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_110\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 10:23:43.603064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - ETA: 0s - loss: 2.2965 - accuracy: 0.4400\n",
      "Epoch 1: val_loss improved from inf to 2.57642, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 75s 140ms/step - loss: 2.2965 - accuracy: 0.4400 - val_loss: 2.5764 - val_accuracy: 0.4382 - lr: 0.1000\n",
      "Epoch 2/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7975 - accuracy: 0.6081\n",
      "Epoch 2: val_loss improved from 2.57642 to 2.54349, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 1.7975 - accuracy: 0.6081 - val_loss: 2.5435 - val_accuracy: 0.4324 - lr: 0.1000\n",
      "Epoch 3/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5331 - accuracy: 0.6792\n",
      "Epoch 3: val_loss improved from 2.54349 to 1.58750, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 1.5331 - accuracy: 0.6792 - val_loss: 1.5875 - val_accuracy: 0.6522 - lr: 0.1000\n",
      "Epoch 4/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3489 - accuracy: 0.7301\n",
      "Epoch 4: val_loss improved from 1.58750 to 1.36849, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 1.3489 - accuracy: 0.7301 - val_loss: 1.3685 - val_accuracy: 0.7194 - lr: 0.1000\n",
      "Epoch 5/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2171 - accuracy: 0.7592\n",
      "Epoch 5: val_loss improved from 1.36849 to 1.21046, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 51s 144ms/step - loss: 1.2171 - accuracy: 0.7592 - val_loss: 1.2105 - val_accuracy: 0.7600 - lr: 0.1000\n",
      "Epoch 6/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1170 - accuracy: 0.7799\n",
      "Epoch 6: val_loss improved from 1.21046 to 1.19581, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 51s 146ms/step - loss: 1.1170 - accuracy: 0.7799 - val_loss: 1.1958 - val_accuracy: 0.7544 - lr: 0.1000\n",
      "Epoch 7/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0333 - accuracy: 0.7967\n",
      "Epoch 7: val_loss did not improve from 1.19581\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 1.0333 - accuracy: 0.7967 - val_loss: 1.2312 - val_accuracy: 0.7320 - lr: 0.1000\n",
      "Epoch 8/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9702 - accuracy: 0.8093\n",
      "Epoch 8: val_loss improved from 1.19581 to 1.08831, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 51s 145ms/step - loss: 0.9702 - accuracy: 0.8093 - val_loss: 1.0883 - val_accuracy: 0.7662 - lr: 0.1000\n",
      "Epoch 9/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9128 - accuracy: 0.8177\n",
      "Epoch 9: val_loss did not improve from 1.08831\n",
      "351/351 [==============================] - 51s 144ms/step - loss: 0.9128 - accuracy: 0.8177 - val_loss: 1.1598 - val_accuracy: 0.7472 - lr: 0.1000\n",
      "Epoch 10/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8628 - accuracy: 0.8274\n",
      "Epoch 10: val_loss did not improve from 1.08831\n",
      "351/351 [==============================] - 52s 148ms/step - loss: 0.8628 - accuracy: 0.8274 - val_loss: 1.7758 - val_accuracy: 0.6520 - lr: 0.1000\n",
      "Epoch 11/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8203 - accuracy: 0.8345\n",
      "Epoch 11: val_loss improved from 1.08831 to 1.05898, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 58s 164ms/step - loss: 0.8203 - accuracy: 0.8345 - val_loss: 1.0590 - val_accuracy: 0.7648 - lr: 0.1000\n",
      "Epoch 12/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7857 - accuracy: 0.8415\n",
      "Epoch 12: val_loss did not improve from 1.05898\n",
      "351/351 [==============================] - 58s 164ms/step - loss: 0.7857 - accuracy: 0.8415 - val_loss: 1.4121 - val_accuracy: 0.6602 - lr: 0.1000\n",
      "Epoch 13/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7507 - accuracy: 0.8483\n",
      "Epoch 13: val_loss did not improve from 1.05898\n",
      "351/351 [==============================] - 63s 179ms/step - loss: 0.7507 - accuracy: 0.8483 - val_loss: 1.2202 - val_accuracy: 0.7188 - lr: 0.1000\n",
      "Epoch 14/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7263 - accuracy: 0.8522\n",
      "Epoch 14: val_loss improved from 1.05898 to 0.93839, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 63s 180ms/step - loss: 0.7263 - accuracy: 0.8522 - val_loss: 0.9384 - val_accuracy: 0.7850 - lr: 0.1000\n",
      "Epoch 15/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.8575\n",
      "Epoch 15: val_loss did not improve from 0.93839\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 0.7015 - accuracy: 0.8575 - val_loss: 1.0269 - val_accuracy: 0.7652 - lr: 0.1000\n",
      "Epoch 16/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.8602\n",
      "Epoch 16: val_loss improved from 0.93839 to 0.91662, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 0.6838 - accuracy: 0.8602 - val_loss: 0.9166 - val_accuracy: 0.7946 - lr: 0.1000\n",
      "Epoch 17/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6691 - accuracy: 0.8632\n",
      "Epoch 17: val_loss improved from 0.91662 to 0.91505, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 63s 180ms/step - loss: 0.6691 - accuracy: 0.8632 - val_loss: 0.9150 - val_accuracy: 0.7836 - lr: 0.1000\n",
      "Epoch 18/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.8694\n",
      "Epoch 18: val_loss did not improve from 0.91505\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 0.6435 - accuracy: 0.8694 - val_loss: 1.5161 - val_accuracy: 0.6892 - lr: 0.1000\n",
      "Epoch 19/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6291 - accuracy: 0.8729\n",
      "Epoch 19: val_loss improved from 0.91505 to 0.83978, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 0.6291 - accuracy: 0.8729 - val_loss: 0.8398 - val_accuracy: 0.8200 - lr: 0.1000\n",
      "Epoch 20/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6207 - accuracy: 0.8730\n",
      "Epoch 20: val_loss did not improve from 0.83978\n",
      "351/351 [==============================] - 63s 179ms/step - loss: 0.6207 - accuracy: 0.8730 - val_loss: 1.1383 - val_accuracy: 0.7452 - lr: 0.1000\n",
      "Epoch 21/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6071 - accuracy: 0.8773\n",
      "Epoch 21: val_loss did not improve from 0.83978\n",
      "351/351 [==============================] - 64s 181ms/step - loss: 0.6071 - accuracy: 0.8773 - val_loss: 0.9211 - val_accuracy: 0.7966 - lr: 0.1000\n",
      "Epoch 22/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5941 - accuracy: 0.8823\n",
      "Epoch 22: val_loss did not improve from 0.83978\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 0.5941 - accuracy: 0.8823 - val_loss: 0.8784 - val_accuracy: 0.7920 - lr: 0.1000\n",
      "Epoch 23/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5878 - accuracy: 0.8826\n",
      "Epoch 23: val_loss did not improve from 0.83978\n",
      "351/351 [==============================] - 57s 162ms/step - loss: 0.5878 - accuracy: 0.8826 - val_loss: 0.9085 - val_accuracy: 0.7910 - lr: 0.1000\n",
      "Epoch 24/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5846 - accuracy: 0.8834\n",
      "Epoch 24: val_loss did not improve from 0.83978\n",
      "351/351 [==============================] - 57s 161ms/step - loss: 0.5846 - accuracy: 0.8834 - val_loss: 1.1533 - val_accuracy: 0.7426 - lr: 0.1000\n",
      "Epoch 25/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5711 - accuracy: 0.8872\n",
      "Epoch 25: val_loss did not improve from 0.83978\n",
      "351/351 [==============================] - 59s 168ms/step - loss: 0.5711 - accuracy: 0.8872 - val_loss: 0.9861 - val_accuracy: 0.7972 - lr: 0.1000\n",
      "Epoch 26/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.8889\n",
      "Epoch 26: val_loss did not improve from 0.83978\n",
      "351/351 [==============================] - 62s 175ms/step - loss: 0.5656 - accuracy: 0.8889 - val_loss: 0.9140 - val_accuracy: 0.7950 - lr: 0.1000\n",
      "Epoch 27/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.8906\n",
      "Epoch 27: val_loss did not improve from 0.83978\n",
      "351/351 [==============================] - 63s 178ms/step - loss: 0.5634 - accuracy: 0.8906 - val_loss: 1.0894 - val_accuracy: 0.7436 - lr: 0.1000\n",
      "Epoch 28/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.8921\n",
      "Epoch 28: val_loss improved from 0.83978 to 0.82480, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 64s 181ms/step - loss: 0.5582 - accuracy: 0.8921 - val_loss: 0.8248 - val_accuracy: 0.8166 - lr: 0.1000\n",
      "Epoch 29/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.8934\n",
      "Epoch 29: val_loss improved from 0.82480 to 0.69452, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 64s 181ms/step - loss: 0.5525 - accuracy: 0.8934 - val_loss: 0.6945 - val_accuracy: 0.8442 - lr: 0.1000\n",
      "Epoch 30/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.8958\n",
      "Epoch 30: val_loss did not improve from 0.69452\n",
      "351/351 [==============================] - 57s 163ms/step - loss: 0.5436 - accuracy: 0.8958 - val_loss: 0.7355 - val_accuracy: 0.8424 - lr: 0.1000\n",
      "Epoch 31/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.8965\n",
      "Epoch 31: val_loss did not improve from 0.69452\n",
      "351/351 [==============================] - 56s 160ms/step - loss: 0.5447 - accuracy: 0.8965 - val_loss: 0.7654 - val_accuracy: 0.8332 - lr: 0.1000\n",
      "Epoch 32/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5396 - accuracy: 0.8987\n",
      "Epoch 32: val_loss did not improve from 0.69452\n",
      "351/351 [==============================] - 62s 175ms/step - loss: 0.5396 - accuracy: 0.8987 - val_loss: 0.7539 - val_accuracy: 0.8388 - lr: 0.1000\n",
      "Epoch 33/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.8995\n",
      "Epoch 33: val_loss did not improve from 0.69452\n",
      "351/351 [==============================] - 62s 177ms/step - loss: 0.5351 - accuracy: 0.8995 - val_loss: 1.1698 - val_accuracy: 0.7490 - lr: 0.1000\n",
      "Epoch 34/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5353 - accuracy: 0.8994\n",
      "Epoch 34: val_loss improved from 0.69452 to 0.66335, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 58s 166ms/step - loss: 0.5353 - accuracy: 0.8994 - val_loss: 0.6634 - val_accuracy: 0.8602 - lr: 0.1000\n",
      "Epoch 35/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.9025\n",
      "Epoch 35: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 0.5299 - accuracy: 0.9025 - val_loss: 0.6773 - val_accuracy: 0.8546 - lr: 0.1000\n",
      "Epoch 36/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5264 - accuracy: 0.9037\n",
      "Epoch 36: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 53s 150ms/step - loss: 0.5264 - accuracy: 0.9037 - val_loss: 0.8401 - val_accuracy: 0.8128 - lr: 0.1000\n",
      "Epoch 37/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5199 - accuracy: 0.9068\n",
      "Epoch 37: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 62s 177ms/step - loss: 0.5199 - accuracy: 0.9068 - val_loss: 1.0772 - val_accuracy: 0.7576 - lr: 0.1000\n",
      "Epoch 38/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.9034\n",
      "Epoch 38: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 63s 180ms/step - loss: 0.5251 - accuracy: 0.9034 - val_loss: 0.8518 - val_accuracy: 0.8196 - lr: 0.1000\n",
      "Epoch 39/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.9081\n",
      "Epoch 39: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 61s 175ms/step - loss: 0.5172 - accuracy: 0.9081 - val_loss: 1.4194 - val_accuracy: 0.7402 - lr: 0.1000\n",
      "Epoch 40/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5197 - accuracy: 0.9057\n",
      "Epoch 40: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 58s 166ms/step - loss: 0.5197 - accuracy: 0.9057 - val_loss: 0.8548 - val_accuracy: 0.8294 - lr: 0.1000\n",
      "Epoch 41/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5106 - accuracy: 0.9094\n",
      "Epoch 41: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 0.5106 - accuracy: 0.9094 - val_loss: 0.8549 - val_accuracy: 0.8158 - lr: 0.1000\n",
      "Epoch 42/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5128 - accuracy: 0.9103\n",
      "Epoch 42: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 64s 182ms/step - loss: 0.5128 - accuracy: 0.9103 - val_loss: 1.0485 - val_accuracy: 0.7790 - lr: 0.1000\n",
      "Epoch 43/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.9124\n",
      "Epoch 43: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 56s 160ms/step - loss: 0.5078 - accuracy: 0.9124 - val_loss: 0.8084 - val_accuracy: 0.8368 - lr: 0.1000\n",
      "Epoch 44/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5160 - accuracy: 0.9104\n",
      "Epoch 44: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 0.5160 - accuracy: 0.9104 - val_loss: 1.1109 - val_accuracy: 0.7476 - lr: 0.1000\n",
      "Epoch 45/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.9145\n",
      "Epoch 45: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 56s 159ms/step - loss: 0.5031 - accuracy: 0.9145 - val_loss: 0.7267 - val_accuracy: 0.8502 - lr: 0.1000\n",
      "Epoch 46/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5088 - accuracy: 0.9130\n",
      "Epoch 46: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 57s 162ms/step - loss: 0.5088 - accuracy: 0.9130 - val_loss: 0.8415 - val_accuracy: 0.8242 - lr: 0.1000\n",
      "Epoch 47/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.9147\n",
      "Epoch 47: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 0.5043 - accuracy: 0.9147 - val_loss: 1.1816 - val_accuracy: 0.7798 - lr: 0.1000\n",
      "Epoch 48/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5021 - accuracy: 0.9142\n",
      "Epoch 48: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 59s 169ms/step - loss: 0.5021 - accuracy: 0.9142 - val_loss: 0.7629 - val_accuracy: 0.8402 - lr: 0.1000\n",
      "Epoch 49/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.9152\n",
      "Epoch 49: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 63s 180ms/step - loss: 0.5040 - accuracy: 0.9152 - val_loss: 0.8639 - val_accuracy: 0.8166 - lr: 0.1000\n",
      "Epoch 50/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.9151\n",
      "Epoch 50: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 0.5025 - accuracy: 0.9151 - val_loss: 0.7442 - val_accuracy: 0.8420 - lr: 0.1000\n",
      "Epoch 51/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.9163\n",
      "Epoch 51: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 57s 163ms/step - loss: 0.5022 - accuracy: 0.9163 - val_loss: 0.7686 - val_accuracy: 0.8460 - lr: 0.1000\n",
      "Epoch 52/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5024 - accuracy: 0.9155\n",
      "Epoch 52: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 0.5024 - accuracy: 0.9155 - val_loss: 1.0991 - val_accuracy: 0.7574 - lr: 0.1000\n",
      "Epoch 53/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4912 - accuracy: 0.9206\n",
      "Epoch 53: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 0.4912 - accuracy: 0.9206 - val_loss: 0.6853 - val_accuracy: 0.8606 - lr: 0.1000\n",
      "Epoch 54/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4978 - accuracy: 0.9169\n",
      "Epoch 54: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 62s 177ms/step - loss: 0.4978 - accuracy: 0.9169 - val_loss: 0.7882 - val_accuracy: 0.8372 - lr: 0.1000\n",
      "Epoch 55/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4938 - accuracy: 0.9186\n",
      "Epoch 55: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 64s 181ms/step - loss: 0.4938 - accuracy: 0.9186 - val_loss: 0.7269 - val_accuracy: 0.8636 - lr: 0.1000\n",
      "Epoch 56/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4908 - accuracy: 0.9212\n",
      "Epoch 56: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 63s 179ms/step - loss: 0.4908 - accuracy: 0.9212 - val_loss: 1.0655 - val_accuracy: 0.7838 - lr: 0.1000\n",
      "Epoch 57/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4955 - accuracy: 0.9189\n",
      "Epoch 57: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 0.4955 - accuracy: 0.9189 - val_loss: 0.8469 - val_accuracy: 0.8238 - lr: 0.1000\n",
      "Epoch 58/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4949 - accuracy: 0.9196\n",
      "Epoch 58: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 56s 159ms/step - loss: 0.4949 - accuracy: 0.9196 - val_loss: 0.8125 - val_accuracy: 0.8308 - lr: 0.1000\n",
      "Epoch 59/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.9205\n",
      "Epoch 59: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 52s 149ms/step - loss: 0.4921 - accuracy: 0.9205 - val_loss: 0.8658 - val_accuracy: 0.8250 - lr: 0.1000\n",
      "Epoch 60/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4935 - accuracy: 0.9217\n",
      "Epoch 60: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 0.4935 - accuracy: 0.9217 - val_loss: 0.7697 - val_accuracy: 0.8446 - lr: 0.1000\n",
      "Epoch 61/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4951 - accuracy: 0.9210\n",
      "Epoch 61: val_loss did not improve from 0.66335\n",
      "351/351 [==============================] - 66s 187ms/step - loss: 0.4951 - accuracy: 0.9210 - val_loss: 0.8147 - val_accuracy: 0.8310 - lr: 0.1000\n",
      "Epoch 62/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4898 - accuracy: 0.9242\n",
      "Epoch 62: val_loss improved from 0.66335 to 0.66036, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 63s 178ms/step - loss: 0.4898 - accuracy: 0.9242 - val_loss: 0.6604 - val_accuracy: 0.8674 - lr: 0.1000\n",
      "Epoch 63/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4888 - accuracy: 0.9234\n",
      "Epoch 63: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 56s 160ms/step - loss: 0.4888 - accuracy: 0.9234 - val_loss: 0.7527 - val_accuracy: 0.8558 - lr: 0.1000\n",
      "Epoch 64/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4865 - accuracy: 0.9237\n",
      "Epoch 64: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 0.4865 - accuracy: 0.9237 - val_loss: 0.8301 - val_accuracy: 0.8294 - lr: 0.1000\n",
      "Epoch 65/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.9231\n",
      "Epoch 65: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 62s 175ms/step - loss: 0.4883 - accuracy: 0.9231 - val_loss: 0.9799 - val_accuracy: 0.8034 - lr: 0.1000\n",
      "Epoch 66/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4906 - accuracy: 0.9236\n",
      "Epoch 66: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 60s 172ms/step - loss: 0.4906 - accuracy: 0.9236 - val_loss: 0.8566 - val_accuracy: 0.8232 - lr: 0.1000\n",
      "Epoch 67/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.9249\n",
      "Epoch 67: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 0.4847 - accuracy: 0.9249 - val_loss: 0.9416 - val_accuracy: 0.8152 - lr: 0.1000\n",
      "Epoch 68/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.9241\n",
      "Epoch 68: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 0.4850 - accuracy: 0.9241 - val_loss: 0.7254 - val_accuracy: 0.8518 - lr: 0.1000\n",
      "Epoch 69/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4868 - accuracy: 0.9246\n",
      "Epoch 69: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 53s 151ms/step - loss: 0.4868 - accuracy: 0.9246 - val_loss: 0.9470 - val_accuracy: 0.8084 - lr: 0.1000\n",
      "Epoch 70/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.9253\n",
      "Epoch 70: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 52s 149ms/step - loss: 0.4856 - accuracy: 0.9253 - val_loss: 0.9123 - val_accuracy: 0.8186 - lr: 0.1000\n",
      "Epoch 71/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4903 - accuracy: 0.9243\n",
      "Epoch 71: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 0.4903 - accuracy: 0.9243 - val_loss: 0.7735 - val_accuracy: 0.8454 - lr: 0.1000\n",
      "Epoch 72/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4875 - accuracy: 0.9250\n",
      "Epoch 72: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 0.4875 - accuracy: 0.9250 - val_loss: 0.8849 - val_accuracy: 0.8310 - lr: 0.1000\n",
      "Epoch 73/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4865 - accuracy: 0.9267\n",
      "Epoch 73: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 62s 175ms/step - loss: 0.4865 - accuracy: 0.9267 - val_loss: 0.7683 - val_accuracy: 0.8486 - lr: 0.1000\n",
      "Epoch 74/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.9266\n",
      "Epoch 74: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 0.4846 - accuracy: 0.9266 - val_loss: 0.7377 - val_accuracy: 0.8576 - lr: 0.1000\n",
      "Epoch 75/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.9273\n",
      "Epoch 75: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 0.4846 - accuracy: 0.9273 - val_loss: 0.6807 - val_accuracy: 0.8708 - lr: 0.1000\n",
      "Epoch 76/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4835 - accuracy: 0.9266\n",
      "Epoch 76: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 0.4835 - accuracy: 0.9266 - val_loss: 0.6877 - val_accuracy: 0.8712 - lr: 0.1000\n",
      "Epoch 77/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.9269\n",
      "Epoch 77: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 56s 159ms/step - loss: 0.4859 - accuracy: 0.9269 - val_loss: 0.9031 - val_accuracy: 0.8034 - lr: 0.1000\n",
      "Epoch 78/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.9265\n",
      "Epoch 78: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 0.4866 - accuracy: 0.9265 - val_loss: 0.7853 - val_accuracy: 0.8502 - lr: 0.1000\n",
      "Epoch 79/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4829 - accuracy: 0.9272\n",
      "Epoch 79: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 63s 180ms/step - loss: 0.4829 - accuracy: 0.9272 - val_loss: 0.7425 - val_accuracy: 0.8528 - lr: 0.1000\n",
      "Epoch 80/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4819 - accuracy: 0.9271\n",
      "Epoch 80: val_loss did not improve from 0.66036\n",
      "351/351 [==============================] - 64s 181ms/step - loss: 0.4819 - accuracy: 0.9271 - val_loss: 0.7045 - val_accuracy: 0.8672 - lr: 0.1000\n",
      "Epoch 81/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.9571\n",
      "Epoch 81: val_loss improved from 0.66036 to 0.49132, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 65s 184ms/step - loss: 0.4045 - accuracy: 0.9571 - val_loss: 0.4913 - val_accuracy: 0.9284 - lr: 0.0100\n",
      "Epoch 82/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3542 - accuracy: 0.9735\n",
      "Epoch 82: val_loss improved from 0.49132 to 0.48232, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 62s 176ms/step - loss: 0.3542 - accuracy: 0.9735 - val_loss: 0.4823 - val_accuracy: 0.9336 - lr: 0.0100\n",
      "Epoch 83/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.9781\n",
      "Epoch 83: val_loss improved from 0.48232 to 0.47761, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 62s 175ms/step - loss: 0.3360 - accuracy: 0.9781 - val_loss: 0.4776 - val_accuracy: 0.9346 - lr: 0.0100\n",
      "Epoch 84/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3254 - accuracy: 0.9808\n",
      "Epoch 84: val_loss did not improve from 0.47761\n",
      "351/351 [==============================] - 64s 182ms/step - loss: 0.3254 - accuracy: 0.9808 - val_loss: 0.4780 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 85/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.9838\n",
      "Epoch 85: val_loss did not improve from 0.47761\n",
      "351/351 [==============================] - 62s 176ms/step - loss: 0.3138 - accuracy: 0.9838 - val_loss: 0.4804 - val_accuracy: 0.9356 - lr: 0.0100\n",
      "Epoch 86/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.9858\n",
      "Epoch 86: val_loss improved from 0.47761 to 0.47041, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 65s 184ms/step - loss: 0.3040 - accuracy: 0.9858 - val_loss: 0.4704 - val_accuracy: 0.9386 - lr: 0.0100\n",
      "Epoch 87/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.9865\n",
      "Epoch 87: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 0.2968 - accuracy: 0.9865 - val_loss: 0.4778 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 88/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9882\n",
      "Epoch 88: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 0.2903 - accuracy: 0.9882 - val_loss: 0.4826 - val_accuracy: 0.9376 - lr: 0.0100\n",
      "Epoch 89/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9897\n",
      "Epoch 89: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 58s 166ms/step - loss: 0.2834 - accuracy: 0.9897 - val_loss: 0.4741 - val_accuracy: 0.9382 - lr: 0.0100\n",
      "Epoch 90/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.9903\n",
      "Epoch 90: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 58s 165ms/step - loss: 0.2773 - accuracy: 0.9903 - val_loss: 0.4739 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 91/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9908\n",
      "Epoch 91: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 0.2730 - accuracy: 0.9908 - val_loss: 0.4785 - val_accuracy: 0.9378 - lr: 0.0100\n",
      "Epoch 92/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9918\n",
      "Epoch 92: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 0.2680 - accuracy: 0.9918 - val_loss: 0.4726 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 93/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.9913\n",
      "Epoch 93: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 58s 165ms/step - loss: 0.2637 - accuracy: 0.9913 - val_loss: 0.4856 - val_accuracy: 0.9354 - lr: 0.0100\n",
      "Epoch 94/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9928\n",
      "Epoch 94: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 0.2579 - accuracy: 0.9928 - val_loss: 0.4782 - val_accuracy: 0.9376 - lr: 0.0100\n",
      "Epoch 95/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.9936\n",
      "Epoch 95: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 63s 181ms/step - loss: 0.2527 - accuracy: 0.9936 - val_loss: 0.4773 - val_accuracy: 0.9372 - lr: 0.0100\n",
      "Epoch 96/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.9932\n",
      "Epoch 96: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 0.2503 - accuracy: 0.9932 - val_loss: 0.4813 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 97/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.9933\n",
      "Epoch 97: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 0.2460 - accuracy: 0.9933 - val_loss: 0.4751 - val_accuracy: 0.9378 - lr: 0.0100\n",
      "Epoch 98/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9940\n",
      "Epoch 98: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 61s 175ms/step - loss: 0.2421 - accuracy: 0.9940 - val_loss: 0.4715 - val_accuracy: 0.9384 - lr: 0.0100\n",
      "Epoch 99/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.9943\n",
      "Epoch 99: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 63s 179ms/step - loss: 0.2387 - accuracy: 0.9943 - val_loss: 0.4725 - val_accuracy: 0.9380 - lr: 0.0100\n",
      "Epoch 100/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.9945\n",
      "Epoch 100: val_loss did not improve from 0.47041\n",
      "351/351 [==============================] - 63s 178ms/step - loss: 0.2354 - accuracy: 0.9945 - val_loss: 0.4704 - val_accuracy: 0.9376 - lr: 0.0100\n",
      "Epoch 101/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.9943\n",
      "Epoch 101: val_loss improved from 0.47041 to 0.46791, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 58s 164ms/step - loss: 0.2328 - accuracy: 0.9943 - val_loss: 0.4679 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 102/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9955\n",
      "Epoch 102: val_loss did not improve from 0.46791\n",
      "351/351 [==============================] - 53s 150ms/step - loss: 0.2269 - accuracy: 0.9955 - val_loss: 0.4778 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 103/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9947\n",
      "Epoch 103: val_loss did not improve from 0.46791\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 0.2259 - accuracy: 0.9947 - val_loss: 0.4923 - val_accuracy: 0.9344 - lr: 0.0100\n",
      "Epoch 104/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9951\n",
      "Epoch 104: val_loss did not improve from 0.46791\n",
      "351/351 [==============================] - 53s 151ms/step - loss: 0.2227 - accuracy: 0.9951 - val_loss: 0.4932 - val_accuracy: 0.9322 - lr: 0.0100\n",
      "Epoch 105/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2208 - accuracy: 0.9946\n",
      "Epoch 105: val_loss did not improve from 0.46791\n",
      "351/351 [==============================] - 52s 147ms/step - loss: 0.2208 - accuracy: 0.9946 - val_loss: 0.4747 - val_accuracy: 0.9380 - lr: 0.0100\n",
      "Epoch 106/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.9953\n",
      "Epoch 106: val_loss improved from 0.46791 to 0.46787, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 0.2164 - accuracy: 0.9953 - val_loss: 0.4679 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 107/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2139 - accuracy: 0.9953\n",
      "Epoch 107: val_loss did not improve from 0.46787\n",
      "351/351 [==============================] - 61s 174ms/step - loss: 0.2139 - accuracy: 0.9953 - val_loss: 0.4731 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 108/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9952\n",
      "Epoch 108: val_loss improved from 0.46787 to 0.46459, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 64s 183ms/step - loss: 0.2106 - accuracy: 0.9952 - val_loss: 0.4646 - val_accuracy: 0.9368 - lr: 0.0100\n",
      "Epoch 109/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9954\n",
      "Epoch 109: val_loss improved from 0.46459 to 0.45832, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 64s 182ms/step - loss: 0.2076 - accuracy: 0.9954 - val_loss: 0.4583 - val_accuracy: 0.9402 - lr: 0.0100\n",
      "Epoch 110/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.9963\n",
      "Epoch 110: val_loss did not improve from 0.45832\n",
      "351/351 [==============================] - 57s 162ms/step - loss: 0.2036 - accuracy: 0.9963 - val_loss: 0.4710 - val_accuracy: 0.9372 - lr: 0.0100\n",
      "Epoch 111/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.9961\n",
      "Epoch 111: val_loss improved from 0.45832 to 0.45155, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 58s 165ms/step - loss: 0.2019 - accuracy: 0.9961 - val_loss: 0.4515 - val_accuracy: 0.9372 - lr: 0.0100\n",
      "Epoch 112/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9965\n",
      "Epoch 112: val_loss did not improve from 0.45155\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 0.1986 - accuracy: 0.9965 - val_loss: 0.4527 - val_accuracy: 0.9384 - lr: 0.0100\n",
      "Epoch 113/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9967\n",
      "Epoch 113: val_loss did not improve from 0.45155\n",
      "351/351 [==============================] - 59s 169ms/step - loss: 0.1958 - accuracy: 0.9967 - val_loss: 0.4627 - val_accuracy: 0.9382 - lr: 0.0100\n",
      "Epoch 114/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9968\n",
      "Epoch 114: val_loss did not improve from 0.45155\n",
      "351/351 [==============================] - 60s 172ms/step - loss: 0.1932 - accuracy: 0.9968 - val_loss: 0.4624 - val_accuracy: 0.9334 - lr: 0.0100\n",
      "Epoch 115/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1921 - accuracy: 0.9963\n",
      "Epoch 115: val_loss did not improve from 0.45155\n",
      "351/351 [==============================] - 63s 179ms/step - loss: 0.1921 - accuracy: 0.9963 - val_loss: 0.4736 - val_accuracy: 0.9344 - lr: 0.0100\n",
      "Epoch 116/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.9961\n",
      "Epoch 116: val_loss did not improve from 0.45155\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 0.1911 - accuracy: 0.9961 - val_loss: 0.4583 - val_accuracy: 0.9386 - lr: 0.0100\n",
      "Epoch 117/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9965\n",
      "Epoch 117: val_loss did not improve from 0.45155\n",
      "351/351 [==============================] - 61s 173ms/step - loss: 0.1874 - accuracy: 0.9965 - val_loss: 0.4745 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 118/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9964\n",
      "Epoch 118: val_loss improved from 0.45155 to 0.45080, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 57s 161ms/step - loss: 0.1853 - accuracy: 0.9964 - val_loss: 0.4508 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 119/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1841 - accuracy: 0.9963\n",
      "Epoch 119: val_loss did not improve from 0.45080\n",
      "351/351 [==============================] - 53s 150ms/step - loss: 0.1841 - accuracy: 0.9963 - val_loss: 0.4673 - val_accuracy: 0.9322 - lr: 0.0100\n",
      "Epoch 120/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 0.9965\n",
      "Epoch 120: val_loss did not improve from 0.45080\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 0.1803 - accuracy: 0.9965 - val_loss: 0.4546 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 1/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.9973\n",
      "Epoch 1: val_loss improved from 0.45080 to 0.44960, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 52s 121ms/step - loss: 0.1776 - accuracy: 0.9973 - val_loss: 0.4496 - val_accuracy: 0.9390 - lr: 0.0100\n",
      "Epoch 2/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.9977\n",
      "Epoch 2: val_loss improved from 0.44960 to 0.44606, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.1758 - accuracy: 0.9977 - val_loss: 0.4461 - val_accuracy: 0.9380 - lr: 0.0100\n",
      "Epoch 3/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9983\n",
      "Epoch 3: val_loss improved from 0.44606 to 0.44057, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.1744 - accuracy: 0.9983 - val_loss: 0.4406 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 4/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9978\n",
      "Epoch 4: val_loss did not improve from 0.44057\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1746 - accuracy: 0.9978 - val_loss: 0.4443 - val_accuracy: 0.9390 - lr: 0.0100\n",
      "Epoch 5/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9982\n",
      "Epoch 5: val_loss did not improve from 0.44057\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.1741 - accuracy: 0.9982 - val_loss: 0.4434 - val_accuracy: 0.9404 - lr: 0.0100\n",
      "Epoch 6/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.9985\n",
      "Epoch 6: val_loss did not improve from 0.44057\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1725 - accuracy: 0.9985 - val_loss: 0.4513 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 7/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.9981\n",
      "Epoch 7: val_loss did not improve from 0.44057\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.1725 - accuracy: 0.9981 - val_loss: 0.4471 - val_accuracy: 0.9408 - lr: 0.0100\n",
      "Epoch 8/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9984\n",
      "Epoch 8: val_loss improved from 0.44057 to 0.44024, saving model to model_resnet_c10_best.hdf5\n",
      "351/351 [==============================] - 40s 113ms/step - loss: 0.1717 - accuracy: 0.9984 - val_loss: 0.4402 - val_accuracy: 0.9402 - lr: 0.0100\n",
      "Epoch 9/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9983\n",
      "Epoch 9: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1717 - accuracy: 0.9983 - val_loss: 0.4520 - val_accuracy: 0.9390 - lr: 0.0100\n",
      "Epoch 10/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9986\n",
      "Epoch 10: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.1705 - accuracy: 0.9986 - val_loss: 0.4435 - val_accuracy: 0.9408 - lr: 0.0100\n",
      "Epoch 11/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9985\n",
      "Epoch 11: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.1703 - accuracy: 0.9985 - val_loss: 0.4446 - val_accuracy: 0.9412 - lr: 0.0100\n",
      "Epoch 12/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9984\n",
      "Epoch 12: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 113ms/step - loss: 0.1701 - accuracy: 0.9984 - val_loss: 0.4529 - val_accuracy: 0.9394 - lr: 0.0100\n",
      "Epoch 13/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1693 - accuracy: 0.9985\n",
      "Epoch 13: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.1693 - accuracy: 0.9985 - val_loss: 0.4510 - val_accuracy: 0.9398 - lr: 0.0100\n",
      "Epoch 14/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1695 - accuracy: 0.9986\n",
      "Epoch 14: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.1695 - accuracy: 0.9986 - val_loss: 0.4554 - val_accuracy: 0.9380 - lr: 0.0100\n",
      "Epoch 15/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.9988\n",
      "Epoch 15: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.1685 - accuracy: 0.9988 - val_loss: 0.4550 - val_accuracy: 0.9376 - lr: 0.0100\n",
      "Epoch 16/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.9984\n",
      "Epoch 16: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.1690 - accuracy: 0.9984 - val_loss: 0.4567 - val_accuracy: 0.9372 - lr: 0.0100\n",
      "Epoch 17/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9983\n",
      "Epoch 17: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.1682 - accuracy: 0.9983 - val_loss: 0.4578 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 18/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9987\n",
      "Epoch 18: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 113ms/step - loss: 0.1677 - accuracy: 0.9987 - val_loss: 0.4524 - val_accuracy: 0.9382 - lr: 0.0100\n",
      "Epoch 19/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9986\n",
      "Epoch 19: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.1678 - accuracy: 0.9986 - val_loss: 0.4489 - val_accuracy: 0.9384 - lr: 0.0100\n",
      "Epoch 20/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9989\n",
      "Epoch 20: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.1665 - accuracy: 0.9989 - val_loss: 0.4454 - val_accuracy: 0.9390 - lr: 0.0100\n",
      "Epoch 21/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.9985\n",
      "Epoch 21: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.1668 - accuracy: 0.9985 - val_loss: 0.4585 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 22/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9989\n",
      "Epoch 22: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1658 - accuracy: 0.9989 - val_loss: 0.4504 - val_accuracy: 0.9378 - lr: 0.0100\n",
      "Epoch 23/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.9988\n",
      "Epoch 23: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1664 - accuracy: 0.9988 - val_loss: 0.4508 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 24/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9988\n",
      "Epoch 24: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 107ms/step - loss: 0.1660 - accuracy: 0.9988 - val_loss: 0.4517 - val_accuracy: 0.9412 - lr: 0.0100\n",
      "Epoch 25/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9986\n",
      "Epoch 25: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.1660 - accuracy: 0.9986 - val_loss: 0.4472 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 26/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1653 - accuracy: 0.9988\n",
      "Epoch 26: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1653 - accuracy: 0.9988 - val_loss: 0.4592 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 27/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1648 - accuracy: 0.9988\n",
      "Epoch 27: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.1648 - accuracy: 0.9988 - val_loss: 0.4546 - val_accuracy: 0.9386 - lr: 0.0100\n",
      "Epoch 28/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9990\n",
      "Epoch 28: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.1642 - accuracy: 0.9990 - val_loss: 0.4544 - val_accuracy: 0.9386 - lr: 0.0100\n",
      "Epoch 29/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9988\n",
      "Epoch 29: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1644 - accuracy: 0.9988 - val_loss: 0.4586 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 30/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.9983\n",
      "Epoch 30: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1649 - accuracy: 0.9983 - val_loss: 0.4585 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 31/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9987\n",
      "Epoch 31: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.1641 - accuracy: 0.9987 - val_loss: 0.4467 - val_accuracy: 0.9412 - lr: 0.0010\n",
      "Epoch 32/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9989\n",
      "Epoch 32: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.1632 - accuracy: 0.9989 - val_loss: 0.4454 - val_accuracy: 0.9408 - lr: 0.0010\n",
      "Epoch 33/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9990\n",
      "Epoch 33: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1632 - accuracy: 0.9990 - val_loss: 0.4451 - val_accuracy: 0.9392 - lr: 0.0010\n",
      "Epoch 34/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1635 - accuracy: 0.9990\n",
      "Epoch 34: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.1635 - accuracy: 0.9990 - val_loss: 0.4450 - val_accuracy: 0.9398 - lr: 0.0010\n",
      "Epoch 35/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9992\n",
      "Epoch 35: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.1627 - accuracy: 0.9992 - val_loss: 0.4454 - val_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 36/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9990\n",
      "Epoch 36: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1633 - accuracy: 0.9990 - val_loss: 0.4450 - val_accuracy: 0.9398 - lr: 0.0010\n",
      "Epoch 37/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9990\n",
      "Epoch 37: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1631 - accuracy: 0.9990 - val_loss: 0.4448 - val_accuracy: 0.9404 - lr: 0.0010\n",
      "Epoch 38/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9991\n",
      "Epoch 38: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1628 - accuracy: 0.9991 - val_loss: 0.4445 - val_accuracy: 0.9414 - lr: 0.0010\n",
      "Epoch 39/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9992\n",
      "Epoch 39: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1627 - accuracy: 0.9992 - val_loss: 0.4431 - val_accuracy: 0.9416 - lr: 0.0010\n",
      "Epoch 40/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9987\n",
      "Epoch 40: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1633 - accuracy: 0.9987 - val_loss: 0.4449 - val_accuracy: 0.9412 - lr: 0.0010\n",
      "Epoch 41/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9992\n",
      "Epoch 41: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.1624 - accuracy: 0.9992 - val_loss: 0.4450 - val_accuracy: 0.9396 - lr: 0.0010\n",
      "Epoch 42/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9991\n",
      "Epoch 42: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.1630 - accuracy: 0.9991 - val_loss: 0.4451 - val_accuracy: 0.9398 - lr: 0.0010\n",
      "Epoch 43/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9992\n",
      "Epoch 43: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.1627 - accuracy: 0.9992 - val_loss: 0.4443 - val_accuracy: 0.9408 - lr: 0.0010\n",
      "Epoch 44/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9994\n",
      "Epoch 44: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1627 - accuracy: 0.9994 - val_loss: 0.4449 - val_accuracy: 0.9410 - lr: 0.0010\n",
      "Epoch 45/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9990\n",
      "Epoch 45: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1629 - accuracy: 0.9990 - val_loss: 0.4458 - val_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 46/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9992\n",
      "Epoch 46: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1623 - accuracy: 0.9992 - val_loss: 0.4454 - val_accuracy: 0.9406 - lr: 0.0010\n",
      "Epoch 47/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9990\n",
      "Epoch 47: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.1627 - accuracy: 0.9990 - val_loss: 0.4454 - val_accuracy: 0.9410 - lr: 0.0010\n",
      "Epoch 48/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9993\n",
      "Epoch 48: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1621 - accuracy: 0.9993 - val_loss: 0.4474 - val_accuracy: 0.9398 - lr: 0.0010\n",
      "Epoch 49/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9992\n",
      "Epoch 49: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.1624 - accuracy: 0.9992 - val_loss: 0.4461 - val_accuracy: 0.9408 - lr: 0.0010\n",
      "Epoch 50/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9990\n",
      "Epoch 50: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1631 - accuracy: 0.9990 - val_loss: 0.4467 - val_accuracy: 0.9396 - lr: 0.0010\n",
      "Epoch 51/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.9991\n",
      "Epoch 51: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.1626 - accuracy: 0.9991 - val_loss: 0.4466 - val_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 52/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9988\n",
      "Epoch 52: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1629 - accuracy: 0.9988 - val_loss: 0.4465 - val_accuracy: 0.9416 - lr: 0.0010\n",
      "Epoch 53/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9992\n",
      "Epoch 53: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1623 - accuracy: 0.9992 - val_loss: 0.4482 - val_accuracy: 0.9404 - lr: 0.0010\n",
      "Epoch 54/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9993\n",
      "Epoch 54: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1621 - accuracy: 0.9993 - val_loss: 0.4482 - val_accuracy: 0.9406 - lr: 0.0010\n",
      "Epoch 55/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9992\n",
      "Epoch 55: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1621 - accuracy: 0.9992 - val_loss: 0.4482 - val_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 56/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9991\n",
      "Epoch 56: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1622 - accuracy: 0.9991 - val_loss: 0.4483 - val_accuracy: 0.9404 - lr: 0.0010\n",
      "Epoch 57/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1617 - accuracy: 0.9994\n",
      "Epoch 57: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1617 - accuracy: 0.9994 - val_loss: 0.4483 - val_accuracy: 0.9402 - lr: 0.0010\n",
      "Epoch 58/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9991\n",
      "Epoch 58: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.1621 - accuracy: 0.9991 - val_loss: 0.4481 - val_accuracy: 0.9402 - lr: 0.0010\n",
      "Epoch 59/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9991\n",
      "Epoch 59: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.1622 - accuracy: 0.9991 - val_loss: 0.4488 - val_accuracy: 0.9390 - lr: 0.0010\n",
      "Epoch 60/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9992\n",
      "Epoch 60: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1621 - accuracy: 0.9992 - val_loss: 0.4487 - val_accuracy: 0.9388 - lr: 0.0010\n",
      "Epoch 61/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9993\n",
      "Epoch 61: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 114ms/step - loss: 0.1618 - accuracy: 0.9993 - val_loss: 0.4495 - val_accuracy: 0.9398 - lr: 0.0010\n",
      "Epoch 62/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9993\n",
      "Epoch 62: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1620 - accuracy: 0.9993 - val_loss: 0.4483 - val_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 63/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9990\n",
      "Epoch 63: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1621 - accuracy: 0.9990 - val_loss: 0.4507 - val_accuracy: 0.9392 - lr: 0.0010\n",
      "Epoch 64/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9988\n",
      "Epoch 64: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.1624 - accuracy: 0.9988 - val_loss: 0.4494 - val_accuracy: 0.9396 - lr: 0.0010\n",
      "Epoch 65/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9992\n",
      "Epoch 65: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.1616 - accuracy: 0.9992 - val_loss: 0.4498 - val_accuracy: 0.9394 - lr: 0.0010\n",
      "Epoch 66/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9992\n",
      "Epoch 66: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.1616 - accuracy: 0.9992 - val_loss: 0.4492 - val_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 67/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9992\n",
      "Epoch 67: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.1620 - accuracy: 0.9992 - val_loss: 0.4499 - val_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 68/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9992\n",
      "Epoch 68: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.1618 - accuracy: 0.9992 - val_loss: 0.4498 - val_accuracy: 0.9396 - lr: 0.0010\n",
      "Epoch 69/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9991\n",
      "Epoch 69: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1620 - accuracy: 0.9991 - val_loss: 0.4506 - val_accuracy: 0.9384 - lr: 0.0010\n",
      "Epoch 70/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9991\n",
      "Epoch 70: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1615 - accuracy: 0.9991 - val_loss: 0.4493 - val_accuracy: 0.9386 - lr: 0.0010\n",
      "Epoch 71/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9992\n",
      "Epoch 71: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1616 - accuracy: 0.9992 - val_loss: 0.4501 - val_accuracy: 0.9390 - lr: 0.0010\n",
      "Epoch 72/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9990\n",
      "Epoch 72: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.1618 - accuracy: 0.9990 - val_loss: 0.4485 - val_accuracy: 0.9390 - lr: 0.0010\n",
      "Epoch 73/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9993\n",
      "Epoch 73: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.1613 - accuracy: 0.9993 - val_loss: 0.4487 - val_accuracy: 0.9398 - lr: 0.0010\n",
      "Epoch 74/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9991\n",
      "Epoch 74: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.1615 - accuracy: 0.9991 - val_loss: 0.4491 - val_accuracy: 0.9390 - lr: 0.0010\n",
      "Epoch 75/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9991\n",
      "Epoch 75: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.1616 - accuracy: 0.9991 - val_loss: 0.4492 - val_accuracy: 0.9386 - lr: 0.0010\n",
      "Epoch 76/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9991\n",
      "Epoch 76: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.1618 - accuracy: 0.9991 - val_loss: 0.4501 - val_accuracy: 0.9396 - lr: 0.0010\n",
      "Epoch 77/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1617 - accuracy: 0.9991\n",
      "Epoch 77: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.1617 - accuracy: 0.9991 - val_loss: 0.4497 - val_accuracy: 0.9396 - lr: 0.0010\n",
      "Epoch 78/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9992\n",
      "Epoch 78: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1614 - accuracy: 0.9992 - val_loss: 0.4499 - val_accuracy: 0.9392 - lr: 0.0010\n",
      "Epoch 79/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9993\n",
      "Epoch 79: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 40s 115ms/step - loss: 0.1614 - accuracy: 0.9993 - val_loss: 0.4493 - val_accuracy: 0.9390 - lr: 0.0010\n",
      "Epoch 80/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9994\n",
      "Epoch 80: val_loss did not improve from 0.44024\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.1610 - accuracy: 0.9994 - val_loss: 0.4512 - val_accuracy: 0.9394 - lr: 0.0010\n",
      "Current:  308\n",
      "313/313 [==============================] - 9s 21ms/step\n",
      "Accuracy: 92.85\n",
      "Error: 7.150000000000006\n",
      "ECE: 0.05053877526223651\n",
      "MCE: 0.7344198822975159\n",
      "Loss: 0.3587348235516382\n",
      "brier: 0.062430451437824386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.150000000000006,\n",
       " 0.05053877526223651,\n",
       " 0.7344198822975159,\n",
       " 0.3587348235516382,\n",
       " 0.062430451437824386]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, datagen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test, freezing_list,\n",
    "                       batch_size=128, lr_schedule=[[0, 0.1], [80, 0.01], [150, 0.001]], cbks=[checkpointer],\n",
    "                       name='resnet_cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606691b-593c-4eb8-8597-c57fe502f0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
