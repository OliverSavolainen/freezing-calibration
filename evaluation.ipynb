{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUVkDBFdDUsd"
   },
   "outputs": [],
   "source": [
    "#File for evaluating all models before and after temperature scaling\n",
    "#Listed weights files are not included\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from keras.activations import softmax as keras_softmax\n",
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10,cifar100\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "import time\n",
    "from keras import Input, Model\n",
    "from keras import regularizers\n",
    "from keras.layers import (add,\n",
    "                          Conv2D, GlobalAveragePooling2D)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4LWDPSunIJfb"
   },
   "outputs": [],
   "source": [
    "def compute_acc_bin(conf_thresh_lower, conf_thresh_upper, conf, pred, true):\n",
    "    \"\"\"\n",
    "    # Computes accuracy and average confidence for bin\n",
    "\n",
    "    Args:\n",
    "        conf_thresh_lower (float): Lower Threshold of confidence interval\n",
    "        conf_thresh_upper (float): Upper Threshold of confidence interval\n",
    "        conf (numpy.ndarray): list of confidences\n",
    "        pred (numpy.ndarray): list of predictions\n",
    "        true (numpy.ndarray): list of true labels\n",
    "\n",
    "    Returns:\n",
    "        (accuracy, avg_conf, len_bin): accuracy of bin, confidence of bin and number of elements in bin.\n",
    "    \"\"\"\n",
    "    filtered_tuples = [x for x in zip(pred, true, conf) if x[2] > conf_thresh_lower and x[2] <= conf_thresh_upper]\n",
    "    if len(filtered_tuples) < 1:\n",
    "        return 0, 0, 0\n",
    "    else:\n",
    "        correct = len([x for x in filtered_tuples if x[0] == x[1]])  # How many correct labels\n",
    "        len_bin = len(filtered_tuples)  # How many elements falls into given bin\n",
    "        avg_conf = sum([x[2] for x in filtered_tuples]) / len_bin  # Avg confidence of BIN\n",
    "        accuracy = float(correct) / len_bin  # accuracy of BIN\n",
    "        return accuracy, avg_conf, len_bin\n",
    "\n",
    "\n",
    "def ECE(conf, pred, true, bin_size=0.1):\n",
    "    \"\"\"\n",
    "    Expected Calibration Error\n",
    "\n",
    "    Args:\n",
    "        conf (numpy.ndarray): list of confidences\n",
    "        pred (numpy.ndarray): list of predictions\n",
    "        true (numpy.ndarray): list of true labels\n",
    "        bin_size: (float): size of one bin (0,1)  # TODO should convert to number of bins?\n",
    "\n",
    "    Returns:\n",
    "        ece: expected calibration error\n",
    "    \"\"\"\n",
    "\n",
    "    upper_bounds = np.arange(bin_size, 1 + bin_size, bin_size)  # Get bounds of bins\n",
    "\n",
    "    n = len(conf)\n",
    "    ece = 0  # Starting error\n",
    "\n",
    "    for conf_thresh in upper_bounds:  # Go through bounds and find accuracies and confidences\n",
    "        acc, avg_conf, len_bin = compute_acc_bin(conf_thresh - bin_size, conf_thresh, conf, pred, true)\n",
    "        ece += np.abs(acc - avg_conf) * len_bin / n  # Add weigthed difference to ECE\n",
    "\n",
    "    return ece\n",
    "\n",
    "\n",
    "def MCE(conf, pred, true, bin_size=0.1):\n",
    "    \"\"\"\n",
    "    Maximal Calibration Error\n",
    "\n",
    "    Args:\n",
    "        conf (numpy.ndarray): list of confidences\n",
    "        pred (numpy.ndarray): list of predictions\n",
    "        true (numpy.ndarray): list of true labels\n",
    "        bin_size: (float): size of one bin (0,1)  # TODO should convert to number of bins?\n",
    "\n",
    "    Returns:\n",
    "        mce: maximum calibration error\n",
    "    \"\"\"\n",
    "\n",
    "    upper_bounds = np.arange(bin_size, 1 + bin_size, bin_size)\n",
    "\n",
    "    cal_errors = []\n",
    "\n",
    "    for conf_thresh in upper_bounds:\n",
    "        acc, avg_conf, _ = compute_acc_bin(conf_thresh - bin_size, conf_thresh, conf, pred, true)\n",
    "        cal_errors.append(np.abs(acc - avg_conf))\n",
    "\n",
    "    return max(cal_errors)\n",
    "\n",
    "\n",
    "def get_bin_info(conf, pred, true, bin_size=0.1):\n",
    "    \"\"\"\n",
    "    Get accuracy, confidence and elements in bin information for all the bins.\n",
    "\n",
    "    Args:\n",
    "        conf (numpy.ndarray): list of confidences\n",
    "        pred (numpy.ndarray): list of predictions\n",
    "        true (numpy.ndarray): list of true labels\n",
    "        bin_size: (float): size of one bin (0,1)  # TODO should convert to number of bins?\n",
    "\n",
    "    Returns:\n",
    "        (acc, conf, len_bins): tuple containing all the necessary info for reliability diagrams.\n",
    "    \"\"\"\n",
    "\n",
    "    upper_bounds = np.arange(bin_size, 1 + bin_size, bin_size)\n",
    "\n",
    "    accuracies = []\n",
    "    confidences = []\n",
    "    bin_lengths = []\n",
    "\n",
    "    for conf_thresh in upper_bounds:\n",
    "        acc, avg_conf, len_bin = compute_acc_bin(conf_thresh - bin_size, conf_thresh, conf, pred, true)\n",
    "        accuracies.append(acc)\n",
    "        confidences.append(avg_conf)\n",
    "        bin_lengths.append(len_bin)\n",
    "\n",
    "    return accuracies, confidences, bin_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "class TemperatureScaling():\n",
    "\n",
    "    def __init__(self, temp=1, maxiter=50, solver=\"BFGS\"):\n",
    "        \"\"\"\n",
    "        Initialize class\n",
    "        \n",
    "        Params:\n",
    "            temp (float): starting temperature, default 1\n",
    "            maxiter (int): maximum iterations done by optimizer, however 8 iterations have been maximum.\n",
    "        \"\"\"\n",
    "        self.temp = temp\n",
    "        self.maxiter = maxiter\n",
    "        self.solver = solver\n",
    "\n",
    "    def _loss_fun(self, x, probs, true):\n",
    "        # Calculates the loss using log-loss (cross-entropy loss)\n",
    "        scaled_probs = self.predict(probs, x)\n",
    "        loss = log_loss(y_true=true, y_pred=scaled_probs)\n",
    "        return loss\n",
    "\n",
    "    # Find the temperature\n",
    "    def fit(self, logits, true):\n",
    "        \"\"\"\n",
    "        Trains the model and finds optimal temperature\n",
    "        \n",
    "        Params:\n",
    "            logits: the output from neural network for each class (shape [samples, classes])\n",
    "            true: one-hot-encoding of true labels.\n",
    "            \n",
    "        Returns:\n",
    "            the results of optimizer after minimizing is finished.\n",
    "        \"\"\"\n",
    "\n",
    "        true = true.flatten()  # Flatten y_val\n",
    "        opt = minimize(self._loss_fun, x0=1, args=(logits, true), options={'maxiter': self.maxiter}, method=self.solver)\n",
    "        self.temp = opt.x[0]\n",
    "\n",
    "        return opt\n",
    "\n",
    "    def predict(self, logits, temp=None):\n",
    "        \"\"\"\n",
    "        Scales logits based on the temperature and returns calibrated probabilities\n",
    "        \n",
    "        Params:\n",
    "            logits: logits values of data (output from neural network) for each class (shape [samples, classes])\n",
    "            temp: if not set use temperatures find by model or previously set.\n",
    "            \n",
    "        Returns:\n",
    "            calibrated probabilities (nd.array with shape [samples, classes])\n",
    "        \"\"\"\n",
    "\n",
    "        if not temp:\n",
    "            logits_tensor = tf.convert_to_tensor(logits/self.temp)  # Convert NumPy array to TensorFlow tensor\n",
    "            return keras_softmax(logits_tensor, axis=-1)\n",
    "        else:\n",
    "            logits_tensor = tf.convert_to_tensor(logits/temp)\n",
    "            return keras_softmax(logits_tensor, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qI1ho0mpF7CF"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_metrics_to_csv(filename, error, ece, mce, log_loss, brier):\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['error', 'ece', 'mce', 'log_loss', 'brier']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header only if the file is empty\n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerow({\n",
    "            'error': error,\n",
    "            'ece': ece,\n",
    "            'mce': mce,\n",
    "            'log_loss': log_loss,\n",
    "            'brier': brier\n",
    "        })\n",
    "def write_mean_and_std_to_csv(filename, model_name, value_names, mean, std_dev):\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['model_name'] + [f'{name}_mean' for name in value_names] + [f'{name}_std_dev' for name in value_names]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header only if the file is empty\n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writeheader()\n",
    "\n",
    "        row_dict = {'model_name': model_name}\n",
    "        row_dict.update({f'{name}_mean': mean_value for name, mean_value in zip(value_names, mean)})\n",
    "        row_dict.update({f'{name}_std_dev': std_dev_value for name, std_dev_value in zip(value_names, std_dev)})\n",
    "        writer.writerow(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gk_M_gBRGI21"
   },
   "outputs": [],
   "source": [
    "def evaluate(probs, y_true, verbose=False, normalize=False, bins=15):\n",
    "    \"\"\"\n",
    "    Evaluate model using various scoring measures: Error Rate, ECE, MCE, NLL, Brier Score\n",
    "    \n",
    "    Params:\n",
    "        probs: a list containing probabilities for all the classes with a shape of (samples, classes)\n",
    "        y_true: a list containing the actual class labels\n",
    "        verbose: (bool) are the scores printed out. (default = False)\n",
    "        normalize: (bool) in case of 1-vs-K calibration, the probabilities need to be normalized.\n",
    "        bins: (int) - into how many bins are probabilities divided (default = 15)\n",
    "        \n",
    "    Returns:\n",
    "        (error, ece, mce, loss, brier), returns various scoring measures\n",
    "    \"\"\"\n",
    "    probs = probs.numpy()\n",
    "    preds = np.argmax(probs, axis=1)  # Take maximum confidence as prediction\n",
    "\n",
    "    if normalize:\n",
    "        confs = np.max(probs, axis=1) / np.sum(probs, axis=1)\n",
    "        # Check if everything below or equal to 1?\n",
    "    else:\n",
    "        confs = np.max(probs, axis=1)  # Take only maximum confidence\n",
    "\n",
    "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:  # If 1-hot representation, get back to numeric\n",
    "      y_true = np.array([[np.where(r == 1)[0][0]] for r in y_true])  # Back to np array also\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true, preds) * 100\n",
    "    error = 100 - accuracy\n",
    "\n",
    "    # Calculate ECE\n",
    "    ece = ECE(confs, preds, y_true, bin_size=1 / bins)\n",
    "    # Calculate MCE\n",
    "    mce = MCE(confs, preds, y_true, bin_size=1 / bins)\n",
    "    loss = log_loss(y_true=y_true, y_pred=probs)\n",
    "    # Calculate Brier score for each class\n",
    "    y_prob_true = np.array([probs[i, idx] for i, idx in enumerate(y_true)])\n",
    "    for i in range(len(y_true)):\n",
    "      y_true[i] = 1\n",
    "    brier = brier_score_loss(y_true=y_true, y_prob=y_prob_true)  # Brier Score (MSE)\n",
    "\n",
    "    return (error, ece, mce, loss, brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C3NAzJCw_fgI"
   },
   "outputs": [],
   "source": [
    "def cal_results(fn,name, logits_val,logits_test,y_val,y_test, m_kwargs={}, approach=\"all\"):\n",
    "    \"\"\"\n",
    "    Calibrate models scores, using output from logits files and given function (fn). \n",
    "    There are implemented to different approaches \"all\" and \"1-vs-K\" for calibration,\n",
    "    the approach of calibration should match with function used for calibration.\n",
    "    \n",
    "    TODO: split calibration of single and all into separate functions for more use cases.\n",
    "    \n",
    "    Params:\n",
    "        fn (class): class of the calibration method used. It must contain methods \"fit\" and \"predict\", \n",
    "                    where first fits the models and second outputs calibrated probabilities.\n",
    "        path (string): path to the folder with logits files\n",
    "        files (list of strings): pickled logits files ((logits_val, y_val), (logits_test, y_test))\n",
    "        m_kwargs (dictionary): keyword arguments for the calibration class initialization\n",
    "        approach (string): \"all\" for multiclass calibration and \"1-vs-K\" for 1-vs-K approach.\n",
    "        \n",
    "    Returns:\n",
    "        df (pandas.DataFrame): dataframe with calibrated and uncalibrated results for all the input files.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"Name\", \"Error\", \"ECE\", \"MCE\", \"Loss\", \"Brier\"])\n",
    "\n",
    "    total_t1 = time.time()\n",
    "\n",
    "    if approach == \"all\":\n",
    "        if y_val.shape[1] > 1:  # If 1-hot representation, get back to numeric\n",
    "          y_val = np.array([[np.where(r == 1)[0][0]] for r in y_val])  # Back to np array also\n",
    "\n",
    "        y_val = y_val.flatten()\n",
    "\n",
    "        model = fn(**m_kwargs)\n",
    "\n",
    "        model.fit(logits_val, y_val)\n",
    "\n",
    "        probs_val = model.predict(logits_val)\n",
    "        probs_test = model.predict(logits_test)\n",
    "        logits_tensor = tf.convert_to_tensor(logits_test)  # Convert NumPy array to TensorFlow tensor\n",
    "        error, ece, mce, loss, brier = evaluate(keras_softmax(logits_tensor, axis=-1), y_test, verbose=True)  # Test before scaling\n",
    "        error2, ece2, mce2, loss2, brier2 = evaluate(probs_test, y_test, verbose=False)\n",
    "\n",
    "        print(\"Test: Error %f; ece %f; mce %f; loss %f, brier %f\" % (error2, ece2, mce2, loss2, brier2))\n",
    "        print(\"Val: Error %f; ece %f; mce %f; loss %f, brier %f\" % evaluate(probs_val, y_val, verbose=False,\n",
    "                                                                        normalize=True))\n",
    "\n",
    "\n",
    "    else:  # 1-vs-k models\n",
    "        probs_val = keras_softmax(logits_val)  # Softmax logits\n",
    "        probs_test = keras_softmax(logits_test)\n",
    "        probs_test1 = keras_softmax(logits_test)\n",
    "        K = probs_test.shape[1]\n",
    "\n",
    "        # Go through all the classes\n",
    "        for k in range(K):\n",
    "            # Prep class labels (1 fixed true class, 0 other classes)\n",
    "            y_cal = np.array(y_val == k, dtype=\"int\")[:, 0]\n",
    "\n",
    "            # Train model\n",
    "            model = fn(**m_kwargs)\n",
    "            model.fit(probs_val[:, k], y_cal)  # Get only one column with probs for given class \"k\"\n",
    "\n",
    "            probs_val[:, k] = model.predict(probs_val[:, k])  # Predict new values based on the fittting\n",
    "            probs_test[:, k] = model.predict(probs_test[:, k])\n",
    "\n",
    "            # Replace NaN with 0, as it should be close to zero  # TODO is it needed?\n",
    "            idx_nan = np.where(np.isnan(probs_test))\n",
    "            probs_test[idx_nan] = 0\n",
    "\n",
    "            idx_nan = np.where(np.isnan(probs_val))\n",
    "            probs_val[idx_nan] = 0\n",
    "\n",
    "        # Get results for test set\n",
    "        error, ece, mce, loss, brier = evaluate(probs_test1, y_test, verbose=True, normalize=False)\n",
    "        error2, ece2, mce2, loss2, brier2 = evaluate(probs_test, y_test, verbose=False, normalize=True)\n",
    "\n",
    "        print(\"Test: Error %f; ece %f; mce %f; loss %f, brier %f\" % (error2, ece2, mce2, loss2, brier2))\n",
    "        print(\"Val: Error %f; ece %f; mce %f; loss %f, brier %f\" % evaluate(probs_val, y_val, verbose=False,\n",
    "                                                                        normalize=True))\n",
    "\n",
    "    return (error, ece, mce, loss, brier),(error2, ece2, mce2, loss2, brier2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "v-7_VJm8BETY"
   },
   "outputs": [],
   "source": [
    "#Method for applying temperature scaling to the model\n",
    "def temp_scaling(model,weights_file,name,x_val,x_test,y_val,y_test):\n",
    "    last_layer = model.layers.pop()\n",
    "    last_layer.activation = keras.activations.linear\n",
    "    i = model.input\n",
    "    o = last_layer(model.layers[-2].output)\n",
    "\n",
    "    model = keras.models.Model(inputs=i, outputs=[o])\n",
    "\n",
    "    # First load in the weights\n",
    "    model.load_weights(weights_file)\n",
    "    model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\")\n",
    "    # Next get predictions\n",
    "    logits_val = model.predict(x_val, verbose=1)\n",
    "    logits_test = model.predict(x_test, verbose=1)\n",
    "    (error, ece, mce, loss, brier),(error2, ece2, mce2, loss2, brier2) = cal_results(TemperatureScaling,name,logits_val,logits_test,y_val,y_test, approach = \"all\")\n",
    "    write_metrics_to_csv('uncal.csv', error, ece, mce, loss, brier)\n",
    "    write_metrics_to_csv('temp.csv', error2, ece2, mce2, loss2, brier2)\n",
    "\n",
    "    return error, ece, mce, loss, brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VOVEMXGwDJUO"
   },
   "outputs": [],
   "source": [
    "import densenet\n",
    "import wrn\n",
    "import resnet_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BNik6i5uFHZf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 18ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "Test: Error 7.150000; ece 0.012143; mce 0.808834; loss 0.225950, brier 0.061908\n",
      "Val: Error 6.060000; ece 0.006799; mce 0.249597; loss 0.193776, brier 0.053334\n",
      "157/157 [==============================] - 5s 18ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "Test: Error 6.930000; ece 0.007459; mce 0.236941; loss 0.210127, brier 0.059982\n",
      "Val: Error 6.320000; ece 0.009264; mce 0.261208; loss 0.204094, brier 0.055949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.930000000000007,\n",
       " 0.04833229484856122,\n",
       " 0.32229046523571014,\n",
       " 0.32020646658444546,\n",
       " 0.06022057435291904)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_n = 18\n",
    "num_classes10 = 10\n",
    "num_classes100 = 100\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "iterations = 45000 // batch_size\n",
    "weight_decay = 0.0001\n",
    "mean = [125.307, 122.95, 113.865]  # Mean (per-pixel mean?) - let it be atm\n",
    "std = [62.9932, 62.0887, 66.7048]\n",
    "seed = 333\n",
    "\n",
    "def residual_network(img_input, classes_num=10, stack_n=5):\n",
    "    def residual_block(intput, out_channel, increase=False):\n",
    "        if increase:\n",
    "            stride = (2, 2)\n",
    "        else:\n",
    "            stride = (1, 1)\n",
    "\n",
    "        pre_bn = BatchNormalization()(intput)\n",
    "        pre_relu = Activation('relu')(pre_bn)\n",
    "\n",
    "        conv_1 = Conv2D(out_channel, kernel_size=(3, 3), strides=stride, padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(pre_relu)\n",
    "        bn_1 = BatchNormalization()(conv_1)\n",
    "        relu1 = Activation('relu')(bn_1)\n",
    "        conv_2 = Conv2D(out_channel, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(relu1)\n",
    "        if increase:\n",
    "            projection = Conv2D(out_channel,\n",
    "                                kernel_size=(1, 1),\n",
    "                                strides=(2, 2),\n",
    "                                padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(weight_decay))(intput)\n",
    "            block = add([conv_2, projection])\n",
    "        else:\n",
    "            block = add([intput, conv_2])\n",
    "        return block\n",
    "\n",
    "    # build model\n",
    "    # total layers = stack_n * 3 * 2 + 2\n",
    "    # stack_n = 5 by default, total layers = 32\n",
    "    # input: 32x32x3 output: 32x32x16\n",
    "    x = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "\n",
    "    # input: 32x32x16 output: 32x32x16\n",
    "    for _ in range(stack_n):\n",
    "        x = residual_block(x, 16, False)\n",
    "\n",
    "    # input: 32x32x16 output: 16x16x32\n",
    "    x = residual_block(x, 32, True)\n",
    "    for _ in range(1, stack_n):\n",
    "        x = residual_block(x, 32, False)\n",
    "\n",
    "    # input: 16x16x32 output: 8x8x64\n",
    "    x = residual_block(x, 64, True)\n",
    "    for _ in range(1, stack_n):\n",
    "        x = residual_block(x, 64, False)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # input: 64 output: 10\n",
    "    x = Dense(classes_num, activation='softmax',\n",
    "              kernel_initializer=\"he_normal\",\n",
    "              kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    return x\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes10)\n",
    "\n",
    "# color preprocessing - using precalculated means and std-s\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1,\n",
    "                                                      random_state=seed)  # random_state = seed\n",
    "\n",
    "img_mean = x_train45.mean(axis=0)  # per-pixel mean\n",
    "img_std = x_train45.std(axis=0)\n",
    "x_train45 = (x_train45 - img_mean) / img_std\n",
    "x_val = (x_val - img_mean) / img_std\n",
    "x_test = (x_test - img_mean) / img_std\n",
    "\n",
    "img_input = Input(shape=(img_rows, img_cols, img_channels))\n",
    "output = residual_network(img_input, num_classes10, stack_n)\n",
    "model = Model(img_input, output)\n",
    "\n",
    "temp_scaling(model,'resnet_cifar10.h5','resnet_cifar10',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'resnet_cifar10_2.h5','resnet_cifar10_2',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 18ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "Test: Error 7.170000; ece 0.011899; mce 0.093680; loss 0.224852, brier 0.062412\n",
      "Val: Error 5.980000; ece 0.008525; mce 0.264836; loss 0.189279, brier 0.052737\n",
      "157/157 [==============================] - 5s 19ms/step\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "Test: Error 7.000000; ece 0.006109; mce 0.231387; loss 0.207924, brier 0.060495\n",
      "Val: Error 6.300000; ece 0.010049; mce 0.238677; loss 0.202768, brier 0.056649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.0,\n",
       " 0.046890796139836356,\n",
       " 0.34358163606161357,\n",
       " 0.3065435069459051,\n",
       " 0.06024696970467692)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_resnet_c10_best.hdf5','resnet_cifar10_best',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'model_resnet_c10_best_2.hdf5','resnet_cifar10_2_best',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8HmW9F_zH2CF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 18ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "Test: Error 29.980000; ece 0.017710; mce 0.062538; loss 1.138919, brier 0.304810\n",
      "Val: Error 28.500000; ece 0.023259; mce 0.121306; loss 1.098821, brier 0.293941\n",
      "157/157 [==============================] - 5s 18ms/step\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "Test: Error 30.320000; ece 0.025545; mce 0.061441; loss 1.164974, brier 0.308856\n",
      "Val: Error 29.300000; ece 0.022722; mce 0.075312; loss 1.103952, brier 0.299339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30.320000000000007,\n",
       " 0.18645093108713628,\n",
       " 0.3648860052113643,\n",
       " 1.7314717884921924,\n",
       " 0.280367236719004)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_n = 18\n",
    "num_classes = 100\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "iterations = 45000 // batch_size\n",
    "weight_decay = 0.0001\n",
    "seed = 333\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes100)\n",
    "\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1,\n",
    "                                                      random_state=seed)  # random_state = seed\n",
    "\n",
    "img_mean = x_train45.mean(axis=0)  # per-pixel mean\n",
    "img_std = x_train45.std(axis=0)\n",
    "x_train45 = (x_train45 - img_mean) / img_std\n",
    "x_val = (x_val - img_mean) / img_std\n",
    "x_test = (x_test - img_mean) / img_std\n",
    "img_input = Input(shape=(img_rows, img_cols, img_channels))\n",
    "output = residual_network(img_input, num_classes, stack_n)\n",
    "model = Model(img_input, output)\n",
    "\n",
    "temp_scaling(model,'resnet_cifar100.h5','resnet_cifar100',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'resnet_cifar100_2.h5','resnet_cifar100_2',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 20ms/step\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "Test: Error 29.910000; ece 0.016236; mce 0.051957; loss 1.062137, brier 0.296532\n",
      "Val: Error 28.220000; ece 0.019273; mce 0.080173; loss 1.022614, brier 0.285813\n",
      "157/157 [==============================] - 5s 18ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "Test: Error 30.310000; ece 0.017048; mce 0.044367; loss 1.079424, brier 0.298648\n",
      "Val: Error 29.500000; ece 0.015818; mce 0.071135; loss 1.033385, brier 0.292662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30.310000000000002,\n",
       " 0.1237496825732291,\n",
       " 0.22596183901061562,\n",
       " 1.2479673907868696,\n",
       " 0.27430659409144625)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_resnet_c100_best.hdf5','resnet_cifar100_best',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'model_resnet_c100_best_2.hdf5','resnet_cifar100_2_best',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "59_cVhE9K4TM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 3s 9ms/step\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "Test: Error 7.570000; ece 0.009323; mce 0.355586; loss 0.224925, brier 0.064755\n",
      "Val: Error 6.400000; ece 0.013225; mce 0.307475; loss 0.195476, brier 0.056430\n",
      "157/157 [==============================] - 2s 7ms/step\n",
      "313/313 [==============================] - 2s 7ms/step\n",
      "Test: Error 7.990000; ece 0.006296; mce 0.254074; loss 0.239117, brier 0.068429\n",
      "Val: Error 6.940000; ece 0.006648; mce 0.246471; loss 0.210352, brier 0.060541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.989999999999995,\n",
       " 0.05804203577041622,\n",
       " 0.3539295910048658,\n",
       " 0.4450498906669829,\n",
       " 0.06945803569195254)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_classes10 = 10\n",
    "nb_classes100 = 100\n",
    "\n",
    "nb_epoch = 300\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "\n",
    "img_dim = (img_channels, img_rows, img_cols) if K.image_data_format() == 'channels_first' else (img_rows, img_cols, img_channels)\n",
    "depth = 40\n",
    "nb_dense_block = 3\n",
    "growth_rate = 12\n",
    "nb_filter = -1\n",
    "dropout_rate = 0.0 # 0.0 for data augmentation\n",
    "seed = 333\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.1\n",
    "\n",
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "model = densenet.DenseNet(img_dim, classes=nb_classes10, depth=depth, nb_dense_block=nb_dense_block,\n",
    "                          growth_rate=growth_rate, nb_filter=nb_filter, dropout_rate=dropout_rate, weights=None, weight_decay=1e-4)\n",
    "\n",
    "                          \n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "#For data preprocessing, we normalize the data using the channel means and standard deviations (https://arxiv.org/pdf/1608.06993v3.pdf)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)\n",
    "\n",
    "\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "\n",
    "\n",
    "y_train45 = np_utils.to_categorical(y_train45, nb_classes10)  # 1-hot vector\n",
    "y_val = np_utils.to_categorical(y_val, nb_classes10)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes10)\n",
    "\n",
    "temp_scaling(model,'dense_cifar10.h5','dense_cifar10',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'dense_cifar10_2.h5','dense_cifar10_2',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2s 7ms/step\n",
      "313/313 [==============================] - 2s 7ms/step\n",
      "Test: Error 7.490000; ece 0.008717; mce 0.257427; loss 0.225905, brier 0.064981\n",
      "Val: Error 6.360000; ece 0.007015; mce 0.239123; loss 0.194102, brier 0.055814\n",
      "157/157 [==============================] - 2s 7ms/step\n",
      "313/313 [==============================] - 2s 7ms/step\n",
      "Test: Error 7.370000; ece 0.006262; mce 0.246989; loss 0.218735, brier 0.062979\n",
      "Val: Error 6.520000; ece 0.003578; mce 0.415141; loss 0.192373, brier 0.055955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.3700000000000045,\n",
       " 0.05018369305431836,\n",
       " 0.3286578632415609,\n",
       " 0.35682083705536327,\n",
       " 0.06372033332182321)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_dense_c10_best.hdf5','dense_cifar10_best',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'model_dense_c10_best_2.hdf5','dense_cifar10_2_best',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2s 7ms/step\n",
      "313/313 [==============================] - 2s 7ms/step\n",
      "Test: Error 8.050000; ece 0.007991; mce 0.755234; loss 0.244564, brier 0.070114\n",
      "Val: Error 7.120000; ece 0.006689; mce 0.104014; loss 0.214130, brier 0.061756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.049999999999997,\n",
       " 0.053611510443687366,\n",
       " 0.28426332821448647,\n",
       " 0.38701807015427775,\n",
       " 0.06993229177593825)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_dense_c10_best_2_cont.hdf5','dense_cifar10_2_best_cont',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3nVJnf_bK4cu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2s 7ms/step\n",
      "313/313 [==============================] - 2s 7ms/step\n",
      "Test: Error 30.020000; ece 0.009139; mce 0.101549; loss 1.070529, brier 0.297455\n",
      "Val: Error 28.920000; ece 0.013151; mce 0.063356; loss 1.057021, brier 0.294576\n",
      "157/157 [==============================] - 2s 7ms/step\n",
      "313/313 [==============================] - 2s 8ms/step\n",
      "Test: Error 29.870000; ece 0.013654; mce 0.047472; loss 1.062526, brier 0.297955\n",
      "Val: Error 28.680000; ece 0.017694; mce 0.080801; loss 1.045124, brier 0.289957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29.86999999999999,\n",
       " 0.21113098706901073,\n",
       " 0.4941648625272899,\n",
       " 1.9960649609469205,\n",
       " 0.2746932379026984)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_classes10 = 10\n",
    "nb_classes100 = 100\n",
    "\n",
    "nb_epoch = 300\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "\n",
    "img_dim = (img_channels, img_rows, img_cols) if K.image_data_format() == 'channels_first' else (img_rows, img_cols, img_channels)\n",
    "depth = 40\n",
    "nb_dense_block = 3\n",
    "growth_rate = 12\n",
    "nb_filter = 12\n",
    "dropout_rate = 0.0 # 0.0 for data augmentation\n",
    "seed = 333\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = densenet.DenseNet(img_dim, classes=nb_classes100, depth=depth, nb_dense_block=nb_dense_block,\n",
    "                          growth_rate=growth_rate, nb_filter=nb_filter, dropout_rate=dropout_rate, weights=None, weight_decay=1e-4)\n",
    "\n",
    "                          \n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "#For data preprocessing, we normalize the data using the channel means and standard deviations (https://arxiv.org/pdf/1608.06993v3.pdf)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)\n",
    "\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "\n",
    "y_train45 = np_utils.to_categorical(y_train45, nb_classes100)  # 1-hot vector\n",
    "y_val = np_utils.to_categorical(y_val, nb_classes100)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes100)\n",
    "\n",
    "temp_scaling(model,'dense_cifar100.h5','dense_cifar100',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'dense_cifar100_2.h5','dense_cifar100_2',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2s 7ms/step\n",
      "313/313 [==============================] - 2s 7ms/step\n",
      "Test: Error 41.000000; ece 0.013354; mce 0.038356; loss 1.464651, brier 0.410153\n",
      "Val: Error 39.460000; ece 0.022350; mce 0.074995; loss 1.426372, brier 0.402228\n",
      "157/157 [==============================] - 2s 7ms/step\n",
      "313/313 [==============================] - 2s 7ms/step\n",
      "Test: Error 36.880000; ece 0.010857; mce 0.029601; loss 1.318632, brier 0.373211\n",
      "Val: Error 36.840000; ece 0.014180; mce 0.074884; loss 1.309039, brier 0.372017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36.88,\n",
       " 0.1185962333671749,\n",
       " 0.23064466863870625,\n",
       " 1.4556572277420063,\n",
       " 0.3414685977690999)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_dense_c100_best.hdf5','dense_cifar100_best',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'model_dense_c100_best_2.hdf5','dense_cifar100_2_best',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ukJ8LFJlL5wA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 21ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "Test: Error 24.890000; ece 0.011509; mce 0.046618; loss 0.874497, brier 0.247391\n",
      "Val: Error 25.000000; ece 0.012717; mce 0.067875; loss 0.875597, brier 0.246840\n",
      "157/157 [==============================] - 6s 21ms/step\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "Test: Error 25.540000; ece 0.009618; mce 0.060505; loss 0.892248, brier 0.252746\n",
      "Val: Error 25.080000; ece 0.014532; mce 0.135358; loss 0.886396, brier 0.248326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25.539999999999992,\n",
       " 0.1425576159015298,\n",
       " 0.30673319780698394,\n",
       " 1.2519651670233336,\n",
       " 0.22999444853856102)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "nb_epochs = 500\n",
    "batch_size = 128\n",
    "nb_classes = 100\n",
    "seed = 333\n",
    "\n",
    "def color_preprocessing(x_train, x_val, x_test):\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')    \n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    mean = np.mean(x_train, axis=(0,1,2))  # Per channel mean\n",
    "    std = np.std(x_train, axis=(0,1,2))\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_val = (x_val - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "    \n",
    "    return x_train, x_val, x_test    \n",
    "    \n",
    "# data\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Data splitting (get additional 5k validation set)\n",
    "# Sklearn to split\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "x_train45, x_val, x_test = color_preprocessing(x_train45, x_val, x_test)  # Mean per channel    \n",
    "\n",
    "y_train45 = np_utils.to_categorical(y_train45, nb_classes)  # 1-hot vector\n",
    "y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "    \n",
    "# building and training net\n",
    "model = resnet_sd.resnet_sd_model(img_shape = (32,32), img_channels = 3, \n",
    "                        layers = 110, nb_classes = nb_classes, verbose = False)\n",
    "\n",
    "temp_scaling(model,'resnet_sd_cifar100.h5','resnet_sd_cifar100',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'resnet_sd_cifar100_2.h5','resnet_sd_cifar100_2',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "Test: Error 25.400000; ece 0.015537; mce 0.057541; loss 0.891047, brier 0.253329\n",
      "Val: Error 25.720000; ece 0.009878; mce 0.091173; loss 0.895839, brier 0.253629\n",
      "157/157 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "Test: Error 26.280000; ece 0.009767; mce 0.074996; loss 0.910788, brier 0.257595\n",
      "Val: Error 25.340000; ece 0.016301; mce 0.062810; loss 0.896131, brier 0.252719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26.28,\n",
       " 0.13251221177279948,\n",
       " 0.3038850770138277,\n",
       " 1.1787124481166609,\n",
       " 0.23442392355329608)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_resnet110SD_c100_best.hdf5','resnet_sd_cifar100_best',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'model_resnet110SD_c100_best_2.hdf5','resnet_sd_cifar100_2_best',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "# Split train data into train and validation gettting certain number of labels from each class\n",
    "def train_val_split_count(x_train, y_train, size, seed):\n",
    "    \n",
    "    if seed != None:\n",
    "        np.random.seed(seed)  # Set seed if it is stated.\n",
    "\n",
    "    labels = set(y_train.flatten())  # Get label names\n",
    "    n_labels = len(labels)  # Get number of labels\n",
    "\n",
    "    x_val = []\n",
    "    y_val = []\n",
    "    split = []\n",
    "\n",
    "    \n",
    "    for i in labels:\n",
    "        labels_i = np.where(y_train == i)[0]  # Take set of only one label\n",
    "        samples = np.random.choice(labels_i, size)  # TODO: Check if enough labels in the class\n",
    "        split.append(samples)\n",
    "\n",
    "    split = np.array(split).flatten()\n",
    "    #print(split[:10])\n",
    "\n",
    "    x_val = np.array(x_train[split])\n",
    "    y_val = np.array(y_train[split])\n",
    "    \n",
    "    x_train = np.delete(x_train, split, axis=0)\n",
    "    y_train = np.delete(y_train, split, axis=0)\n",
    "    \n",
    "    return (x_train, x_val, y_train, y_val)\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def load_data_svhn(seed = None):\n",
    "    \n",
    "\n",
    "    # Load in MatLab matrices\n",
    "    test_mat = scipy.io.loadmat('test_32x32.mat')\n",
    "    train_mat = scipy.io.loadmat('train_32x32.mat')\n",
    "    extra_mat = scipy.io.loadmat('extra_32x32.mat')\n",
    "\n",
    "\n",
    "    # Get data from matrices\n",
    "    x_test = test_mat.get('X')  #numpy arrays\n",
    "    y_test = test_mat.get('y')\n",
    "\n",
    "    x_train = train_mat.get('X')\n",
    "    y_train = train_mat.get('y')\n",
    "\n",
    "    x_extra = extra_mat.get('X')\n",
    "    y_extra = extra_mat.get('y')\n",
    "    \n",
    "\n",
    "    # Reshape the matrices\n",
    "\n",
    "    # [h,w,channels,samples] -> [samples,h,w,channels]\n",
    "    \n",
    "    x_test = np.transpose(x_test, axes=(3,0,1,2))\n",
    "    x_train = np.transpose(x_train, axes=(3,0,1,2))\n",
    "    x_extra = np.transpose(x_extra, axes=(3,0,1,2))\n",
    "\n",
    "\n",
    "    # Split DATA\n",
    "    x_train1, x_val1, y_train1, y_val1 = train_val_split_count(x_train, y_train, size = 400, seed = seed)\n",
    "    x_extra2, x_val2, y_extra2, y_val2 = train_val_split_count(x_extra, y_extra, size = 200, seed = seed)\n",
    "\n",
    "\n",
    "    # Add together train and extra data\n",
    "\n",
    "    x_train_all = np.concatenate([x_train1, x_extra2])\n",
    "    y_train_all = np.concatenate([y_train1, y_extra2])\n",
    "    \n",
    "    y_train_all -= 1  # So 0 would be smallest label and 9 biggest.\n",
    "    #NB! Note that this way the labels are not actually correct, because 10 indicates the 0, FIX this.\n",
    "\n",
    "    x_val_all = np.concatenate([x_val1, x_val2])\n",
    "    y_val_all = np.concatenate([y_val1, y_val2])\n",
    "    \n",
    "    y_val_all -= 1  # So 0 would be smallest label and 9 biggest\n",
    "    y_test -= 1  # So 0 would be smallest label and 9 biggest\n",
    "\n",
    "\n",
    "    return ((x_train_all, y_train_all), (x_val_all, y_val_all), (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data, may take some time and memory!\n",
      "(598526, 32, 32, 3)\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "def color_preprocessing(x_train, x_val, x_test):\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')    \n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    mean = np.mean(x_train, axis=(0,1,2))  # Per channel mean\n",
    "    std = np.std(x_train, axis=(0,1,2))\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_val = (x_val - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "    \n",
    "    return x_train, x_val, x_test\n",
    "  \n",
    "learning_rate = 0.1\n",
    "nb_epochs = 50\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "seed = 333\n",
    "layers = 152 # n = 25 (152-2)/6\n",
    "\n",
    "\n",
    "# data\n",
    "print(\"Loading data, may take some time and memory!\")\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_data_svhn(seed = seed)\n",
    "print(x_train.shape)\n",
    "print(\"Data loaded\")\n",
    "\n",
    "x_train, x_val, x_test = color_preprocessing(x_train, x_val, x_test)  # Per channel mean\n",
    "\n",
    "\n",
    "# Try with ImageDataGenerator, otherwise it takes massive amount of memory\n",
    "img_gen = ImageDataGenerator(\n",
    "    data_format=\"channels_last\"\n",
    ")\n",
    "\n",
    "img_gen.fit(x_train)\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)  # 1-hot vector\n",
    "y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "t_FVhI5RMkgC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 9s 27ms/step\n",
      "814/814 [==============================] - 22s 27ms/step\n",
      "Test: Error 2.301014; ece 0.008141; mce 0.316229; loss 0.099291, brier 0.022560\n",
      "Val: Error 3.183333; ece 0.004195; mce 0.157624; loss 0.117429, brier 0.029506\n",
      "188/188 [==============================] - 10s 29ms/step\n",
      "814/814 [==============================] - 23s 28ms/step\n",
      "Test: Error 2.769668; ece 0.006915; mce 0.197043; loss 0.112414, brier 0.026258\n",
      "Val: Error 3.466667; ece 0.004958; mce 0.167990; loss 0.125080, brier 0.031592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.7696681007990094,\n",
       " 0.019507523587164725,\n",
       " 0.16301905051717225,\n",
       " 0.11796248997539165,\n",
       " 0.028432896422701158)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building and training net\n",
    "model = resnet_sd.resnet_sd_model(img_shape = (32,32), img_channels = 3, \n",
    "                        layers = layers, nb_classes = nb_classes, verbose = False)\n",
    "\n",
    "temp_scaling(model,'resnet_sd_svhn.h5','resnet_sd_svhn',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'resnet_sd_svhn_2.h5','resnet_sd_svhn_2',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 8s 27ms/step\n",
      "814/814 [==============================] - 23s 28ms/step\n",
      "Test: Error 4.701905; ece 0.009102; mce 0.320366; loss 0.176724, brier 0.045495\n",
      "Val: Error 5.033333; ece 0.007897; mce 0.803988; loss 0.171608, brier 0.047293\n",
      "188/188 [==============================] - 9s 26ms/step\n",
      "814/814 [==============================] - 22s 27ms/step\n",
      "Test: Error 4.310080; ece 0.012458; mce 0.147075; loss 0.169260, brier 0.042970\n",
      "Val: Error 4.866667; ece 0.005398; mce 0.110209; loss 0.169226, brier 0.045257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.310079901659492,\n",
       " 0.027316260308979833,\n",
       " 0.1752937728608096,\n",
       " 0.17610916402339824,\n",
       " 0.04678971112771387)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_svhn_best.hdf5','resnet_sd_svhn_best',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'model_svhn_best_2.hdf5','resnet_sd_svhn_2_best',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 8s 26ms/step\n",
      "814/814 [==============================] - 22s 27ms/step\n",
      "Test: Error 2.377843; ece 0.007606; mce 0.256758; loss 0.100569, brier 0.023051\n",
      "Val: Error 3.083333; ece 0.003926; mce 0.161668; loss 0.117169, brier 0.029404\n",
      "188/188 [==============================] - 10s 29ms/step\n",
      "814/814 [==============================] - 24s 30ms/step\n",
      "Test: Error 2.769668; ece 0.007038; mce 0.187466; loss 0.112593, brier 0.026373\n",
      "Val: Error 3.416667; ece 0.005528; mce 0.234899; loss 0.125653, brier 0.031935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.7696681007990094,\n",
       " 0.02332488790534544,\n",
       " 0.21633289485092622,\n",
       " 0.12061749820046395,\n",
       " 0.029346950305191098)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_svhn_best_cont.hdf5','resnet_sd_svhn_best_cont',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'model_svhn_best_2_cont.hdf5','resnet_sd_svhn_2_best_cont',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Dm3nnSNVMkn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide Residual Network-34-10 created.\n",
      "157/157 [==============================] - 4s 22ms/step\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "Test: Error 5.930000; ece 0.005796; mce 0.242373; loss 0.185372, brier 0.052319\n",
      "Val: Error 5.540000; ece 0.004356; mce 0.218576; loss 0.171936, brier 0.048427\n",
      "157/157 [==============================] - 4s 21ms/step\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "Test: Error 6.040000; ece 0.005165; mce 0.187059; loss 0.182160, brier 0.052785\n",
      "Val: Error 5.560000; ece 0.007672; mce 0.251901; loss 0.168101, brier 0.048308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.040000000000006,\n",
       " 0.04464357871413235,\n",
       " 0.3518637418746948,\n",
       " 0.3407603477393879,\n",
       " 0.053745594327139955)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth              = 34  # 32, if ignoring conv layers carrying residuals, which are needed for increasing filter size.\n",
    "growth_rate        = 10  # Growth factor\n",
    "n                  = (depth-4)//6\n",
    "num_classes        = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels       = 3\n",
    "batch_size         = 128\n",
    "epochs             = 200\n",
    "iterations         = 45000 // batch_size\n",
    "weight_decay       = 0.0005\n",
    "seed = 333\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# color preprocessing\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "x_train45, x_val, x_test = color_preprocessing(x_train45, x_val, x_test)    \n",
    "\n",
    "y_train45 = keras.utils.to_categorical(y_train45, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# build network\n",
    "img_input = Input(shape=(img_rows,img_cols,img_channels))    \n",
    "model = wrn.create_wide_residual_network(img_input, nb_classes=num_classes, N=n, k=growth_rate, dropout=0.0)\n",
    "\n",
    "temp_scaling(model,'resnet_wide_cifar10.h5','resnet_wide_cifar10',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'resnet_wide_cifar10_2.h5','resnet_wide_cifar10_2',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 4s 21ms/step\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "Test: Error 9.090000; ece 0.008489; mce 0.107472; loss 0.265302, brier 0.079169\n",
      "Val: Error 8.200000; ece 0.010146; mce 0.130692; loss 0.247654, brier 0.073278\n",
      "157/157 [==============================] - 4s 21ms/step\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "Test: Error 6.510000; ece 0.005866; mce 0.752218; loss 0.191964, brier 0.056043\n",
      "Val: Error 5.940000; ece 0.007977; mce 0.251501; loss 0.172944, brier 0.050115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.510000000000005,\n",
       " 0.044923282471299175,\n",
       " 0.7162050306797028,\n",
       " 0.3091214689021283,\n",
       " 0.05643797845024942)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_wide_28_10_c10_best.hdf5','wide_cifar10_best',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'model_wide_28_10_c10_best_2.hdf5','wide_cifar10_2_best',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ElC_a-osNipq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide Residual Network-34-10 created.\n",
      "157/157 [==============================] - 4s 22ms/step\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "Test: Error 25.720000; ece 0.015987; mce 0.066368; loss 0.946586, brier 0.258849\n",
      "Val: Error 25.660000; ece 0.015559; mce 0.055260; loss 0.915589, brier 0.254198\n",
      "157/157 [==============================] - 4s 21ms/step\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "Test: Error 25.830000; ece 0.019773; mce 0.064194; loss 0.942272, brier 0.258776\n",
      "Val: Error 24.820000; ece 0.011945; mce 0.059819; loss 0.915681, brier 0.251552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25.83,\n",
       " 0.18215142931938166,\n",
       " 0.45353270836772286,\n",
       " 1.7764153852039823,\n",
       " 0.23724542406314614)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth              = 34  # 32, if ignoring conv layers carrying residuals, which are needed for increasing filter size.\n",
    "growth_rate        = 10  # Growth factor\n",
    "n                  = (depth-4)//6\n",
    "num_classes        = 100\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels       = 3\n",
    "batch_size         = 128\n",
    "epochs             = 200\n",
    "iterations         = 45000 // batch_size\n",
    "weight_decay       = 0.0005\n",
    "seed = 333\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# color preprocessing\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "x_train45, x_val, x_test = color_preprocessing(x_train45, x_val, x_test)    \n",
    "\n",
    "y_train45 = keras.utils.to_categorical(y_train45, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# build network\n",
    "img_input = Input(shape=(img_rows,img_cols,img_channels))    \n",
    "model = wrn.create_wide_residual_network(img_input, nb_classes=num_classes, N=n, k=growth_rate, dropout=0.0)\n",
    "\n",
    "temp_scaling(model,'resnet_wide_cifar100.h5','resnet_wide_cifar100',x_val,x_test,y_val,y_test)\n",
    "temp_scaling(model,'resnet_wide_cifar100_2.h5','resnet_wide_cifar100_2',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "P00mvhP8Nitr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 4s 21ms/step\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "Test: Error 32.800000; ece 0.011115; mce 0.032339; loss 1.155700, brier 0.324982\n",
      "Val: Error 31.300000; ece 0.014966; mce 0.089322; loss 1.129613, brier 0.317525\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' dense_15/kernel:0': Shape mismatch.The variable shape (640, 100), and the assigned value shape (640, 10) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[52], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m temp_scaling(model,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_wide_28_10_c100_best.hdf5\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwide_cifar100_best\u001B[39m\u001B[38;5;124m'\u001B[39m,x_val,x_test,y_val,y_test)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtemp_scaling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel_wide_28_10_c100_best_2.hdf5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwide_cifar100_2_best\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mx_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43mx_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 10\u001B[0m, in \u001B[0;36mtemp_scaling\u001B[0;34m(model, weights_file, name, x_val, x_test, y_val, y_test)\u001B[0m\n\u001B[1;32m      7\u001B[0m model \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mModel(inputs\u001B[38;5;241m=\u001B[39mi, outputs\u001B[38;5;241m=\u001B[39m[o])\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# First load in the weights\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msgd\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Next get predictions\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/tf_thesis/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/.conda/envs/tf_thesis/lib/python3.10/site-packages/keras/backend.py:4302\u001B[0m, in \u001B[0;36mbatch_set_value\u001B[0;34m(tuples)\u001B[0m\n\u001B[1;32m   4300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mexecuting_eagerly() \u001B[38;5;129;01mor\u001B[39;00m tf\u001B[38;5;241m.\u001B[39minside_function():\n\u001B[1;32m   4301\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x, value \u001B[38;5;129;01min\u001B[39;00m tuples:\n\u001B[0;32m-> 4302\u001B[0m         \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4303\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   4304\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_graph()\u001B[38;5;241m.\u001B[39mas_default():\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot assign value to variable ' dense_15/kernel:0': Shape mismatch.The variable shape (640, 100), and the assigned value shape (640, 10) are incompatible."
     ]
    }
   ],
   "source": [
    "temp_scaling(model,'model_wide_28_10_c100_best.hdf5','wide_cifar100_best',x_val,x_test,y_val,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 4s 21ms/step\n",
      "313/313 [==============================] - 7s 21ms/step\n",
      "Test: Error 32.480000; ece 0.017687; mce 0.077031; loss 1.131136, brier 0.323099\n",
      "Val: Error 31.540000; ece 0.017440; mce 0.084228; loss 1.109766, brier 0.316546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32.480000000000004,\n",
       " 0.0976238109499216,\n",
       " 0.17100721828941734,\n",
       " 1.216549081005753,\n",
       " 0.2995340276126886)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaling(model,'model_wide_28_10_c100_best_2.hdf5','wide_cifar100_2_best',x_val,x_test,y_val,y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_thesis",
   "language": "python",
   "name": "tf_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
