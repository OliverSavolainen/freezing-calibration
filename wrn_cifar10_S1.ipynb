{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6382c16e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T16:15:37.355928Z",
     "iopub.status.busy": "2023-04-22T16:15:37.355436Z",
     "iopub.status.idle": "2023-04-22T16:15:45.291402Z",
     "shell.execute_reply": "2023-04-22T16:15:45.290222Z"
    },
    "papermill": {
     "duration": 7.94504,
     "end_time": "2023-04-22T16:15:45.294083",
     "exception": false,
     "start_time": "2023-04-22T16:15:37.349043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import Input\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417565e2",
   "metadata": {
    "_cell_guid": "d714c626-b790-420e-b76b-1f7aab627dc7",
    "_uuid": "46386508-5d86-48fc-86f1-04e2626bb2f5",
    "execution": {
     "iopub.execute_input": "2023-04-22T16:15:45.303197Z",
     "iopub.status.busy": "2023-04-22T16:15:45.302575Z",
     "iopub.status.idle": "2023-04-22T16:15:45.349054Z",
     "shell.execute_reply": "2023-04-22T16:15:45.348033Z"
    },
    "executionInfo": {
     "elapsed": 6101,
     "status": "ok",
     "timestamp": 1682109576845,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "4vOgQ5fvinNh",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.053647,
     "end_time": "2023-04-22T16:15:45.351293",
     "exception": false,
     "start_time": "2023-04-22T16:15:45.297646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import freezing\n",
    "import wrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ca54ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T16:15:45.394189Z",
     "iopub.status.busy": "2023-04-22T16:15:45.393894Z",
     "iopub.status.idle": "2023-04-22T16:15:45.401066Z",
     "shell.execute_reply": "2023-04-22T16:15:45.399999Z"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682109576847,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "eFW7ePv8inNn",
    "papermill": {
     "duration": 0.013613,
     "end_time": "2023-04-22T16:15:45.403266",
     "exception": false,
     "start_time": "2023-04-22T16:15:45.389653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "depth              = 34  # 32, if ignoring conv layers carrying residuals, which are needed for increasing filter size.\n",
    "growth_rate        = 10  # Growth factor\n",
    "n                  = (depth-4)//6\n",
    "num_classes        = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels       = 3\n",
    "batch_size         = 128\n",
    "epochs             = 200\n",
    "iterations         = 45000 // batch_size\n",
    "weight_decay       = 0.0005\n",
    "seed = 333\n",
    "def color_preprocessing(x_train, x_val, x_test):\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')    \n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    mean = np.mean(x_train, axis=(0,1,2))  # Per channel mean\n",
    "    std = np.std(x_train, axis=(0,1,2))\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_val = (x_val - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "    \n",
    "    return x_train, x_val, x_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05d1b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T16:15:45.411001Z",
     "iopub.status.busy": "2023-04-22T16:15:45.410708Z",
     "iopub.status.idle": "2023-04-22T16:15:56.456376Z",
     "shell.execute_reply": "2023-04-22T16:15:56.455069Z"
    },
    "executionInfo": {
     "elapsed": 10712,
     "status": "ok",
     "timestamp": 1682109777593,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "oghEV45AinNo",
    "outputId": "a00812e9-f46c-4548-dec2-5af843f1ec31",
    "papermill": {
     "duration": 11.098354,
     "end_time": "2023-04-22T16:15:56.504833",
     "exception": false,
     "start_time": "2023-04-22T16:15:45.406479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# color preprocessing\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "x_train45, x_val, x_test = color_preprocessing(x_train45, x_val, x_test)\n",
    "\n",
    "y_train45 = keras.utils.to_categorical(y_train45, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# build network\n",
    "img_input = Input(shape=(img_rows,img_cols,img_channels))    \n",
    "model = wrn.create_wide_residual_network(img_input, nb_classes=num_classes, N=n, k=growth_rate, dropout=0.0)\n",
    "print(model.summary())\n",
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "\n",
    "# set callback\n",
    "checkpointer = ModelCheckpoint('model_wide_28_10_c10_best.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "# set data augmentation\n",
    "print('Using real-time data augmentation.')\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "        width_shift_range=0.125,height_shift_range=0.125,fill_mode='reflect') # Missing pixels replaced with reflections\n",
    "\n",
    "datagen.fit(x_train45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38febd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T16:15:56.549745Z",
     "iopub.status.busy": "2023-04-22T16:15:56.549384Z",
     "iopub.status.idle": "2023-04-22T16:15:56.571068Z",
     "shell.execute_reply": "2023-04-22T16:15:56.570107Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1682109600331,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "f-lVxqBhinNp",
    "papermill": {
     "duration": 0.04649,
     "end_time": "2023-04-22T16:15:56.573286",
     "exception": false,
     "start_time": "2023-04-22T16:15:56.526796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "    if i < len(model.layers) * 0.8:\n",
    "        freezing_list.append(int(epochs*0.6))\n",
    "freezing_list.append(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66aef854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T16:15:56.619178Z",
     "iopub.status.busy": "2023-04-22T16:15:56.618144Z",
     "iopub.status.idle": "2023-04-22T22:13:44.669915Z",
     "shell.execute_reply": "2023-04-22T22:13:44.668405Z"
    },
    "id": "5WwxjRTUinNp",
    "outputId": "a00ab3dd-b414-487f-9622-0896a8674e61",
    "papermill": {
     "duration": 21468.07681,
     "end_time": "2023-04-22T22:13:44.672299",
     "exception": false,
     "start_time": "2023-04-22T16:15:56.595489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-22 16:15:56        53367\n",
      "variables.h5                                   2023-04-22 16:15:57    185018824\n",
      "metadata.json                                  2023-04-22 16:15:56           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-22 16:15:56        53367\n",
      "variables.h5                                   2023-04-22 16:15:56    185018824\n",
      "metadata.json                                  2023-04-22 16:15:56           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4570 - accuracy: 0.4572\n",
      "Epoch 1: val_loss improved from inf to 1.05862, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 138s 368ms/step - loss: 1.4570 - accuracy: 0.4572 - val_loss: 1.0586 - val_accuracy: 0.6228 - lr: 0.1000\n",
      "Epoch 2/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8976 - accuracy: 0.6817\n",
      "Epoch 2: val_loss improved from 1.05862 to 0.81612, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.8976 - accuracy: 0.6817 - val_loss: 0.8161 - val_accuracy: 0.7104 - lr: 0.1000\n",
      "Epoch 3/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.7621\n",
      "Epoch 3: val_loss improved from 0.81612 to 0.61043, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.6859 - accuracy: 0.7621 - val_loss: 0.6104 - val_accuracy: 0.7940 - lr: 0.1000\n",
      "Epoch 4/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5675 - accuracy: 0.8046\n",
      "Epoch 4: val_loss improved from 0.61043 to 0.57168, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.5675 - accuracy: 0.8046 - val_loss: 0.5717 - val_accuracy: 0.8074 - lr: 0.1000\n",
      "Epoch 5/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.8303\n",
      "Epoch 5: val_loss improved from 0.57168 to 0.47002, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.4883 - accuracy: 0.8303 - val_loss: 0.4700 - val_accuracy: 0.8394 - lr: 0.1000\n",
      "Epoch 6/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8517\n",
      "Epoch 6: val_loss improved from 0.47002 to 0.42445, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.4296 - accuracy: 0.8517 - val_loss: 0.4245 - val_accuracy: 0.8510 - lr: 0.1000\n",
      "Epoch 7/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8675\n",
      "Epoch 7: val_loss improved from 0.42445 to 0.38236, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.3828 - accuracy: 0.8675 - val_loss: 0.3824 - val_accuracy: 0.8650 - lr: 0.1000\n",
      "Epoch 8/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8792\n",
      "Epoch 8: val_loss improved from 0.38236 to 0.36302, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.3478 - accuracy: 0.8792 - val_loss: 0.3630 - val_accuracy: 0.8754 - lr: 0.1000\n",
      "Epoch 9/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.8931\n",
      "Epoch 9: val_loss did not improve from 0.36302\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.3120 - accuracy: 0.8931 - val_loss: 0.3758 - val_accuracy: 0.8746 - lr: 0.1000\n",
      "Epoch 10/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.8996\n",
      "Epoch 10: val_loss improved from 0.36302 to 0.34446, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.2916 - accuracy: 0.8996 - val_loss: 0.3445 - val_accuracy: 0.8832 - lr: 0.1000\n",
      "Epoch 11/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.9070\n",
      "Epoch 11: val_loss improved from 0.34446 to 0.31138, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 128s 365ms/step - loss: 0.2667 - accuracy: 0.9070 - val_loss: 0.3114 - val_accuracy: 0.8988 - lr: 0.1000\n",
      "Epoch 12/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9151\n",
      "Epoch 12: val_loss did not improve from 0.31138\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.2413 - accuracy: 0.9151 - val_loss: 0.3537 - val_accuracy: 0.8844 - lr: 0.1000\n",
      "Epoch 13/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9236\n",
      "Epoch 13: val_loss did not improve from 0.31138\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.2197 - accuracy: 0.9236 - val_loss: 0.3239 - val_accuracy: 0.8916 - lr: 0.1000\n",
      "Epoch 14/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9294\n",
      "Epoch 14: val_loss did not improve from 0.31138\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1987 - accuracy: 0.9294 - val_loss: 0.3192 - val_accuracy: 0.9004 - lr: 0.1000\n",
      "Epoch 15/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1895 - accuracy: 0.9334\n",
      "Epoch 15: val_loss did not improve from 0.31138\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1895 - accuracy: 0.9334 - val_loss: 0.3305 - val_accuracy: 0.8974 - lr: 0.1000\n",
      "Epoch 16/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9393\n",
      "Epoch 16: val_loss improved from 0.31138 to 0.30559, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1719 - accuracy: 0.9393 - val_loss: 0.3056 - val_accuracy: 0.9022 - lr: 0.1000\n",
      "Epoch 17/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9442\n",
      "Epoch 17: val_loss improved from 0.30559 to 0.28935, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1572 - accuracy: 0.9442 - val_loss: 0.2894 - val_accuracy: 0.9124 - lr: 0.1000\n",
      "Epoch 18/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.9476\n",
      "Epoch 18: val_loss did not improve from 0.28935\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1477 - accuracy: 0.9476 - val_loss: 0.2959 - val_accuracy: 0.9086 - lr: 0.1000\n",
      "Epoch 19/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9525\n",
      "Epoch 19: val_loss did not improve from 0.28935\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1351 - accuracy: 0.9525 - val_loss: 0.3010 - val_accuracy: 0.9142 - lr: 0.1000\n",
      "Epoch 20/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9579\n",
      "Epoch 20: val_loss did not improve from 0.28935\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.1213 - accuracy: 0.9579 - val_loss: 0.2941 - val_accuracy: 0.9130 - lr: 0.1000\n",
      "Epoch 21/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9581\n",
      "Epoch 21: val_loss improved from 0.28935 to 0.28176, saving model to model_wide_28_10_c10_best.hdf5\n",
      "351/351 [==============================] - 127s 363ms/step - loss: 0.1183 - accuracy: 0.9581 - val_loss: 0.2818 - val_accuracy: 0.9180 - lr: 0.1000\n",
      "Epoch 22/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9618\n",
      "Epoch 22: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.1087 - accuracy: 0.9618 - val_loss: 0.3426 - val_accuracy: 0.9086 - lr: 0.1000\n",
      "Epoch 23/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9642\n",
      "Epoch 23: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.1002 - accuracy: 0.9642 - val_loss: 0.3133 - val_accuracy: 0.9120 - lr: 0.1000\n",
      "Epoch 24/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9671\n",
      "Epoch 24: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0931 - accuracy: 0.9671 - val_loss: 0.3226 - val_accuracy: 0.9108 - lr: 0.1000\n",
      "Epoch 25/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9703\n",
      "Epoch 25: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0831 - accuracy: 0.9703 - val_loss: 0.2893 - val_accuracy: 0.9186 - lr: 0.1000\n",
      "Epoch 26/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9718\n",
      "Epoch 26: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0806 - accuracy: 0.9718 - val_loss: 0.3247 - val_accuracy: 0.9184 - lr: 0.1000\n",
      "Epoch 27/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9724\n",
      "Epoch 27: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0790 - accuracy: 0.9724 - val_loss: 0.2862 - val_accuracy: 0.9192 - lr: 0.1000\n",
      "Epoch 28/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9743\n",
      "Epoch 28: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0736 - accuracy: 0.9743 - val_loss: 0.2928 - val_accuracy: 0.9220 - lr: 0.1000\n",
      "Epoch 29/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9764\n",
      "Epoch 29: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0660 - accuracy: 0.9764 - val_loss: 0.3316 - val_accuracy: 0.9144 - lr: 0.1000\n",
      "Epoch 30/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9793\n",
      "Epoch 30: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0576 - accuracy: 0.9793 - val_loss: 0.3239 - val_accuracy: 0.9170 - lr: 0.1000\n",
      "Epoch 31/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9784\n",
      "Epoch 31: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0626 - accuracy: 0.9784 - val_loss: 0.3477 - val_accuracy: 0.9170 - lr: 0.1000\n",
      "Epoch 32/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9790\n",
      "Epoch 32: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0582 - accuracy: 0.9790 - val_loss: 0.3151 - val_accuracy: 0.9204 - lr: 0.1000\n",
      "Epoch 33/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9824\n",
      "Epoch 33: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0512 - accuracy: 0.9824 - val_loss: 0.3232 - val_accuracy: 0.9224 - lr: 0.1000\n",
      "Epoch 34/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9827\n",
      "Epoch 34: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0499 - accuracy: 0.9827 - val_loss: 0.3241 - val_accuracy: 0.9166 - lr: 0.1000\n",
      "Epoch 35/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9831\n",
      "Epoch 35: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0486 - accuracy: 0.9831 - val_loss: 0.3240 - val_accuracy: 0.9226 - lr: 0.1000\n",
      "Epoch 36/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9850\n",
      "Epoch 36: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0431 - accuracy: 0.9850 - val_loss: 0.3307 - val_accuracy: 0.9212 - lr: 0.1000\n",
      "Epoch 37/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9855\n",
      "Epoch 37: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0428 - accuracy: 0.9855 - val_loss: 0.3277 - val_accuracy: 0.9242 - lr: 0.1000\n",
      "Epoch 38/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9870\n",
      "Epoch 38: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0387 - accuracy: 0.9870 - val_loss: 0.3178 - val_accuracy: 0.9226 - lr: 0.1000\n",
      "Epoch 39/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9868\n",
      "Epoch 39: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0384 - accuracy: 0.9868 - val_loss: 0.3340 - val_accuracy: 0.9206 - lr: 0.1000\n",
      "Epoch 40/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9884\n",
      "Epoch 40: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0346 - accuracy: 0.9884 - val_loss: 0.3387 - val_accuracy: 0.9214 - lr: 0.1000\n",
      "Epoch 41/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9874\n",
      "Epoch 41: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0369 - accuracy: 0.9874 - val_loss: 0.3147 - val_accuracy: 0.9258 - lr: 0.1000\n",
      "Epoch 42/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9886\n",
      "Epoch 42: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0331 - accuracy: 0.9886 - val_loss: 0.2902 - val_accuracy: 0.9346 - lr: 0.1000\n",
      "Epoch 43/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9889\n",
      "Epoch 43: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0324 - accuracy: 0.9889 - val_loss: 0.3259 - val_accuracy: 0.9290 - lr: 0.1000\n",
      "Epoch 44/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9898\n",
      "Epoch 44: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0303 - accuracy: 0.9898 - val_loss: 0.3385 - val_accuracy: 0.9282 - lr: 0.1000\n",
      "Epoch 45/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9901\n",
      "Epoch 45: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0288 - accuracy: 0.9901 - val_loss: 0.3054 - val_accuracy: 0.9296 - lr: 0.1000\n",
      "Epoch 46/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9897\n",
      "Epoch 46: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0294 - accuracy: 0.9897 - val_loss: 0.3168 - val_accuracy: 0.9274 - lr: 0.1000\n",
      "Epoch 47/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9911\n",
      "Epoch 47: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0259 - accuracy: 0.9911 - val_loss: 0.3596 - val_accuracy: 0.9214 - lr: 0.1000\n",
      "Epoch 48/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9905\n",
      "Epoch 48: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0268 - accuracy: 0.9905 - val_loss: 0.3788 - val_accuracy: 0.9208 - lr: 0.1000\n",
      "Epoch 49/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9900\n",
      "Epoch 49: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0288 - accuracy: 0.9900 - val_loss: 0.3431 - val_accuracy: 0.9236 - lr: 0.1000\n",
      "Epoch 50/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9919\n",
      "Epoch 50: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.3466 - val_accuracy: 0.9300 - lr: 0.1000\n",
      "Epoch 51/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9912\n",
      "Epoch 51: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0251 - accuracy: 0.9912 - val_loss: 0.3154 - val_accuracy: 0.9310 - lr: 0.1000\n",
      "Epoch 52/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9924\n",
      "Epoch 52: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0216 - accuracy: 0.9924 - val_loss: 0.3634 - val_accuracy: 0.9236 - lr: 0.1000\n",
      "Epoch 53/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9930\n",
      "Epoch 53: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0206 - accuracy: 0.9930 - val_loss: 0.3160 - val_accuracy: 0.9352 - lr: 0.1000\n",
      "Epoch 54/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9923\n",
      "Epoch 54: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0224 - accuracy: 0.9923 - val_loss: 0.3403 - val_accuracy: 0.9320 - lr: 0.1000\n",
      "Epoch 55/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9937\n",
      "Epoch 55: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0187 - accuracy: 0.9937 - val_loss: 0.3512 - val_accuracy: 0.9308 - lr: 0.1000\n",
      "Epoch 56/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9938\n",
      "Epoch 56: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0184 - accuracy: 0.9938 - val_loss: 0.3492 - val_accuracy: 0.9298 - lr: 0.1000\n",
      "Epoch 57/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9948\n",
      "Epoch 57: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.3683 - val_accuracy: 0.9296 - lr: 0.1000\n",
      "Epoch 58/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9945\n",
      "Epoch 58: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0169 - accuracy: 0.9945 - val_loss: 0.3843 - val_accuracy: 0.9312 - lr: 0.1000\n",
      "Epoch 59/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9937\n",
      "Epoch 59: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 0.3563 - val_accuracy: 0.9308 - lr: 0.1000\n",
      "Epoch 60/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9943\n",
      "Epoch 60: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0170 - accuracy: 0.9943 - val_loss: 0.3478 - val_accuracy: 0.9284 - lr: 0.1000\n",
      "Epoch 61/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9968\n",
      "Epoch 61: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0093 - accuracy: 0.9968 - val_loss: 0.3078 - val_accuracy: 0.9408 - lr: 0.0200\n",
      "Epoch 62/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 62: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.3220 - val_accuracy: 0.9378 - lr: 0.0200\n",
      "Epoch 63/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9987\n",
      "Epoch 63: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.3192 - val_accuracy: 0.9366 - lr: 0.0200\n",
      "Epoch 64/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9992\n",
      "Epoch 64: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.3017 - val_accuracy: 0.9412 - lr: 0.0200\n",
      "Epoch 65/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 65: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.2912 - val_accuracy: 0.9398 - lr: 0.0200\n",
      "Epoch 66/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 66: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.3153 - val_accuracy: 0.9390 - lr: 0.0200\n",
      "Epoch 67/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 67: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.3162 - val_accuracy: 0.9436 - lr: 0.0200\n",
      "Epoch 68/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 68: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.3132 - val_accuracy: 0.9410 - lr: 0.0200\n",
      "Epoch 69/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 69: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.3125 - val_accuracy: 0.9406 - lr: 0.0200\n",
      "Epoch 70/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 70: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3171 - val_accuracy: 0.9406 - lr: 0.0200\n",
      "Epoch 71/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 71: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.3198 - val_accuracy: 0.9414 - lr: 0.0200\n",
      "Epoch 72/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 72: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.3218 - val_accuracy: 0.9404 - lr: 0.0200\n",
      "Epoch 73/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 73: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.3129 - val_accuracy: 0.9438 - lr: 0.0200\n",
      "Epoch 74/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 74: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.3008 - val_accuracy: 0.9428 - lr: 0.0200\n",
      "Epoch 75/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 75: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.3046 - val_accuracy: 0.9418 - lr: 0.0200\n",
      "Epoch 76/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 76: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3032 - val_accuracy: 0.9450 - lr: 0.0200\n",
      "Epoch 77/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 77: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.3166 - val_accuracy: 0.9410 - lr: 0.0200\n",
      "Epoch 78/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 78: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.3204 - val_accuracy: 0.9396 - lr: 0.0200\n",
      "Epoch 79/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 79: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3233 - val_accuracy: 0.9428 - lr: 0.0200\n",
      "Epoch 80/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 80: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3184 - val_accuracy: 0.9440 - lr: 0.0200\n",
      "Epoch 81/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 81: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3287 - val_accuracy: 0.9408 - lr: 0.0200\n",
      "Epoch 82/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.8249e-04 - accuracy: 0.9998\n",
      "Epoch 82: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 9.8249e-04 - accuracy: 0.9998 - val_loss: 0.3300 - val_accuracy: 0.9394 - lr: 0.0200\n",
      "Epoch 83/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.9164e-04 - accuracy: 0.9998\n",
      "Epoch 83: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 8.9164e-04 - accuracy: 0.9998 - val_loss: 0.3264 - val_accuracy: 0.9430 - lr: 0.0200\n",
      "Epoch 84/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8885e-04 - accuracy: 0.9999\n",
      "Epoch 84: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 7.8885e-04 - accuracy: 0.9999 - val_loss: 0.3105 - val_accuracy: 0.9432 - lr: 0.0200\n",
      "Epoch 85/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.6711e-04 - accuracy: 0.9998\n",
      "Epoch 85: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 9.6711e-04 - accuracy: 0.9998 - val_loss: 0.3205 - val_accuracy: 0.9412 - lr: 0.0200\n",
      "Epoch 86/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.9921e-04 - accuracy: 0.9999\n",
      "Epoch 86: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 5.9921e-04 - accuracy: 0.9999 - val_loss: 0.3185 - val_accuracy: 0.9402 - lr: 0.0200\n",
      "Epoch 87/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.9987e-04 - accuracy: 0.9998\n",
      "Epoch 87: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 8.9987e-04 - accuracy: 0.9998 - val_loss: 0.3210 - val_accuracy: 0.9454 - lr: 0.0200\n",
      "Epoch 88/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.0729e-04 - accuracy: 0.9999\n",
      "Epoch 88: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 7.0729e-04 - accuracy: 0.9999 - val_loss: 0.3356 - val_accuracy: 0.9450 - lr: 0.0200\n",
      "Epoch 89/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.6705e-04 - accuracy: 0.9998\n",
      "Epoch 89: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 7.6705e-04 - accuracy: 0.9998 - val_loss: 0.3489 - val_accuracy: 0.9396 - lr: 0.0200\n",
      "Epoch 90/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.6776e-04 - accuracy: 0.9999\n",
      "Epoch 90: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 7.6776e-04 - accuracy: 0.9999 - val_loss: 0.3181 - val_accuracy: 0.9430 - lr: 0.0200\n",
      "Epoch 91/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7430e-04 - accuracy: 0.9999\n",
      "Epoch 91: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 7.7430e-04 - accuracy: 0.9999 - val_loss: 0.3197 - val_accuracy: 0.9420 - lr: 0.0200\n",
      "Epoch 92/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.6324e-04 - accuracy: 0.9998\n",
      "Epoch 92: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 8.6324e-04 - accuracy: 0.9998 - val_loss: 0.3214 - val_accuracy: 0.9452 - lr: 0.0200\n",
      "Epoch 93/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.0972e-04 - accuracy: 0.9999\n",
      "Epoch 93: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 7.0972e-04 - accuracy: 0.9999 - val_loss: 0.3306 - val_accuracy: 0.9430 - lr: 0.0200\n",
      "Epoch 94/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.0263e-04 - accuracy: 0.9999\n",
      "Epoch 94: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 7.0263e-04 - accuracy: 0.9999 - val_loss: 0.3394 - val_accuracy: 0.9422 - lr: 0.0200\n",
      "Epoch 95/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.5934e-04 - accuracy: 0.9999\n",
      "Epoch 95: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 6.5934e-04 - accuracy: 0.9999 - val_loss: 0.3107 - val_accuracy: 0.9448 - lr: 0.0200\n",
      "Epoch 96/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.7053e-04 - accuracy: 0.9998\n",
      "Epoch 96: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 6.7053e-04 - accuracy: 0.9998 - val_loss: 0.3366 - val_accuracy: 0.9414 - lr: 0.0200\n",
      "Epoch 97/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.7990e-04 - accuracy: 0.9998\n",
      "Epoch 97: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 7.7990e-04 - accuracy: 0.9998 - val_loss: 0.3163 - val_accuracy: 0.9420 - lr: 0.0200\n",
      "Epoch 98/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 98: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.3151 - val_accuracy: 0.9428 - lr: 0.0200\n",
      "Epoch 99/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.4586e-04 - accuracy: 0.9998\n",
      "Epoch 99: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 7.4586e-04 - accuracy: 0.9998 - val_loss: 0.3318 - val_accuracy: 0.9428 - lr: 0.0200\n",
      "Epoch 100/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.0399e-04 - accuracy: 0.9998\n",
      "Epoch 100: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 9.0399e-04 - accuracy: 0.9998 - val_loss: 0.3308 - val_accuracy: 0.9434 - lr: 0.0200\n",
      "Epoch 101/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.3436e-04 - accuracy: 1.0000\n",
      "Epoch 101: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 5.3436e-04 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9428 - lr: 0.0200\n",
      "Epoch 102/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.5309e-04 - accuracy: 0.9998\n",
      "Epoch 102: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 5.5309e-04 - accuracy: 0.9998 - val_loss: 0.3258 - val_accuracy: 0.9422 - lr: 0.0200\n",
      "Epoch 103/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.3654e-04 - accuracy: 0.9999\n",
      "Epoch 103: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 5.3654e-04 - accuracy: 0.9999 - val_loss: 0.3302 - val_accuracy: 0.9406 - lr: 0.0200\n",
      "Epoch 104/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.1721e-04 - accuracy: 0.9999\n",
      "Epoch 104: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 5.1721e-04 - accuracy: 0.9999 - val_loss: 0.3454 - val_accuracy: 0.9412 - lr: 0.0200\n",
      "Epoch 105/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.1629e-04 - accuracy: 0.9999\n",
      "Epoch 105: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 6.1629e-04 - accuracy: 0.9999 - val_loss: 0.3335 - val_accuracy: 0.9426 - lr: 0.0200\n",
      "Epoch 106/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.8190e-04 - accuracy: 0.9998\n",
      "Epoch 106: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 5.8190e-04 - accuracy: 0.9998 - val_loss: 0.3211 - val_accuracy: 0.9446 - lr: 0.0200\n",
      "Epoch 107/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.0421e-04 - accuracy: 1.0000\n",
      "Epoch 107: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 4.0421e-04 - accuracy: 1.0000 - val_loss: 0.3558 - val_accuracy: 0.9390 - lr: 0.0200\n",
      "Epoch 108/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.2399e-04 - accuracy: 0.9999\n",
      "Epoch 108: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 5.2399e-04 - accuracy: 0.9999 - val_loss: 0.3244 - val_accuracy: 0.9414 - lr: 0.0200\n",
      "Epoch 109/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.8126e-04 - accuracy: 0.9998\n",
      "Epoch 109: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 5.8126e-04 - accuracy: 0.9998 - val_loss: 0.3412 - val_accuracy: 0.9424 - lr: 0.0200\n",
      "Epoch 110/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.0877e-04 - accuracy: 0.9999\n",
      "Epoch 110: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 5.0877e-04 - accuracy: 0.9999 - val_loss: 0.3363 - val_accuracy: 0.9414 - lr: 0.0200\n",
      "Epoch 111/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.6650e-04 - accuracy: 0.9999\n",
      "Epoch 111: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 4.6650e-04 - accuracy: 0.9999 - val_loss: 0.3364 - val_accuracy: 0.9400 - lr: 0.0200\n",
      "Epoch 112/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.2653e-04 - accuracy: 0.9998\n",
      "Epoch 112: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 5.2653e-04 - accuracy: 0.9998 - val_loss: 0.3232 - val_accuracy: 0.9444 - lr: 0.0200\n",
      "Epoch 113/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.0520e-04 - accuracy: 0.9998\n",
      "Epoch 113: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 7.0520e-04 - accuracy: 0.9998 - val_loss: 0.3417 - val_accuracy: 0.9416 - lr: 0.0200\n",
      "Epoch 114/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5278e-04 - accuracy: 1.0000\n",
      "Epoch 114: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 3.5278e-04 - accuracy: 1.0000 - val_loss: 0.3444 - val_accuracy: 0.9432 - lr: 0.0200\n",
      "Epoch 115/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.0337e-04 - accuracy: 0.9999\n",
      "Epoch 115: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 5.0337e-04 - accuracy: 0.9999 - val_loss: 0.3420 - val_accuracy: 0.9448 - lr: 0.0200\n",
      "Epoch 116/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.3908e-04 - accuracy: 0.9999\n",
      "Epoch 116: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 4.3908e-04 - accuracy: 0.9999 - val_loss: 0.3297 - val_accuracy: 0.9428 - lr: 0.0200\n",
      "Epoch 117/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.6894e-04 - accuracy: 0.9999\n",
      "Epoch 117: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 5.6894e-04 - accuracy: 0.9999 - val_loss: 0.3467 - val_accuracy: 0.9408 - lr: 0.0200\n",
      "Epoch 118/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.9613e-04 - accuracy: 0.9999\n",
      "Epoch 118: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 4.9613e-04 - accuracy: 0.9999 - val_loss: 0.3473 - val_accuracy: 0.9408 - lr: 0.0200\n",
      "Epoch 119/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.1117e-04 - accuracy: 0.9998\n",
      "Epoch 119: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 7.1117e-04 - accuracy: 0.9998 - val_loss: 0.3424 - val_accuracy: 0.9414 - lr: 0.0200\n",
      "Epoch 120/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1849e-04 - accuracy: 0.9999\n",
      "Epoch 120: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 4.1849e-04 - accuracy: 0.9999 - val_loss: 0.3319 - val_accuracy: 0.9432 - lr: 0.0200\n",
      "Epoch 1/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4626e-04 - accuracy: 0.9999\n",
      "Epoch 1: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 59s 160ms/step - loss: 3.4626e-04 - accuracy: 0.9999 - val_loss: 0.3514 - val_accuracy: 0.9412 - lr: 0.0040\n",
      "Epoch 2/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.8117e-04 - accuracy: 0.9998\n",
      "Epoch 2: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 5.8117e-04 - accuracy: 0.9998 - val_loss: 0.3537 - val_accuracy: 0.9396 - lr: 0.0040\n",
      "Epoch 3/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.7036e-04 - accuracy: 1.0000\n",
      "Epoch 3: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.7036e-04 - accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9442 - lr: 0.0040\n",
      "Epoch 4/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.4143e-04 - accuracy: 0.9999\n",
      "Epoch 4: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 4.4143e-04 - accuracy: 0.9999 - val_loss: 0.3334 - val_accuracy: 0.9450 - lr: 0.0040\n",
      "Epoch 5/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.9396e-04 - accuracy: 0.9998\n",
      "Epoch 5: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 4.9396e-04 - accuracy: 0.9998 - val_loss: 0.3412 - val_accuracy: 0.9440 - lr: 0.0040\n",
      "Epoch 6/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1382e-04 - accuracy: 1.0000\n",
      "Epoch 6: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 4.1382e-04 - accuracy: 1.0000 - val_loss: 0.3510 - val_accuracy: 0.9400 - lr: 0.0040\n",
      "Epoch 7/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5154e-04 - accuracy: 0.9999\n",
      "Epoch 7: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 3.5154e-04 - accuracy: 0.9999 - val_loss: 0.3476 - val_accuracy: 0.9410 - lr: 0.0040\n",
      "Epoch 8/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.5468e-04 - accuracy: 0.9998\n",
      "Epoch 8: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 5.5468e-04 - accuracy: 0.9998 - val_loss: 0.3263 - val_accuracy: 0.9442 - lr: 0.0040\n",
      "Epoch 9/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.5361e-04 - accuracy: 0.9998\n",
      "Epoch 9: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 4.5361e-04 - accuracy: 0.9998 - val_loss: 0.3621 - val_accuracy: 0.9382 - lr: 0.0040\n",
      "Epoch 10/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2377e-04 - accuracy: 1.0000\n",
      "Epoch 10: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 3.2377e-04 - accuracy: 1.0000 - val_loss: 0.3322 - val_accuracy: 0.9442 - lr: 0.0040\n",
      "Epoch 11/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1562e-04 - accuracy: 1.0000\n",
      "Epoch 11: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 56s 158ms/step - loss: 3.1562e-04 - accuracy: 1.0000 - val_loss: 0.3448 - val_accuracy: 0.9422 - lr: 0.0040\n",
      "Epoch 12/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4815e-04 - accuracy: 0.9999\n",
      "Epoch 12: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 3.4815e-04 - accuracy: 0.9999 - val_loss: 0.3371 - val_accuracy: 0.9428 - lr: 0.0040\n",
      "Epoch 13/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.1285e-04 - accuracy: 0.9999\n",
      "Epoch 13: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 5.1285e-04 - accuracy: 0.9999 - val_loss: 0.3420 - val_accuracy: 0.9430 - lr: 0.0040\n",
      "Epoch 14/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.3047e-04 - accuracy: 0.9999\n",
      "Epoch 14: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.3047e-04 - accuracy: 0.9999 - val_loss: 0.3387 - val_accuracy: 0.9436 - lr: 0.0040\n",
      "Epoch 15/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4831e-04 - accuracy: 1.0000\n",
      "Epoch 15: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 2.4831e-04 - accuracy: 1.0000 - val_loss: 0.3332 - val_accuracy: 0.9430 - lr: 0.0040\n",
      "Epoch 16/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.2327e-04 - accuracy: 0.9999\n",
      "Epoch 16: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 5.2327e-04 - accuracy: 0.9999 - val_loss: 0.3391 - val_accuracy: 0.9424 - lr: 0.0040\n",
      "Epoch 17/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6770e-04 - accuracy: 0.9999\n",
      "Epoch 17: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 3.6770e-04 - accuracy: 0.9999 - val_loss: 0.3538 - val_accuracy: 0.9420 - lr: 0.0040\n",
      "Epoch 18/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.0150e-04 - accuracy: 0.9999\n",
      "Epoch 18: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 4.0150e-04 - accuracy: 0.9999 - val_loss: 0.3541 - val_accuracy: 0.9408 - lr: 0.0040\n",
      "Epoch 19/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.9647e-04 - accuracy: 0.9999\n",
      "Epoch 19: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.9647e-04 - accuracy: 0.9999 - val_loss: 0.3354 - val_accuracy: 0.9438 - lr: 0.0040\n",
      "Epoch 20/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.9510e-04 - accuracy: 0.9999\n",
      "Epoch 20: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 3.9510e-04 - accuracy: 0.9999 - val_loss: 0.3384 - val_accuracy: 0.9416 - lr: 0.0040\n",
      "Epoch 21/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.2390e-04 - accuracy: 1.0000\n",
      "Epoch 21: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 4.2390e-04 - accuracy: 1.0000 - val_loss: 0.3384 - val_accuracy: 0.9420 - lr: 0.0040\n",
      "Epoch 22/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.7372e-04 - accuracy: 0.9999\n",
      "Epoch 22: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 3.7372e-04 - accuracy: 0.9999 - val_loss: 0.3403 - val_accuracy: 0.9406 - lr: 0.0040\n",
      "Epoch 23/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8288e-04 - accuracy: 0.9999\n",
      "Epoch 23: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 3.8288e-04 - accuracy: 0.9999 - val_loss: 0.3589 - val_accuracy: 0.9412 - lr: 0.0040\n",
      "Epoch 24/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.7515e-04 - accuracy: 0.9998\n",
      "Epoch 24: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 4.7515e-04 - accuracy: 0.9998 - val_loss: 0.3311 - val_accuracy: 0.9444 - lr: 0.0040\n",
      "Epoch 25/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.3437e-04 - accuracy: 1.0000\n",
      "Epoch 25: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.3437e-04 - accuracy: 1.0000 - val_loss: 0.3314 - val_accuracy: 0.9428 - lr: 0.0040\n",
      "Epoch 26/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1717e-04 - accuracy: 0.9999\n",
      "Epoch 26: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 4.1717e-04 - accuracy: 0.9999 - val_loss: 0.3451 - val_accuracy: 0.9406 - lr: 0.0040\n",
      "Epoch 27/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2427e-04 - accuracy: 1.0000\n",
      "Epoch 27: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.2427e-04 - accuracy: 1.0000 - val_loss: 0.3573 - val_accuracy: 0.9408 - lr: 0.0040\n",
      "Epoch 28/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.0063e-04 - accuracy: 1.0000\n",
      "Epoch 28: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 3.0063e-04 - accuracy: 1.0000 - val_loss: 0.3486 - val_accuracy: 0.9414 - lr: 0.0040\n",
      "Epoch 29/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.2278e-04 - accuracy: 0.9999\n",
      "Epoch 29: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 4.2278e-04 - accuracy: 0.9999 - val_loss: 0.3368 - val_accuracy: 0.9448 - lr: 0.0040\n",
      "Epoch 30/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.3371e-04 - accuracy: 1.0000\n",
      "Epoch 30: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.3371e-04 - accuracy: 1.0000 - val_loss: 0.3358 - val_accuracy: 0.9414 - lr: 0.0040\n",
      "Epoch 31/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5211e-04 - accuracy: 0.9999\n",
      "Epoch 31: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 3.5211e-04 - accuracy: 0.9999 - val_loss: 0.3454 - val_accuracy: 0.9438 - lr: 0.0040\n",
      "Epoch 32/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2802e-04 - accuracy: 1.0000\n",
      "Epoch 32: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 3.2802e-04 - accuracy: 1.0000 - val_loss: 0.3411 - val_accuracy: 0.9412 - lr: 0.0040\n",
      "Epoch 33/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1524e-04 - accuracy: 1.0000\n",
      "Epoch 33: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 3.1524e-04 - accuracy: 1.0000 - val_loss: 0.3500 - val_accuracy: 0.9428 - lr: 0.0040\n",
      "Epoch 34/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.2178e-04 - accuracy: 0.9999\n",
      "Epoch 34: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 4.2178e-04 - accuracy: 0.9999 - val_loss: 0.3512 - val_accuracy: 0.9414 - lr: 0.0040\n",
      "Epoch 35/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.8941e-04 - accuracy: 1.0000\n",
      "Epoch 35: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 2.8941e-04 - accuracy: 1.0000 - val_loss: 0.3288 - val_accuracy: 0.9432 - lr: 0.0040\n",
      "Epoch 36/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1157e-04 - accuracy: 1.0000\n",
      "Epoch 36: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 4.1157e-04 - accuracy: 1.0000 - val_loss: 0.3417 - val_accuracy: 0.9428 - lr: 0.0040\n",
      "Epoch 37/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5774e-04 - accuracy: 0.9999\n",
      "Epoch 37: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.5774e-04 - accuracy: 0.9999 - val_loss: 0.3531 - val_accuracy: 0.9414 - lr: 0.0040\n",
      "Epoch 38/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2992e-04 - accuracy: 1.0000\n",
      "Epoch 38: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.2992e-04 - accuracy: 1.0000 - val_loss: 0.3465 - val_accuracy: 0.9422 - lr: 0.0040\n",
      "Epoch 39/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.2076e-04 - accuracy: 0.9999\n",
      "Epoch 39: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 4.2076e-04 - accuracy: 0.9999 - val_loss: 0.3372 - val_accuracy: 0.9428 - lr: 0.0040\n",
      "Epoch 40/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2171e-04 - accuracy: 1.0000\n",
      "Epoch 40: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.2171e-04 - accuracy: 1.0000 - val_loss: 0.3317 - val_accuracy: 0.9428 - lr: 0.0040\n",
      "Epoch 41/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2233e-04 - accuracy: 1.0000\n",
      "Epoch 41: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 56s 158ms/step - loss: 3.2233e-04 - accuracy: 1.0000 - val_loss: 0.3464 - val_accuracy: 0.9422 - lr: 8.0000e-04\n",
      "Epoch 42/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.3300e-04 - accuracy: 0.9999\n",
      "Epoch 42: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.3300e-04 - accuracy: 0.9999 - val_loss: 0.3426 - val_accuracy: 0.9430 - lr: 8.0000e-04\n",
      "Epoch 43/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.0095e-04 - accuracy: 0.9999\n",
      "Epoch 43: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 5.0095e-04 - accuracy: 0.9999 - val_loss: 0.3472 - val_accuracy: 0.9434 - lr: 8.0000e-04\n",
      "Epoch 44/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.9003e-04 - accuracy: 0.9999\n",
      "Epoch 44: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 3.9003e-04 - accuracy: 0.9999 - val_loss: 0.3499 - val_accuracy: 0.9416 - lr: 8.0000e-04\n",
      "Epoch 45/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4889e-04 - accuracy: 0.9999\n",
      "Epoch 45: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.4889e-04 - accuracy: 0.9999 - val_loss: 0.3572 - val_accuracy: 0.9436 - lr: 8.0000e-04\n",
      "Epoch 46/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.5453e-04 - accuracy: 1.0000\n",
      "Epoch 46: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 2.5453e-04 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9406 - lr: 8.0000e-04\n",
      "Epoch 47/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8798e-04 - accuracy: 1.0000\n",
      "Epoch 47: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.8798e-04 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 0.9462 - lr: 8.0000e-04\n",
      "Epoch 48/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2036e-04 - accuracy: 0.9999\n",
      "Epoch 48: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.2036e-04 - accuracy: 0.9999 - val_loss: 0.3561 - val_accuracy: 0.9396 - lr: 8.0000e-04\n",
      "Epoch 49/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6109e-04 - accuracy: 0.9999\n",
      "Epoch 49: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.6109e-04 - accuracy: 0.9999 - val_loss: 0.3479 - val_accuracy: 0.9410 - lr: 8.0000e-04\n",
      "Epoch 50/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3746e-04 - accuracy: 1.0000\n",
      "Epoch 50: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 2.3746e-04 - accuracy: 1.0000 - val_loss: 0.3461 - val_accuracy: 0.9436 - lr: 8.0000e-04\n",
      "Epoch 51/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.9702e-04 - accuracy: 0.9999\n",
      "Epoch 51: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 56s 160ms/step - loss: 3.9702e-04 - accuracy: 0.9999 - val_loss: 0.3515 - val_accuracy: 0.9422 - lr: 8.0000e-04\n",
      "Epoch 52/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.7764e-04 - accuracy: 0.9999\n",
      "Epoch 52: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 2.7764e-04 - accuracy: 0.9999 - val_loss: 0.3616 - val_accuracy: 0.9426 - lr: 8.0000e-04\n",
      "Epoch 53/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1047e-04 - accuracy: 1.0000\n",
      "Epoch 53: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.1047e-04 - accuracy: 1.0000 - val_loss: 0.3528 - val_accuracy: 0.9428 - lr: 8.0000e-04\n",
      "Epoch 54/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6757e-04 - accuracy: 0.9999\n",
      "Epoch 54: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 3.6757e-04 - accuracy: 0.9999 - val_loss: 0.3447 - val_accuracy: 0.9412 - lr: 8.0000e-04\n",
      "Epoch 55/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6181e-04 - accuracy: 0.9999\n",
      "Epoch 55: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.6181e-04 - accuracy: 0.9999 - val_loss: 0.3605 - val_accuracy: 0.9420 - lr: 8.0000e-04\n",
      "Epoch 56/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0933e-04 - accuracy: 1.0000\n",
      "Epoch 56: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 2.0933e-04 - accuracy: 1.0000 - val_loss: 0.3457 - val_accuracy: 0.9420 - lr: 8.0000e-04\n",
      "Epoch 57/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.0557e-04 - accuracy: 1.0000\n",
      "Epoch 57: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.0557e-04 - accuracy: 1.0000 - val_loss: 0.3445 - val_accuracy: 0.9432 - lr: 8.0000e-04\n",
      "Epoch 58/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6925e-04 - accuracy: 0.9999\n",
      "Epoch 58: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.6925e-04 - accuracy: 0.9999 - val_loss: 0.3335 - val_accuracy: 0.9428 - lr: 8.0000e-04\n",
      "Epoch 59/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1843e-04 - accuracy: 0.9999\n",
      "Epoch 59: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 4.1843e-04 - accuracy: 0.9999 - val_loss: 0.3317 - val_accuracy: 0.9462 - lr: 8.0000e-04\n",
      "Epoch 60/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3688e-04 - accuracy: 1.0000\n",
      "Epoch 60: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 2.3688e-04 - accuracy: 1.0000 - val_loss: 0.3470 - val_accuracy: 0.9402 - lr: 8.0000e-04\n",
      "Epoch 61/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.3611e-04 - accuracy: 0.9999\n",
      "Epoch 61: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 56s 159ms/step - loss: 3.3611e-04 - accuracy: 0.9999 - val_loss: 0.3307 - val_accuracy: 0.9418 - lr: 8.0000e-04\n",
      "Epoch 62/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.0067e-04 - accuracy: 1.0000\n",
      "Epoch 62: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 3.0067e-04 - accuracy: 1.0000 - val_loss: 0.3287 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Epoch 63/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1730e-04 - accuracy: 1.0000\n",
      "Epoch 63: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 3.1730e-04 - accuracy: 1.0000 - val_loss: 0.3406 - val_accuracy: 0.9418 - lr: 8.0000e-04\n",
      "Epoch 64/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1415e-04 - accuracy: 0.9999\n",
      "Epoch 64: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 4.1415e-04 - accuracy: 0.9999 - val_loss: 0.3643 - val_accuracy: 0.9404 - lr: 8.0000e-04\n",
      "Epoch 65/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2857e-04 - accuracy: 1.0000\n",
      "Epoch 65: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 3.2857e-04 - accuracy: 1.0000 - val_loss: 0.3519 - val_accuracy: 0.9412 - lr: 8.0000e-04\n",
      "Epoch 66/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.0855e-04 - accuracy: 1.0000\n",
      "Epoch 66: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 60s 170ms/step - loss: 3.0855e-04 - accuracy: 1.0000 - val_loss: 0.3342 - val_accuracy: 0.9422 - lr: 8.0000e-04\n",
      "Epoch 67/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0865e-04 - accuracy: 1.0000\n",
      "Epoch 67: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 60s 171ms/step - loss: 2.0865e-04 - accuracy: 1.0000 - val_loss: 0.3382 - val_accuracy: 0.9434 - lr: 8.0000e-04\n",
      "Epoch 68/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4329e-04 - accuracy: 0.9999\n",
      "Epoch 68: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 60s 172ms/step - loss: 3.4329e-04 - accuracy: 0.9999 - val_loss: 0.3385 - val_accuracy: 0.9432 - lr: 8.0000e-04\n",
      "Epoch 69/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4988e-04 - accuracy: 0.9999\n",
      "Epoch 69: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 3.4988e-04 - accuracy: 0.9999 - val_loss: 0.3376 - val_accuracy: 0.9434 - lr: 8.0000e-04\n",
      "Epoch 70/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2217e-04 - accuracy: 0.9999\n",
      "Epoch 70: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 3.2217e-04 - accuracy: 0.9999 - val_loss: 0.3489 - val_accuracy: 0.9414 - lr: 8.0000e-04\n",
      "Epoch 71/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.9040e-04 - accuracy: 0.9999\n",
      "Epoch 71: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 158ms/step - loss: 3.9040e-04 - accuracy: 0.9999 - val_loss: 0.3518 - val_accuracy: 0.9406 - lr: 8.0000e-04\n",
      "Epoch 72/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.0958e-04 - accuracy: 0.9999\n",
      "Epoch 72: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 5.0958e-04 - accuracy: 0.9999 - val_loss: 0.3463 - val_accuracy: 0.9436 - lr: 8.0000e-04\n",
      "Epoch 73/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4079e-04 - accuracy: 1.0000\n",
      "Epoch 73: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 2.4079e-04 - accuracy: 1.0000 - val_loss: 0.3320 - val_accuracy: 0.9436 - lr: 8.0000e-04\n",
      "Epoch 74/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.6305e-04 - accuracy: 0.9998\n",
      "Epoch 74: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 6.6305e-04 - accuracy: 0.9998 - val_loss: 0.3381 - val_accuracy: 0.9438 - lr: 8.0000e-04\n",
      "Epoch 75/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5754e-04 - accuracy: 0.9999\n",
      "Epoch 75: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.5754e-04 - accuracy: 0.9999 - val_loss: 0.3380 - val_accuracy: 0.9424 - lr: 8.0000e-04\n",
      "Epoch 76/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.3774e-04 - accuracy: 1.0000\n",
      "Epoch 76: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 3.3774e-04 - accuracy: 1.0000 - val_loss: 0.3422 - val_accuracy: 0.9404 - lr: 8.0000e-04\n",
      "Epoch 77/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.7372e-04 - accuracy: 0.9999\n",
      "Epoch 77: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 3.7372e-04 - accuracy: 0.9999 - val_loss: 0.3420 - val_accuracy: 0.9424 - lr: 8.0000e-04\n",
      "Epoch 78/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1389e-04 - accuracy: 0.9999\n",
      "Epoch 78: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 157ms/step - loss: 3.1389e-04 - accuracy: 0.9999 - val_loss: 0.3377 - val_accuracy: 0.9430 - lr: 8.0000e-04\n",
      "Epoch 79/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5513e-04 - accuracy: 0.9999\n",
      "Epoch 79: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.5513e-04 - accuracy: 0.9999 - val_loss: 0.3481 - val_accuracy: 0.9422 - lr: 8.0000e-04\n",
      "Epoch 80/80\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2976e-04 - accuracy: 1.0000\n",
      "Epoch 80: val_loss did not improve from 0.28176\n",
      "351/351 [==============================] - 55s 156ms/step - loss: 3.2976e-04 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Current:  91\n",
      "313/313 [==============================] - 11s 32ms/step\n",
      "Accuracy: 94.07\n",
      "Error: 5.930000000000007\n",
      "ECE: 0.043575341650843584\n",
      "MCE: 0.4402398339339665\n",
      "Loss: 0.3621238171603589\n",
      "brier: 0.05274446348871372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.930000000000007,\n",
       " 0.043575341650843584,\n",
       " 0.4402398339339665,\n",
       " 0.3621238171603589,\n",
       " 0.05274446348871372]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, datagen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list,lr_schedule = [[0, 0.1],[epochs*0.3,0.02],[epochs*0.6,0.004],[epochs*0.8,0.0008]],cbks=[checkpointer], name='resnet_wide_cifar10')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21532.252031,
   "end_time": "2023-04-22T22:14:20.416215",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-22T16:15:28.164184",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
