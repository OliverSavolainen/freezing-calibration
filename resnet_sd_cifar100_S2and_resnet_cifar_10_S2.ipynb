{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609efbdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:20:51.910104Z",
     "iopub.status.busy": "2023-04-24T21:20:51.908824Z",
     "iopub.status.idle": "2023-04-24T21:20:59.793020Z",
     "shell.execute_reply": "2023-04-24T21:20:59.791841Z"
    },
    "papermill": {
     "duration": 7.893412,
     "end_time": "2023-04-24T21:20:59.795936",
     "exception": false,
     "start_time": "2023-04-24T21:20:51.902524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keras import Input, Model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import (add, Conv2D, GlobalAveragePooling2D)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba123d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:20:59.804196Z",
     "iopub.status.busy": "2023-04-24T21:20:59.803563Z",
     "iopub.status.idle": "2023-04-24T21:20:59.849802Z",
     "shell.execute_reply": "2023-04-24T21:20:59.848832Z"
    },
    "papermill": {
     "duration": 0.052976,
     "end_time": "2023-04-24T21:20:59.852081",
     "exception": false,
     "start_time": "2023-04-24T21:20:59.799105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import freezing\n",
    "import resnet_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58086fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:20:59.892198Z",
     "iopub.status.busy": "2023-04-24T21:20:59.891892Z",
     "iopub.status.idle": "2023-04-24T21:20:59.899178Z",
     "shell.execute_reply": "2023-04-24T21:20:59.898263Z"
    },
    "papermill": {
     "duration": 0.013583,
     "end_time": "2023-04-24T21:20:59.901335",
     "exception": false,
     "start_time": "2023-04-24T21:20:59.887752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Per channel mean and std normalization\n",
    "def color_preprocessing(x_train, x_val, x_test):\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')    \n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    mean = np.mean(x_train, axis=(0,1,2))  # Per channel mean\n",
    "    std = np.std(x_train, axis=(0,1,2))\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_val = (x_val - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "    \n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44618bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:20:59.908446Z",
     "iopub.status.busy": "2023-04-24T21:20:59.908164Z",
     "iopub.status.idle": "2023-04-24T21:21:14.072717Z",
     "shell.execute_reply": "2023-04-24T21:21:14.069910Z"
    },
    "papermill": {
     "duration": 14.969336,
     "end_time": "2023-04-24T21:21:14.873588",
     "exception": false,
     "start_time": "2023-04-24T21:20:59.904252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "nb_epochs = 500\n",
    "batch_size = 128\n",
    "nb_classes = 100\n",
    "seed = 333\n",
    "\n",
    "\n",
    "# data\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "\n",
    "# Data splitting (get additional 5k validation set)\n",
    "# Sklearn to split\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "x_train45, x_val, x_test = color_preprocessing(x_train45, x_val, x_test)\n",
    "\n",
    "\n",
    "img_gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    data_format=\"channels_last\",\n",
    "    width_shift_range=0.125,  # 0.125*32 = 4 so max padding of 4 pixels, as described in paper.\n",
    "    height_shift_range=0.125,\n",
    "    fill_mode=\"constant\",\n",
    "    cval = 0\n",
    ")\n",
    "\n",
    "img_gen.fit(x_train45)\n",
    "\n",
    "y_train45 = np_utils.to_categorical(y_train45, nb_classes)  # 1-hot vector\n",
    "y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "# building and training net\n",
    "model = resnet_sd.resnet_sd_model(img_shape = (32,32), img_channels = 3, layers = 110, nb_classes = nb_classes, verbose = True)\n",
    "\n",
    "sgd = SGD(learning_rate=0.1, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "checkpointer = ModelCheckpoint('model_resnet110SD_c100_best_2.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9119d5c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:21:14.998576Z",
     "iopub.status.busy": "2023-04-24T21:21:14.997913Z",
     "iopub.status.idle": "2023-04-24T21:21:15.195456Z",
     "shell.execute_reply": "2023-04-24T21:21:15.194404Z"
    },
    "papermill": {
     "duration": 0.265538,
     "end_time": "2023-04-24T21:21:15.198013",
     "exception": false,
     "start_time": "2023-04-24T21:21:14.932475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.9:\n",
    "    freezing_list.append(int(nb_epochs*0.6))\n",
    "  elif i < len(model.layers) * 0.98:\n",
    "    freezing_list.append(int(nb_epochs*0.96))\n",
    "freezing_list.append(nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e222f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:21:15.316346Z",
     "iopub.status.busy": "2023-04-24T21:21:15.315747Z",
     "iopub.status.idle": "2023-04-25T04:44:44.605313Z",
     "shell.execute_reply": "2023-04-25T04:44:44.604221Z"
    },
    "papermill": {
     "duration": 26609.34883,
     "end_time": "2023-04-25T04:44:44.607662",
     "exception": false,
     "start_time": "2023-04-24T21:21:15.258832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......average_pooling2d_2\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......lambda\n",
      ".........vars\n",
      "......lambda_1\n",
      ".........vars\n",
      "......lambda_10\n",
      ".........vars\n",
      "......lambda_11\n",
      ".........vars\n",
      "......lambda_12\n",
      ".........vars\n",
      "......lambda_13\n",
      ".........vars\n",
      "......lambda_14\n",
      ".........vars\n",
      "......lambda_15\n",
      ".........vars\n",
      "......lambda_16\n",
      ".........vars\n",
      "......lambda_17\n",
      ".........vars\n",
      "......lambda_18\n",
      ".........vars\n",
      "......lambda_19\n",
      ".........vars\n",
      "......lambda_2\n",
      ".........vars\n",
      "......lambda_20\n",
      ".........vars\n",
      "......lambda_21\n",
      ".........vars\n",
      "......lambda_22\n",
      ".........vars\n",
      "......lambda_23\n",
      ".........vars\n",
      "......lambda_24\n",
      ".........vars\n",
      "......lambda_25\n",
      ".........vars\n",
      "......lambda_26\n",
      ".........vars\n",
      "......lambda_27\n",
      ".........vars\n",
      "......lambda_28\n",
      ".........vars\n",
      "......lambda_29\n",
      ".........vars\n",
      "......lambda_3\n",
      ".........vars\n",
      "......lambda_30\n",
      ".........vars\n",
      "......lambda_31\n",
      ".........vars\n",
      "......lambda_32\n",
      ".........vars\n",
      "......lambda_33\n",
      ".........vars\n",
      "......lambda_34\n",
      ".........vars\n",
      "......lambda_35\n",
      ".........vars\n",
      "......lambda_36\n",
      ".........vars\n",
      "......lambda_37\n",
      ".........vars\n",
      "......lambda_38\n",
      ".........vars\n",
      "......lambda_39\n",
      ".........vars\n",
      "......lambda_4\n",
      ".........vars\n",
      "......lambda_40\n",
      ".........vars\n",
      "......lambda_41\n",
      ".........vars\n",
      "......lambda_42\n",
      ".........vars\n",
      "......lambda_43\n",
      ".........vars\n",
      "......lambda_44\n",
      ".........vars\n",
      "......lambda_45\n",
      ".........vars\n",
      "......lambda_46\n",
      ".........vars\n",
      "......lambda_47\n",
      ".........vars\n",
      "......lambda_48\n",
      ".........vars\n",
      "......lambda_49\n",
      ".........vars\n",
      "......lambda_5\n",
      ".........vars\n",
      "......lambda_50\n",
      ".........vars\n",
      "......lambda_51\n",
      ".........vars\n",
      "......lambda_52\n",
      ".........vars\n",
      "......lambda_53\n",
      ".........vars\n",
      "......lambda_54\n",
      ".........vars\n",
      "......lambda_55\n",
      ".........vars\n",
      "......lambda_6\n",
      ".........vars\n",
      "......lambda_7\n",
      ".........vars\n",
      "......lambda_8\n",
      ".........vars\n",
      "......lambda_9\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-24 21:21:15       208671\n",
      "variables.h5                                   2023-04-24 21:21:16      7954080\n",
      "metadata.json                                  2023-04-24 21:21:15           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-24 21:21:14       208671\n",
      "variables.h5                                   2023-04-24 21:21:16      7954080\n",
      "metadata.json                                  2023-04-24 21:21:14           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......average_pooling2d_2\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......lambda\n",
      ".........vars\n",
      "......lambda_1\n",
      ".........vars\n",
      "......lambda_10\n",
      ".........vars\n",
      "......lambda_11\n",
      ".........vars\n",
      "......lambda_12\n",
      ".........vars\n",
      "......lambda_13\n",
      ".........vars\n",
      "......lambda_14\n",
      ".........vars\n",
      "......lambda_15\n",
      ".........vars\n",
      "......lambda_16\n",
      ".........vars\n",
      "......lambda_17\n",
      ".........vars\n",
      "......lambda_18\n",
      ".........vars\n",
      "......lambda_19\n",
      ".........vars\n",
      "......lambda_2\n",
      ".........vars\n",
      "......lambda_20\n",
      ".........vars\n",
      "......lambda_21\n",
      ".........vars\n",
      "......lambda_22\n",
      ".........vars\n",
      "......lambda_23\n",
      ".........vars\n",
      "......lambda_24\n",
      ".........vars\n",
      "......lambda_25\n",
      ".........vars\n",
      "......lambda_26\n",
      ".........vars\n",
      "......lambda_27\n",
      ".........vars\n",
      "......lambda_28\n",
      ".........vars\n",
      "......lambda_29\n",
      ".........vars\n",
      "......lambda_3\n",
      ".........vars\n",
      "......lambda_30\n",
      ".........vars\n",
      "......lambda_31\n",
      ".........vars\n",
      "......lambda_32\n",
      ".........vars\n",
      "......lambda_33\n",
      ".........vars\n",
      "......lambda_34\n",
      ".........vars\n",
      "......lambda_35\n",
      ".........vars\n",
      "......lambda_36\n",
      ".........vars\n",
      "......lambda_37\n",
      ".........vars\n",
      "......lambda_38\n",
      ".........vars\n",
      "......lambda_39\n",
      ".........vars\n",
      "......lambda_4\n",
      ".........vars\n",
      "......lambda_40\n",
      ".........vars\n",
      "......lambda_41\n",
      ".........vars\n",
      "......lambda_42\n",
      ".........vars\n",
      "......lambda_43\n",
      ".........vars\n",
      "......lambda_44\n",
      ".........vars\n",
      "......lambda_45\n",
      ".........vars\n",
      "......lambda_46\n",
      ".........vars\n",
      "......lambda_47\n",
      ".........vars\n",
      "......lambda_48\n",
      ".........vars\n",
      "......lambda_49\n",
      ".........vars\n",
      "......lambda_5\n",
      ".........vars\n",
      "......lambda_50\n",
      ".........vars\n",
      "......lambda_51\n",
      ".........vars\n",
      "......lambda_52\n",
      ".........vars\n",
      "......lambda_53\n",
      ".........vars\n",
      "......lambda_54\n",
      ".........vars\n",
      "......lambda_55\n",
      ".........vars\n",
      "......lambda_6\n",
      ".........vars\n",
      "......lambda_7\n",
      ".........vars\n",
      "......lambda_8\n",
      ".........vars\n",
      "......lambda_9\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.7847 - accuracy: 0.0944\n",
      "Epoch 1: val_loss improved from inf to 4.88339, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 67s 134ms/step - loss: 4.7847 - accuracy: 0.0944 - val_loss: 4.8834 - val_accuracy: 0.1176 - lr: 0.1000\n",
      "Epoch 2/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1605 - accuracy: 0.1767\n",
      "Epoch 2: val_loss improved from 4.88339 to 4.32309, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 4.1605 - accuracy: 0.1767 - val_loss: 4.3231 - val_accuracy: 0.1736 - lr: 0.1000\n",
      "Epoch 3/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8352 - accuracy: 0.2242\n",
      "Epoch 3: val_loss did not improve from 4.32309\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 3.8352 - accuracy: 0.2242 - val_loss: 4.8663 - val_accuracy: 0.1710 - lr: 0.1000\n",
      "Epoch 4/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5499 - accuracy: 0.2688\n",
      "Epoch 4: val_loss improved from 4.32309 to 3.57553, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 3.5499 - accuracy: 0.2688 - val_loss: 3.5755 - val_accuracy: 0.2884 - lr: 0.1000\n",
      "Epoch 5/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.3260 - accuracy: 0.3081\n",
      "Epoch 5: val_loss improved from 3.57553 to 3.49187, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 3.3260 - accuracy: 0.3081 - val_loss: 3.4919 - val_accuracy: 0.2918 - lr: 0.1000\n",
      "Epoch 6/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1623 - accuracy: 0.3374\n",
      "Epoch 6: val_loss improved from 3.49187 to 3.00086, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 3.1623 - accuracy: 0.3374 - val_loss: 3.0009 - val_accuracy: 0.3692 - lr: 0.1000\n",
      "Epoch 7/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9946 - accuracy: 0.3691\n",
      "Epoch 7: val_loss did not improve from 3.00086\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 2.9946 - accuracy: 0.3691 - val_loss: 3.7095 - val_accuracy: 0.2874 - lr: 0.1000\n",
      "Epoch 8/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.8680 - accuracy: 0.3915\n",
      "Epoch 8: val_loss improved from 3.00086 to 2.88174, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 2.8680 - accuracy: 0.3915 - val_loss: 2.8817 - val_accuracy: 0.4008 - lr: 0.1000\n",
      "Epoch 9/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.7464 - accuracy: 0.4120\n",
      "Epoch 9: val_loss improved from 2.88174 to 2.81007, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 2.7464 - accuracy: 0.4120 - val_loss: 2.8101 - val_accuracy: 0.4150 - lr: 0.1000\n",
      "Epoch 10/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.6443 - accuracy: 0.4354\n",
      "Epoch 10: val_loss did not improve from 2.81007\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 2.6443 - accuracy: 0.4354 - val_loss: 2.8654 - val_accuracy: 0.4066 - lr: 0.1000\n",
      "Epoch 11/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.5560 - accuracy: 0.4529\n",
      "Epoch 11: val_loss did not improve from 2.81007\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 2.5560 - accuracy: 0.4529 - val_loss: 3.0491 - val_accuracy: 0.3936 - lr: 0.1000\n",
      "Epoch 12/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4729 - accuracy: 0.4662\n",
      "Epoch 12: val_loss improved from 2.81007 to 2.53921, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 2.4729 - accuracy: 0.4662 - val_loss: 2.5392 - val_accuracy: 0.4592 - lr: 0.1000\n",
      "Epoch 13/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4147 - accuracy: 0.4785\n",
      "Epoch 13: val_loss did not improve from 2.53921\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 2.4147 - accuracy: 0.4785 - val_loss: 2.6562 - val_accuracy: 0.4404 - lr: 0.1000\n",
      "Epoch 14/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3360 - accuracy: 0.4960\n",
      "Epoch 14: val_loss improved from 2.53921 to 2.44166, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 2.3360 - accuracy: 0.4960 - val_loss: 2.4417 - val_accuracy: 0.4808 - lr: 0.1000\n",
      "Epoch 15/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.2813 - accuracy: 0.5070\n",
      "Epoch 15: val_loss improved from 2.44166 to 2.37074, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 2.2813 - accuracy: 0.5070 - val_loss: 2.3707 - val_accuracy: 0.4978 - lr: 0.1000\n",
      "Epoch 16/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.2205 - accuracy: 0.5202\n",
      "Epoch 16: val_loss did not improve from 2.37074\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 2.2205 - accuracy: 0.5202 - val_loss: 2.5148 - val_accuracy: 0.4840 - lr: 0.1000\n",
      "Epoch 17/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.1731 - accuracy: 0.5323\n",
      "Epoch 17: val_loss improved from 2.37074 to 2.24170, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 2.1731 - accuracy: 0.5323 - val_loss: 2.2417 - val_accuracy: 0.5220 - lr: 0.1000\n",
      "Epoch 18/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.1234 - accuracy: 0.5426\n",
      "Epoch 18: val_loss did not improve from 2.24170\n",
      "351/351 [==============================] - 42s 121ms/step - loss: 2.1234 - accuracy: 0.5426 - val_loss: 2.2712 - val_accuracy: 0.5160 - lr: 0.1000\n",
      "Epoch 19/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0869 - accuracy: 0.5472\n",
      "Epoch 19: val_loss improved from 2.24170 to 2.16843, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 2.0869 - accuracy: 0.5472 - val_loss: 2.1684 - val_accuracy: 0.5320 - lr: 0.1000\n",
      "Epoch 20/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0472 - accuracy: 0.5568\n",
      "Epoch 20: val_loss did not improve from 2.16843\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 2.0472 - accuracy: 0.5568 - val_loss: 2.2886 - val_accuracy: 0.5276 - lr: 0.1000\n",
      "Epoch 21/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0070 - accuracy: 0.5658\n",
      "Epoch 21: val_loss did not improve from 2.16843\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 2.0070 - accuracy: 0.5658 - val_loss: 2.1812 - val_accuracy: 0.5382 - lr: 0.1000\n",
      "Epoch 22/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9868 - accuracy: 0.5715\n",
      "Epoch 22: val_loss improved from 2.16843 to 2.02640, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.9868 - accuracy: 0.5715 - val_loss: 2.0264 - val_accuracy: 0.5732 - lr: 0.1000\n",
      "Epoch 23/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9434 - accuracy: 0.5821\n",
      "Epoch 23: val_loss did not improve from 2.02640\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.9434 - accuracy: 0.5821 - val_loss: 2.2719 - val_accuracy: 0.5292 - lr: 0.1000\n",
      "Epoch 24/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9233 - accuracy: 0.5846\n",
      "Epoch 24: val_loss improved from 2.02640 to 2.02483, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.9233 - accuracy: 0.5846 - val_loss: 2.0248 - val_accuracy: 0.5718 - lr: 0.1000\n",
      "Epoch 25/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9016 - accuracy: 0.5904\n",
      "Epoch 25: val_loss did not improve from 2.02483\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.9016 - accuracy: 0.5904 - val_loss: 2.1068 - val_accuracy: 0.5576 - lr: 0.1000\n",
      "Epoch 26/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8750 - accuracy: 0.5951\n",
      "Epoch 26: val_loss did not improve from 2.02483\n",
      "351/351 [==============================] - 42s 121ms/step - loss: 1.8750 - accuracy: 0.5951 - val_loss: 2.0356 - val_accuracy: 0.5806 - lr: 0.1000\n",
      "Epoch 27/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8474 - accuracy: 0.6073\n",
      "Epoch 27: val_loss improved from 2.02483 to 1.99553, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.8474 - accuracy: 0.6073 - val_loss: 1.9955 - val_accuracy: 0.5856 - lr: 0.1000\n",
      "Epoch 28/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8220 - accuracy: 0.6100\n",
      "Epoch 28: val_loss improved from 1.99553 to 1.94856, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.8220 - accuracy: 0.6100 - val_loss: 1.9486 - val_accuracy: 0.5912 - lr: 0.1000\n",
      "Epoch 29/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7936 - accuracy: 0.6158\n",
      "Epoch 29: val_loss did not improve from 1.94856\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.7936 - accuracy: 0.6158 - val_loss: 2.0075 - val_accuracy: 0.5812 - lr: 0.1000\n",
      "Epoch 30/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7671 - accuracy: 0.6234\n",
      "Epoch 30: val_loss did not improve from 1.94856\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.7671 - accuracy: 0.6234 - val_loss: 2.0446 - val_accuracy: 0.5668 - lr: 0.1000\n",
      "Epoch 31/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7482 - accuracy: 0.6290\n",
      "Epoch 31: val_loss improved from 1.94856 to 1.87349, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.7482 - accuracy: 0.6290 - val_loss: 1.8735 - val_accuracy: 0.6018 - lr: 0.1000\n",
      "Epoch 32/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7291 - accuracy: 0.6303\n",
      "Epoch 32: val_loss did not improve from 1.87349\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.7291 - accuracy: 0.6303 - val_loss: 2.0367 - val_accuracy: 0.5750 - lr: 0.1000\n",
      "Epoch 33/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7036 - accuracy: 0.6363\n",
      "Epoch 33: val_loss did not improve from 1.87349\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.7036 - accuracy: 0.6363 - val_loss: 2.0447 - val_accuracy: 0.5762 - lr: 0.1000\n",
      "Epoch 34/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6896 - accuracy: 0.6419\n",
      "Epoch 34: val_loss did not improve from 1.87349\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 1.6896 - accuracy: 0.6419 - val_loss: 1.9291 - val_accuracy: 0.5998 - lr: 0.1000\n",
      "Epoch 35/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6688 - accuracy: 0.6444\n",
      "Epoch 35: val_loss did not improve from 1.87349\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.6688 - accuracy: 0.6444 - val_loss: 1.8993 - val_accuracy: 0.6044 - lr: 0.1000\n",
      "Epoch 36/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6564 - accuracy: 0.6495\n",
      "Epoch 36: val_loss improved from 1.87349 to 1.87148, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.6564 - accuracy: 0.6495 - val_loss: 1.8715 - val_accuracy: 0.6158 - lr: 0.1000\n",
      "Epoch 37/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6455 - accuracy: 0.6494\n",
      "Epoch 37: val_loss improved from 1.87148 to 1.86214, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.6455 - accuracy: 0.6494 - val_loss: 1.8621 - val_accuracy: 0.6242 - lr: 0.1000\n",
      "Epoch 38/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6237 - accuracy: 0.6597\n",
      "Epoch 38: val_loss did not improve from 1.86214\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 1.6237 - accuracy: 0.6597 - val_loss: 1.8708 - val_accuracy: 0.6230 - lr: 0.1000\n",
      "Epoch 39/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6201 - accuracy: 0.6585\n",
      "Epoch 39: val_loss improved from 1.86214 to 1.81509, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.6201 - accuracy: 0.6585 - val_loss: 1.8151 - val_accuracy: 0.6304 - lr: 0.1000\n",
      "Epoch 40/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5976 - accuracy: 0.6634\n",
      "Epoch 40: val_loss improved from 1.81509 to 1.80868, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.5976 - accuracy: 0.6634 - val_loss: 1.8087 - val_accuracy: 0.6350 - lr: 0.1000\n",
      "Epoch 41/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5937 - accuracy: 0.6642\n",
      "Epoch 41: val_loss improved from 1.80868 to 1.78814, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.5937 - accuracy: 0.6642 - val_loss: 1.7881 - val_accuracy: 0.6392 - lr: 0.1000\n",
      "Epoch 42/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5744 - accuracy: 0.6693\n",
      "Epoch 42: val_loss did not improve from 1.78814\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.5744 - accuracy: 0.6693 - val_loss: 1.8483 - val_accuracy: 0.6220 - lr: 0.1000\n",
      "Epoch 43/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5530 - accuracy: 0.6762\n",
      "Epoch 43: val_loss improved from 1.78814 to 1.71422, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.5530 - accuracy: 0.6762 - val_loss: 1.7142 - val_accuracy: 0.6516 - lr: 0.1000\n",
      "Epoch 44/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5394 - accuracy: 0.6781\n",
      "Epoch 44: val_loss did not improve from 1.71422\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.5394 - accuracy: 0.6781 - val_loss: 1.8310 - val_accuracy: 0.6364 - lr: 0.1000\n",
      "Epoch 45/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5290 - accuracy: 0.6820\n",
      "Epoch 45: val_loss did not improve from 1.71422\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.5290 - accuracy: 0.6820 - val_loss: 1.8120 - val_accuracy: 0.6322 - lr: 0.1000\n",
      "Epoch 46/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5150 - accuracy: 0.6851\n",
      "Epoch 46: val_loss did not improve from 1.71422\n",
      "351/351 [==============================] - 42s 121ms/step - loss: 1.5150 - accuracy: 0.6851 - val_loss: 1.8218 - val_accuracy: 0.6368 - lr: 0.1000\n",
      "Epoch 47/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5024 - accuracy: 0.6901\n",
      "Epoch 47: val_loss did not improve from 1.71422\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 1.5024 - accuracy: 0.6901 - val_loss: 1.7830 - val_accuracy: 0.6404 - lr: 0.1000\n",
      "Epoch 48/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5009 - accuracy: 0.6876\n",
      "Epoch 48: val_loss did not improve from 1.71422\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.5009 - accuracy: 0.6876 - val_loss: 1.8190 - val_accuracy: 0.6378 - lr: 0.1000\n",
      "Epoch 49/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4874 - accuracy: 0.6926\n",
      "Epoch 49: val_loss did not improve from 1.71422\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.4874 - accuracy: 0.6926 - val_loss: 1.8615 - val_accuracy: 0.6258 - lr: 0.1000\n",
      "Epoch 50/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4652 - accuracy: 0.6962\n",
      "Epoch 50: val_loss did not improve from 1.71422\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.4652 - accuracy: 0.6962 - val_loss: 1.8906 - val_accuracy: 0.6258 - lr: 0.1000\n",
      "Epoch 51/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4593 - accuracy: 0.6984\n",
      "Epoch 51: val_loss improved from 1.71422 to 1.71087, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 1.4593 - accuracy: 0.6984 - val_loss: 1.7109 - val_accuracy: 0.6604 - lr: 0.1000\n",
      "Epoch 52/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4491 - accuracy: 0.7033\n",
      "Epoch 52: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.4491 - accuracy: 0.7033 - val_loss: 1.7386 - val_accuracy: 0.6592 - lr: 0.1000\n",
      "Epoch 53/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4556 - accuracy: 0.6997\n",
      "Epoch 53: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.4556 - accuracy: 0.6997 - val_loss: 1.9192 - val_accuracy: 0.6184 - lr: 0.1000\n",
      "Epoch 54/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4437 - accuracy: 0.7054\n",
      "Epoch 54: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 1.4437 - accuracy: 0.7054 - val_loss: 1.7346 - val_accuracy: 0.6570 - lr: 0.1000\n",
      "Epoch 55/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4202 - accuracy: 0.7112\n",
      "Epoch 55: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.4202 - accuracy: 0.7112 - val_loss: 1.7403 - val_accuracy: 0.6594 - lr: 0.1000\n",
      "Epoch 56/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4150 - accuracy: 0.7125\n",
      "Epoch 56: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 1.4150 - accuracy: 0.7125 - val_loss: 1.7502 - val_accuracy: 0.6566 - lr: 0.1000\n",
      "Epoch 57/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3985 - accuracy: 0.7170\n",
      "Epoch 57: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 1.3985 - accuracy: 0.7170 - val_loss: 1.7216 - val_accuracy: 0.6608 - lr: 0.1000\n",
      "Epoch 58/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3877 - accuracy: 0.7179\n",
      "Epoch 58: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.3877 - accuracy: 0.7179 - val_loss: 1.7125 - val_accuracy: 0.6680 - lr: 0.1000\n",
      "Epoch 59/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3846 - accuracy: 0.7211\n",
      "Epoch 59: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.3846 - accuracy: 0.7211 - val_loss: 1.7753 - val_accuracy: 0.6506 - lr: 0.1000\n",
      "Epoch 60/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3858 - accuracy: 0.7188\n",
      "Epoch 60: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.3858 - accuracy: 0.7188 - val_loss: 1.7550 - val_accuracy: 0.6612 - lr: 0.1000\n",
      "Epoch 61/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3770 - accuracy: 0.7185\n",
      "Epoch 61: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.3770 - accuracy: 0.7185 - val_loss: 1.7267 - val_accuracy: 0.6656 - lr: 0.1000\n",
      "Epoch 62/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3604 - accuracy: 0.7244\n",
      "Epoch 62: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.3604 - accuracy: 0.7244 - val_loss: 1.7281 - val_accuracy: 0.6674 - lr: 0.1000\n",
      "Epoch 63/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3484 - accuracy: 0.7261\n",
      "Epoch 63: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.3484 - accuracy: 0.7261 - val_loss: 1.8421 - val_accuracy: 0.6388 - lr: 0.1000\n",
      "Epoch 64/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3440 - accuracy: 0.7286\n",
      "Epoch 64: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.3440 - accuracy: 0.7286 - val_loss: 1.7386 - val_accuracy: 0.6662 - lr: 0.1000\n",
      "Epoch 65/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3316 - accuracy: 0.7321\n",
      "Epoch 65: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.3316 - accuracy: 0.7321 - val_loss: 1.7379 - val_accuracy: 0.6682 - lr: 0.1000\n",
      "Epoch 66/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3287 - accuracy: 0.7328\n",
      "Epoch 66: val_loss did not improve from 1.71087\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.3287 - accuracy: 0.7328 - val_loss: 1.7379 - val_accuracy: 0.6704 - lr: 0.1000\n",
      "Epoch 67/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3188 - accuracy: 0.7357\n",
      "Epoch 67: val_loss improved from 1.71087 to 1.68586, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 1.3188 - accuracy: 0.7357 - val_loss: 1.6859 - val_accuracy: 0.6716 - lr: 0.1000\n",
      "Epoch 68/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3246 - accuracy: 0.7335\n",
      "Epoch 68: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.3246 - accuracy: 0.7335 - val_loss: 1.7492 - val_accuracy: 0.6604 - lr: 0.1000\n",
      "Epoch 69/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3010 - accuracy: 0.7406\n",
      "Epoch 69: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 1.3010 - accuracy: 0.7406 - val_loss: 1.7021 - val_accuracy: 0.6710 - lr: 0.1000\n",
      "Epoch 70/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2967 - accuracy: 0.7415\n",
      "Epoch 70: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.2967 - accuracy: 0.7415 - val_loss: 1.7091 - val_accuracy: 0.6732 - lr: 0.1000\n",
      "Epoch 71/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3026 - accuracy: 0.7409\n",
      "Epoch 71: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 1.3026 - accuracy: 0.7409 - val_loss: 1.7480 - val_accuracy: 0.6622 - lr: 0.1000\n",
      "Epoch 72/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2836 - accuracy: 0.7423\n",
      "Epoch 72: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.2836 - accuracy: 0.7423 - val_loss: 1.7097 - val_accuracy: 0.6742 - lr: 0.1000\n",
      "Epoch 73/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2705 - accuracy: 0.7497\n",
      "Epoch 73: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.2705 - accuracy: 0.7497 - val_loss: 1.7049 - val_accuracy: 0.6776 - lr: 0.1000\n",
      "Epoch 74/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2611 - accuracy: 0.7490\n",
      "Epoch 74: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.2611 - accuracy: 0.7490 - val_loss: 1.7360 - val_accuracy: 0.6720 - lr: 0.1000\n",
      "Epoch 75/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2608 - accuracy: 0.7492\n",
      "Epoch 75: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.2608 - accuracy: 0.7492 - val_loss: 1.6990 - val_accuracy: 0.6846 - lr: 0.1000\n",
      "Epoch 76/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2804 - accuracy: 0.7437\n",
      "Epoch 76: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.2804 - accuracy: 0.7437 - val_loss: 1.7154 - val_accuracy: 0.6690 - lr: 0.1000\n",
      "Epoch 77/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2635 - accuracy: 0.7512\n",
      "Epoch 77: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.2635 - accuracy: 0.7512 - val_loss: 1.7138 - val_accuracy: 0.6732 - lr: 0.1000\n",
      "Epoch 78/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2456 - accuracy: 0.7539\n",
      "Epoch 78: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.2456 - accuracy: 0.7539 - val_loss: 1.7500 - val_accuracy: 0.6734 - lr: 0.1000\n",
      "Epoch 79/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2504 - accuracy: 0.7529\n",
      "Epoch 79: val_loss did not improve from 1.68586\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.2504 - accuracy: 0.7529 - val_loss: 1.7110 - val_accuracy: 0.6718 - lr: 0.1000\n",
      "Epoch 80/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2392 - accuracy: 0.7561\n",
      "Epoch 80: val_loss improved from 1.68586 to 1.66573, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.2392 - accuracy: 0.7561 - val_loss: 1.6657 - val_accuracy: 0.6862 - lr: 0.1000\n",
      "Epoch 81/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2208 - accuracy: 0.7613\n",
      "Epoch 81: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 1.2208 - accuracy: 0.7613 - val_loss: 1.7268 - val_accuracy: 0.6714 - lr: 0.1000\n",
      "Epoch 82/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2255 - accuracy: 0.7602\n",
      "Epoch 82: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.2255 - accuracy: 0.7602 - val_loss: 1.7306 - val_accuracy: 0.6718 - lr: 0.1000\n",
      "Epoch 83/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2153 - accuracy: 0.7611\n",
      "Epoch 83: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.2153 - accuracy: 0.7611 - val_loss: 1.6765 - val_accuracy: 0.6792 - lr: 0.1000\n",
      "Epoch 84/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2025 - accuracy: 0.7680\n",
      "Epoch 84: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.2025 - accuracy: 0.7680 - val_loss: 1.7123 - val_accuracy: 0.6748 - lr: 0.1000\n",
      "Epoch 85/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2108 - accuracy: 0.7619\n",
      "Epoch 85: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.2108 - accuracy: 0.7619 - val_loss: 1.7376 - val_accuracy: 0.6740 - lr: 0.1000\n",
      "Epoch 86/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2013 - accuracy: 0.7662\n",
      "Epoch 86: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.2013 - accuracy: 0.7662 - val_loss: 1.6920 - val_accuracy: 0.6828 - lr: 0.1000\n",
      "Epoch 87/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1970 - accuracy: 0.7658\n",
      "Epoch 87: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.1970 - accuracy: 0.7658 - val_loss: 1.6979 - val_accuracy: 0.6824 - lr: 0.1000\n",
      "Epoch 88/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1944 - accuracy: 0.7652\n",
      "Epoch 88: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 1.1944 - accuracy: 0.7652 - val_loss: 1.7698 - val_accuracy: 0.6726 - lr: 0.1000\n",
      "Epoch 89/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1850 - accuracy: 0.7723\n",
      "Epoch 89: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.1850 - accuracy: 0.7723 - val_loss: 1.7246 - val_accuracy: 0.6754 - lr: 0.1000\n",
      "Epoch 90/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1847 - accuracy: 0.7701\n",
      "Epoch 90: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.1847 - accuracy: 0.7701 - val_loss: 1.7984 - val_accuracy: 0.6630 - lr: 0.1000\n",
      "Epoch 91/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1714 - accuracy: 0.7725\n",
      "Epoch 91: val_loss did not improve from 1.66573\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 1.1714 - accuracy: 0.7725 - val_loss: 1.6658 - val_accuracy: 0.6938 - lr: 0.1000\n",
      "Epoch 92/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1579 - accuracy: 0.7765\n",
      "Epoch 92: val_loss improved from 1.66573 to 1.61629, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.1579 - accuracy: 0.7765 - val_loss: 1.6163 - val_accuracy: 0.6916 - lr: 0.1000\n",
      "Epoch 93/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1707 - accuracy: 0.7710\n",
      "Epoch 93: val_loss did not improve from 1.61629\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.1707 - accuracy: 0.7710 - val_loss: 1.6453 - val_accuracy: 0.6934 - lr: 0.1000\n",
      "Epoch 94/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1674 - accuracy: 0.7732\n",
      "Epoch 94: val_loss did not improve from 1.61629\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.1674 - accuracy: 0.7732 - val_loss: 1.7506 - val_accuracy: 0.6732 - lr: 0.1000\n",
      "Epoch 95/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1632 - accuracy: 0.7732\n",
      "Epoch 95: val_loss did not improve from 1.61629\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 1.1632 - accuracy: 0.7732 - val_loss: 1.7215 - val_accuracy: 0.6782 - lr: 0.1000\n",
      "Epoch 96/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1444 - accuracy: 0.7787\n",
      "Epoch 96: val_loss did not improve from 1.61629\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 1.1444 - accuracy: 0.7787 - val_loss: 1.6288 - val_accuracy: 0.6878 - lr: 0.1000\n",
      "Epoch 97/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1479 - accuracy: 0.7793\n",
      "Epoch 97: val_loss improved from 1.61629 to 1.61051, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 46s 132ms/step - loss: 1.1479 - accuracy: 0.7793 - val_loss: 1.6105 - val_accuracy: 0.6994 - lr: 0.1000\n",
      "Epoch 98/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1347 - accuracy: 0.7818\n",
      "Epoch 98: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.1347 - accuracy: 0.7818 - val_loss: 1.6975 - val_accuracy: 0.6810 - lr: 0.1000\n",
      "Epoch 99/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1391 - accuracy: 0.7811\n",
      "Epoch 99: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.1391 - accuracy: 0.7811 - val_loss: 1.6912 - val_accuracy: 0.6844 - lr: 0.1000\n",
      "Epoch 100/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1337 - accuracy: 0.7828\n",
      "Epoch 100: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.1337 - accuracy: 0.7828 - val_loss: 1.6993 - val_accuracy: 0.6842 - lr: 0.1000\n",
      "Epoch 101/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1188 - accuracy: 0.7862\n",
      "Epoch 101: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 1.1188 - accuracy: 0.7862 - val_loss: 1.6644 - val_accuracy: 0.6928 - lr: 0.1000\n",
      "Epoch 102/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1143 - accuracy: 0.7876\n",
      "Epoch 102: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 1.1143 - accuracy: 0.7876 - val_loss: 1.6596 - val_accuracy: 0.6934 - lr: 0.1000\n",
      "Epoch 103/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1071 - accuracy: 0.7909\n",
      "Epoch 103: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 1.1071 - accuracy: 0.7909 - val_loss: 1.7075 - val_accuracy: 0.6786 - lr: 0.1000\n",
      "Epoch 104/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1104 - accuracy: 0.7891\n",
      "Epoch 104: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.1104 - accuracy: 0.7891 - val_loss: 1.6879 - val_accuracy: 0.6904 - lr: 0.1000\n",
      "Epoch 105/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0914 - accuracy: 0.7929\n",
      "Epoch 105: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.0914 - accuracy: 0.7929 - val_loss: 1.6424 - val_accuracy: 0.6958 - lr: 0.1000\n",
      "Epoch 106/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0852 - accuracy: 0.7961\n",
      "Epoch 106: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.0852 - accuracy: 0.7961 - val_loss: 1.6827 - val_accuracy: 0.6976 - lr: 0.1000\n",
      "Epoch 107/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0951 - accuracy: 0.7897\n",
      "Epoch 107: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.0951 - accuracy: 0.7897 - val_loss: 1.6836 - val_accuracy: 0.6890 - lr: 0.1000\n",
      "Epoch 108/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1015 - accuracy: 0.7907\n",
      "Epoch 108: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 43s 124ms/step - loss: 1.1015 - accuracy: 0.7907 - val_loss: 1.6360 - val_accuracy: 0.6932 - lr: 0.1000\n",
      "Epoch 109/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0923 - accuracy: 0.7899\n",
      "Epoch 109: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 1.0923 - accuracy: 0.7899 - val_loss: 1.6264 - val_accuracy: 0.6996 - lr: 0.1000\n",
      "Epoch 110/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0832 - accuracy: 0.7938\n",
      "Epoch 110: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.0832 - accuracy: 0.7938 - val_loss: 1.6822 - val_accuracy: 0.6886 - lr: 0.1000\n",
      "Epoch 111/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0634 - accuracy: 0.8013\n",
      "Epoch 111: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 1.0634 - accuracy: 0.8013 - val_loss: 1.6997 - val_accuracy: 0.6820 - lr: 0.1000\n",
      "Epoch 112/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0799 - accuracy: 0.7947\n",
      "Epoch 112: val_loss did not improve from 1.61051\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.0799 - accuracy: 0.7947 - val_loss: 1.7413 - val_accuracy: 0.6874 - lr: 0.1000\n",
      "Epoch 113/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0695 - accuracy: 0.7976\n",
      "Epoch 113: val_loss improved from 1.61051 to 1.60244, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.0695 - accuracy: 0.7976 - val_loss: 1.6024 - val_accuracy: 0.6950 - lr: 0.1000\n",
      "Epoch 114/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0622 - accuracy: 0.8011\n",
      "Epoch 114: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.0622 - accuracy: 0.8011 - val_loss: 1.6586 - val_accuracy: 0.6960 - lr: 0.1000\n",
      "Epoch 115/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0610 - accuracy: 0.8016\n",
      "Epoch 115: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.0610 - accuracy: 0.8016 - val_loss: 1.7030 - val_accuracy: 0.6904 - lr: 0.1000\n",
      "Epoch 116/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0540 - accuracy: 0.8008\n",
      "Epoch 116: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 1.0540 - accuracy: 0.8008 - val_loss: 1.6533 - val_accuracy: 0.6962 - lr: 0.1000\n",
      "Epoch 117/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0765 - accuracy: 0.7957\n",
      "Epoch 117: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.0765 - accuracy: 0.7957 - val_loss: 1.6119 - val_accuracy: 0.7062 - lr: 0.1000\n",
      "Epoch 118/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0313 - accuracy: 0.8064\n",
      "Epoch 118: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 1.0313 - accuracy: 0.8064 - val_loss: 1.6900 - val_accuracy: 0.6870 - lr: 0.1000\n",
      "Epoch 119/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0378 - accuracy: 0.8077\n",
      "Epoch 119: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.0378 - accuracy: 0.8077 - val_loss: 1.6597 - val_accuracy: 0.6944 - lr: 0.1000\n",
      "Epoch 120/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0497 - accuracy: 0.8010\n",
      "Epoch 120: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 1.0497 - accuracy: 0.8010 - val_loss: 1.7327 - val_accuracy: 0.6818 - lr: 0.1000\n",
      "Epoch 121/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0614 - accuracy: 0.7991\n",
      "Epoch 121: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 1.0614 - accuracy: 0.7991 - val_loss: 1.6758 - val_accuracy: 0.6918 - lr: 0.1000\n",
      "Epoch 122/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0297 - accuracy: 0.8046\n",
      "Epoch 122: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 1.0297 - accuracy: 0.8046 - val_loss: 1.6581 - val_accuracy: 0.7022 - lr: 0.1000\n",
      "Epoch 123/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0329 - accuracy: 0.8055\n",
      "Epoch 123: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.0329 - accuracy: 0.8055 - val_loss: 1.6337 - val_accuracy: 0.7022 - lr: 0.1000\n",
      "Epoch 124/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0163 - accuracy: 0.8090\n",
      "Epoch 124: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.0163 - accuracy: 0.8090 - val_loss: 1.6923 - val_accuracy: 0.6922 - lr: 0.1000\n",
      "Epoch 125/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0159 - accuracy: 0.8120\n",
      "Epoch 125: val_loss did not improve from 1.60244\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.0159 - accuracy: 0.8120 - val_loss: 1.6287 - val_accuracy: 0.7024 - lr: 0.1000\n",
      "Epoch 126/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0144 - accuracy: 0.8120\n",
      "Epoch 126: val_loss improved from 1.60244 to 1.58711, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 43s 124ms/step - loss: 1.0144 - accuracy: 0.8120 - val_loss: 1.5871 - val_accuracy: 0.7170 - lr: 0.1000\n",
      "Epoch 127/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0023 - accuracy: 0.8155\n",
      "Epoch 127: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 1.0023 - accuracy: 0.8155 - val_loss: 1.6336 - val_accuracy: 0.7040 - lr: 0.1000\n",
      "Epoch 128/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0042 - accuracy: 0.8142\n",
      "Epoch 128: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 1.0042 - accuracy: 0.8142 - val_loss: 1.6809 - val_accuracy: 0.6918 - lr: 0.1000\n",
      "Epoch 129/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0104 - accuracy: 0.8096\n",
      "Epoch 129: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 124ms/step - loss: 1.0104 - accuracy: 0.8096 - val_loss: 1.6727 - val_accuracy: 0.6918 - lr: 0.1000\n",
      "Epoch 130/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9993 - accuracy: 0.8139\n",
      "Epoch 130: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.9993 - accuracy: 0.8139 - val_loss: 1.6751 - val_accuracy: 0.7002 - lr: 0.1000\n",
      "Epoch 131/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0031 - accuracy: 0.8147\n",
      "Epoch 131: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 47s 133ms/step - loss: 1.0031 - accuracy: 0.8147 - val_loss: 1.6340 - val_accuracy: 0.7028 - lr: 0.1000\n",
      "Epoch 132/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9842 - accuracy: 0.8174\n",
      "Epoch 132: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.9842 - accuracy: 0.8174 - val_loss: 1.6206 - val_accuracy: 0.7102 - lr: 0.1000\n",
      "Epoch 133/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9865 - accuracy: 0.8186\n",
      "Epoch 133: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.9865 - accuracy: 0.8186 - val_loss: 1.6397 - val_accuracy: 0.7030 - lr: 0.1000\n",
      "Epoch 134/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9727 - accuracy: 0.8226\n",
      "Epoch 134: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.9727 - accuracy: 0.8226 - val_loss: 1.6152 - val_accuracy: 0.7052 - lr: 0.1000\n",
      "Epoch 135/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9884 - accuracy: 0.8191\n",
      "Epoch 135: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.9884 - accuracy: 0.8191 - val_loss: 1.7496 - val_accuracy: 0.6886 - lr: 0.1000\n",
      "Epoch 136/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9819 - accuracy: 0.8179\n",
      "Epoch 136: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.9819 - accuracy: 0.8179 - val_loss: 1.6556 - val_accuracy: 0.6990 - lr: 0.1000\n",
      "Epoch 137/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9919 - accuracy: 0.8157\n",
      "Epoch 137: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.9919 - accuracy: 0.8157 - val_loss: 1.6800 - val_accuracy: 0.7016 - lr: 0.1000\n",
      "Epoch 138/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9846 - accuracy: 0.8168\n",
      "Epoch 138: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 0.9846 - accuracy: 0.8168 - val_loss: 1.6907 - val_accuracy: 0.7012 - lr: 0.1000\n",
      "Epoch 139/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9777 - accuracy: 0.8194\n",
      "Epoch 139: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.9777 - accuracy: 0.8194 - val_loss: 1.6588 - val_accuracy: 0.7060 - lr: 0.1000\n",
      "Epoch 140/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9489 - accuracy: 0.8280\n",
      "Epoch 140: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 124ms/step - loss: 0.9489 - accuracy: 0.8280 - val_loss: 1.6868 - val_accuracy: 0.7062 - lr: 0.1000\n",
      "Epoch 141/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9503 - accuracy: 0.8271\n",
      "Epoch 141: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 47s 132ms/step - loss: 0.9503 - accuracy: 0.8271 - val_loss: 1.6705 - val_accuracy: 0.6992 - lr: 0.1000\n",
      "Epoch 142/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9742 - accuracy: 0.8198\n",
      "Epoch 142: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 124ms/step - loss: 0.9742 - accuracy: 0.8198 - val_loss: 1.6465 - val_accuracy: 0.6958 - lr: 0.1000\n",
      "Epoch 143/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9601 - accuracy: 0.8244\n",
      "Epoch 143: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.9601 - accuracy: 0.8244 - val_loss: 1.6277 - val_accuracy: 0.7120 - lr: 0.1000\n",
      "Epoch 144/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9429 - accuracy: 0.8286\n",
      "Epoch 144: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.9429 - accuracy: 0.8286 - val_loss: 1.6796 - val_accuracy: 0.6974 - lr: 0.1000\n",
      "Epoch 145/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9499 - accuracy: 0.8263\n",
      "Epoch 145: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 124ms/step - loss: 0.9499 - accuracy: 0.8263 - val_loss: 1.6584 - val_accuracy: 0.7066 - lr: 0.1000\n",
      "Epoch 146/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9408 - accuracy: 0.8292\n",
      "Epoch 146: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.9408 - accuracy: 0.8292 - val_loss: 1.6425 - val_accuracy: 0.7068 - lr: 0.1000\n",
      "Epoch 147/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9357 - accuracy: 0.8299\n",
      "Epoch 147: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.9357 - accuracy: 0.8299 - val_loss: 1.7017 - val_accuracy: 0.6926 - lr: 0.1000\n",
      "Epoch 148/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9475 - accuracy: 0.8259\n",
      "Epoch 148: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.9475 - accuracy: 0.8259 - val_loss: 1.6476 - val_accuracy: 0.6996 - lr: 0.1000\n",
      "Epoch 149/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9349 - accuracy: 0.8295\n",
      "Epoch 149: val_loss did not improve from 1.58711\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.9349 - accuracy: 0.8295 - val_loss: 1.6148 - val_accuracy: 0.7090 - lr: 0.1000\n",
      "Epoch 150/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9388 - accuracy: 0.8287\n",
      "Epoch 150: val_loss improved from 1.58711 to 1.58259, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.9388 - accuracy: 0.8287 - val_loss: 1.5826 - val_accuracy: 0.7106 - lr: 0.1000\n",
      "Epoch 151/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9294 - accuracy: 0.8317\n",
      "Epoch 151: val_loss did not improve from 1.58259\n",
      "351/351 [==============================] - 47s 132ms/step - loss: 0.9294 - accuracy: 0.8317 - val_loss: 1.6619 - val_accuracy: 0.7056 - lr: 0.1000\n",
      "Epoch 152/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9295 - accuracy: 0.8327\n",
      "Epoch 152: val_loss did not improve from 1.58259\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.9295 - accuracy: 0.8327 - val_loss: 1.6118 - val_accuracy: 0.7088 - lr: 0.1000\n",
      "Epoch 153/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9161 - accuracy: 0.8353\n",
      "Epoch 153: val_loss did not improve from 1.58259\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.9161 - accuracy: 0.8353 - val_loss: 1.6206 - val_accuracy: 0.7082 - lr: 0.1000\n",
      "Epoch 154/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9322 - accuracy: 0.8291\n",
      "Epoch 154: val_loss did not improve from 1.58259\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.9322 - accuracy: 0.8291 - val_loss: 1.6748 - val_accuracy: 0.7054 - lr: 0.1000\n",
      "Epoch 155/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9345 - accuracy: 0.8291\n",
      "Epoch 155: val_loss improved from 1.58259 to 1.57800, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.9345 - accuracy: 0.8291 - val_loss: 1.5780 - val_accuracy: 0.7140 - lr: 0.1000\n",
      "Epoch 156/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9279 - accuracy: 0.8295\n",
      "Epoch 156: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.9279 - accuracy: 0.8295 - val_loss: 1.6907 - val_accuracy: 0.7016 - lr: 0.1000\n",
      "Epoch 157/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9271 - accuracy: 0.8330\n",
      "Epoch 157: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.9271 - accuracy: 0.8330 - val_loss: 1.6662 - val_accuracy: 0.6960 - lr: 0.1000\n",
      "Epoch 158/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9094 - accuracy: 0.8354\n",
      "Epoch 158: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.9094 - accuracy: 0.8354 - val_loss: 1.6318 - val_accuracy: 0.7104 - lr: 0.1000\n",
      "Epoch 159/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9115 - accuracy: 0.8357\n",
      "Epoch 159: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.9115 - accuracy: 0.8357 - val_loss: 1.6308 - val_accuracy: 0.7016 - lr: 0.1000\n",
      "Epoch 160/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9110 - accuracy: 0.8368\n",
      "Epoch 160: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.9110 - accuracy: 0.8368 - val_loss: 1.6673 - val_accuracy: 0.7040 - lr: 0.1000\n",
      "Epoch 161/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8996 - accuracy: 0.8392\n",
      "Epoch 161: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.8996 - accuracy: 0.8392 - val_loss: 1.6749 - val_accuracy: 0.7032 - lr: 0.1000\n",
      "Epoch 162/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9065 - accuracy: 0.8364\n",
      "Epoch 162: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.9065 - accuracy: 0.8364 - val_loss: 1.6338 - val_accuracy: 0.7126 - lr: 0.1000\n",
      "Epoch 163/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8858 - accuracy: 0.8430\n",
      "Epoch 163: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.8858 - accuracy: 0.8430 - val_loss: 1.6117 - val_accuracy: 0.7108 - lr: 0.1000\n",
      "Epoch 164/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8925 - accuracy: 0.8413\n",
      "Epoch 164: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8925 - accuracy: 0.8413 - val_loss: 1.6167 - val_accuracy: 0.7130 - lr: 0.1000\n",
      "Epoch 165/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8853 - accuracy: 0.8417\n",
      "Epoch 165: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8853 - accuracy: 0.8417 - val_loss: 1.6181 - val_accuracy: 0.7082 - lr: 0.1000\n",
      "Epoch 166/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8919 - accuracy: 0.8416\n",
      "Epoch 166: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.8919 - accuracy: 0.8416 - val_loss: 1.7357 - val_accuracy: 0.6910 - lr: 0.1000\n",
      "Epoch 167/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.8360\n",
      "Epoch 167: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.9004 - accuracy: 0.8360 - val_loss: 1.7021 - val_accuracy: 0.6978 - lr: 0.1000\n",
      "Epoch 168/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9036 - accuracy: 0.8354\n",
      "Epoch 168: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.9036 - accuracy: 0.8354 - val_loss: 1.6785 - val_accuracy: 0.7052 - lr: 0.1000\n",
      "Epoch 169/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8808 - accuracy: 0.8438\n",
      "Epoch 169: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.8808 - accuracy: 0.8438 - val_loss: 1.6498 - val_accuracy: 0.7106 - lr: 0.1000\n",
      "Epoch 170/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8808 - accuracy: 0.8426\n",
      "Epoch 170: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8808 - accuracy: 0.8426 - val_loss: 1.6572 - val_accuracy: 0.7104 - lr: 0.1000\n",
      "Epoch 171/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8821 - accuracy: 0.8405\n",
      "Epoch 171: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.8821 - accuracy: 0.8405 - val_loss: 1.6559 - val_accuracy: 0.7124 - lr: 0.1000\n",
      "Epoch 172/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8856 - accuracy: 0.8412\n",
      "Epoch 172: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8856 - accuracy: 0.8412 - val_loss: 1.6763 - val_accuracy: 0.7120 - lr: 0.1000\n",
      "Epoch 173/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8872 - accuracy: 0.8405\n",
      "Epoch 173: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8872 - accuracy: 0.8405 - val_loss: 1.6406 - val_accuracy: 0.7150 - lr: 0.1000\n",
      "Epoch 174/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8793 - accuracy: 0.8409\n",
      "Epoch 174: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8793 - accuracy: 0.8409 - val_loss: 1.6190 - val_accuracy: 0.7126 - lr: 0.1000\n",
      "Epoch 175/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8671 - accuracy: 0.8464\n",
      "Epoch 175: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8671 - accuracy: 0.8464 - val_loss: 1.5923 - val_accuracy: 0.7172 - lr: 0.1000\n",
      "Epoch 176/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8640 - accuracy: 0.8485\n",
      "Epoch 176: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8640 - accuracy: 0.8485 - val_loss: 1.6232 - val_accuracy: 0.7092 - lr: 0.1000\n",
      "Epoch 177/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8613 - accuracy: 0.8473\n",
      "Epoch 177: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 42s 118ms/step - loss: 0.8613 - accuracy: 0.8473 - val_loss: 1.6015 - val_accuracy: 0.7148 - lr: 0.1000\n",
      "Epoch 178/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8669 - accuracy: 0.8442\n",
      "Epoch 178: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8669 - accuracy: 0.8442 - val_loss: 1.6828 - val_accuracy: 0.7014 - lr: 0.1000\n",
      "Epoch 179/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8725 - accuracy: 0.8431\n",
      "Epoch 179: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8725 - accuracy: 0.8431 - val_loss: 1.7228 - val_accuracy: 0.7050 - lr: 0.1000\n",
      "Epoch 180/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8407 - accuracy: 0.8533\n",
      "Epoch 180: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 42s 118ms/step - loss: 0.8407 - accuracy: 0.8533 - val_loss: 1.6477 - val_accuracy: 0.7158 - lr: 0.1000\n",
      "Epoch 181/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8526 - accuracy: 0.8482\n",
      "Epoch 181: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.8526 - accuracy: 0.8482 - val_loss: 1.6440 - val_accuracy: 0.7100 - lr: 0.1000\n",
      "Epoch 182/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8633 - accuracy: 0.8447\n",
      "Epoch 182: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8633 - accuracy: 0.8447 - val_loss: 1.6779 - val_accuracy: 0.7118 - lr: 0.1000\n",
      "Epoch 183/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8672 - accuracy: 0.8446\n",
      "Epoch 183: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8672 - accuracy: 0.8446 - val_loss: 1.6397 - val_accuracy: 0.7114 - lr: 0.1000\n",
      "Epoch 184/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8570 - accuracy: 0.8476\n",
      "Epoch 184: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 42s 118ms/step - loss: 0.8570 - accuracy: 0.8476 - val_loss: 1.6263 - val_accuracy: 0.7178 - lr: 0.1000\n",
      "Epoch 185/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8611 - accuracy: 0.8455\n",
      "Epoch 185: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8611 - accuracy: 0.8455 - val_loss: 1.7115 - val_accuracy: 0.7048 - lr: 0.1000\n",
      "Epoch 186/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8346 - accuracy: 0.8545\n",
      "Epoch 186: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.8346 - accuracy: 0.8545 - val_loss: 1.6294 - val_accuracy: 0.7152 - lr: 0.1000\n",
      "Epoch 187/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8330 - accuracy: 0.8541\n",
      "Epoch 187: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8330 - accuracy: 0.8541 - val_loss: 1.6803 - val_accuracy: 0.7026 - lr: 0.1000\n",
      "Epoch 188/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8510 - accuracy: 0.8477\n",
      "Epoch 188: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.8510 - accuracy: 0.8477 - val_loss: 1.6135 - val_accuracy: 0.7200 - lr: 0.1000\n",
      "Epoch 189/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8271 - accuracy: 0.8561\n",
      "Epoch 189: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8271 - accuracy: 0.8561 - val_loss: 1.6751 - val_accuracy: 0.6990 - lr: 0.1000\n",
      "Epoch 190/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8426 - accuracy: 0.8510\n",
      "Epoch 190: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8426 - accuracy: 0.8510 - val_loss: 1.6571 - val_accuracy: 0.7134 - lr: 0.1000\n",
      "Epoch 191/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8399 - accuracy: 0.8514\n",
      "Epoch 191: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 0.8399 - accuracy: 0.8514 - val_loss: 1.6343 - val_accuracy: 0.7154 - lr: 0.1000\n",
      "Epoch 192/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8427 - accuracy: 0.8510\n",
      "Epoch 192: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.8427 - accuracy: 0.8510 - val_loss: 1.6792 - val_accuracy: 0.7154 - lr: 0.1000\n",
      "Epoch 193/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8351 - accuracy: 0.8518\n",
      "Epoch 193: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8351 - accuracy: 0.8518 - val_loss: 1.7026 - val_accuracy: 0.7064 - lr: 0.1000\n",
      "Epoch 194/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8330 - accuracy: 0.8531\n",
      "Epoch 194: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.8330 - accuracy: 0.8531 - val_loss: 1.6491 - val_accuracy: 0.7124 - lr: 0.1000\n",
      "Epoch 195/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8366 - accuracy: 0.8496\n",
      "Epoch 195: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8366 - accuracy: 0.8496 - val_loss: 1.6569 - val_accuracy: 0.7088 - lr: 0.1000\n",
      "Epoch 196/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8276 - accuracy: 0.8551\n",
      "Epoch 196: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.8276 - accuracy: 0.8551 - val_loss: 1.6514 - val_accuracy: 0.7142 - lr: 0.1000\n",
      "Epoch 197/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8256 - accuracy: 0.8534\n",
      "Epoch 197: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8256 - accuracy: 0.8534 - val_loss: 1.5919 - val_accuracy: 0.7148 - lr: 0.1000\n",
      "Epoch 198/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8137 - accuracy: 0.8573\n",
      "Epoch 198: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.8137 - accuracy: 0.8573 - val_loss: 1.6043 - val_accuracy: 0.7128 - lr: 0.1000\n",
      "Epoch 199/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8146 - accuracy: 0.8573\n",
      "Epoch 199: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8146 - accuracy: 0.8573 - val_loss: 1.6788 - val_accuracy: 0.7146 - lr: 0.1000\n",
      "Epoch 200/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8168 - accuracy: 0.8558\n",
      "Epoch 200: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8168 - accuracy: 0.8558 - val_loss: 1.6549 - val_accuracy: 0.7174 - lr: 0.1000\n",
      "Epoch 201/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8064 - accuracy: 0.8609\n",
      "Epoch 201: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.8064 - accuracy: 0.8609 - val_loss: 1.6287 - val_accuracy: 0.7202 - lr: 0.1000\n",
      "Epoch 202/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8017 - accuracy: 0.8615\n",
      "Epoch 202: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8017 - accuracy: 0.8615 - val_loss: 1.6320 - val_accuracy: 0.7140 - lr: 0.1000\n",
      "Epoch 203/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7920 - accuracy: 0.8641\n",
      "Epoch 203: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.7920 - accuracy: 0.8641 - val_loss: 1.6571 - val_accuracy: 0.7094 - lr: 0.1000\n",
      "Epoch 204/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8012 - accuracy: 0.8595\n",
      "Epoch 204: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8012 - accuracy: 0.8595 - val_loss: 1.6407 - val_accuracy: 0.7082 - lr: 0.1000\n",
      "Epoch 205/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8095 - accuracy: 0.8582\n",
      "Epoch 205: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.8095 - accuracy: 0.8582 - val_loss: 1.6258 - val_accuracy: 0.7174 - lr: 0.1000\n",
      "Epoch 206/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8062 - accuracy: 0.8605\n",
      "Epoch 206: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.8062 - accuracy: 0.8605 - val_loss: 1.5993 - val_accuracy: 0.7170 - lr: 0.1000\n",
      "Epoch 207/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7954 - accuracy: 0.8619\n",
      "Epoch 207: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.7954 - accuracy: 0.8619 - val_loss: 1.6429 - val_accuracy: 0.7190 - lr: 0.1000\n",
      "Epoch 208/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7953 - accuracy: 0.8616\n",
      "Epoch 208: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7953 - accuracy: 0.8616 - val_loss: 1.6693 - val_accuracy: 0.7146 - lr: 0.1000\n",
      "Epoch 209/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7841 - accuracy: 0.8641\n",
      "Epoch 209: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.7841 - accuracy: 0.8641 - val_loss: 1.6749 - val_accuracy: 0.7096 - lr: 0.1000\n",
      "Epoch 210/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7885 - accuracy: 0.8629\n",
      "Epoch 210: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.7885 - accuracy: 0.8629 - val_loss: 1.6403 - val_accuracy: 0.7136 - lr: 0.1000\n",
      "Epoch 211/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7864 - accuracy: 0.8634\n",
      "Epoch 211: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.7864 - accuracy: 0.8634 - val_loss: 1.6769 - val_accuracy: 0.7060 - lr: 0.1000\n",
      "Epoch 212/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7941 - accuracy: 0.8635\n",
      "Epoch 212: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.7941 - accuracy: 0.8635 - val_loss: 1.6826 - val_accuracy: 0.7080 - lr: 0.1000\n",
      "Epoch 213/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7808 - accuracy: 0.8658\n",
      "Epoch 213: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 127ms/step - loss: 0.7808 - accuracy: 0.8658 - val_loss: 1.6328 - val_accuracy: 0.7208 - lr: 0.1000\n",
      "Epoch 214/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7862 - accuracy: 0.8641\n",
      "Epoch 214: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.7862 - accuracy: 0.8641 - val_loss: 1.7135 - val_accuracy: 0.7120 - lr: 0.1000\n",
      "Epoch 215/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7980 - accuracy: 0.8603\n",
      "Epoch 215: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7980 - accuracy: 0.8603 - val_loss: 1.6193 - val_accuracy: 0.7260 - lr: 0.1000\n",
      "Epoch 216/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7944 - accuracy: 0.8606\n",
      "Epoch 216: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.7944 - accuracy: 0.8606 - val_loss: 1.6612 - val_accuracy: 0.7176 - lr: 0.1000\n",
      "Epoch 217/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7840 - accuracy: 0.8633\n",
      "Epoch 217: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7840 - accuracy: 0.8633 - val_loss: 1.6955 - val_accuracy: 0.7098 - lr: 0.1000\n",
      "Epoch 218/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7868 - accuracy: 0.8627\n",
      "Epoch 218: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.7868 - accuracy: 0.8627 - val_loss: 1.6716 - val_accuracy: 0.7104 - lr: 0.1000\n",
      "Epoch 219/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7709 - accuracy: 0.8659\n",
      "Epoch 219: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.7709 - accuracy: 0.8659 - val_loss: 1.6827 - val_accuracy: 0.7152 - lr: 0.1000\n",
      "Epoch 220/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7666 - accuracy: 0.8680\n",
      "Epoch 220: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.7666 - accuracy: 0.8680 - val_loss: 1.6629 - val_accuracy: 0.7182 - lr: 0.1000\n",
      "Epoch 221/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7649 - accuracy: 0.8704\n",
      "Epoch 221: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 46s 129ms/step - loss: 0.7649 - accuracy: 0.8704 - val_loss: 1.6421 - val_accuracy: 0.7182 - lr: 0.1000\n",
      "Epoch 222/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7763 - accuracy: 0.8658\n",
      "Epoch 222: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.7763 - accuracy: 0.8658 - val_loss: 1.6535 - val_accuracy: 0.7114 - lr: 0.1000\n",
      "Epoch 223/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7815 - accuracy: 0.8631\n",
      "Epoch 223: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7815 - accuracy: 0.8631 - val_loss: 1.6152 - val_accuracy: 0.7176 - lr: 0.1000\n",
      "Epoch 224/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7616 - accuracy: 0.8714\n",
      "Epoch 224: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.7616 - accuracy: 0.8714 - val_loss: 1.6225 - val_accuracy: 0.7192 - lr: 0.1000\n",
      "Epoch 225/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7648 - accuracy: 0.8679\n",
      "Epoch 225: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.7648 - accuracy: 0.8679 - val_loss: 1.7180 - val_accuracy: 0.7064 - lr: 0.1000\n",
      "Epoch 226/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7672 - accuracy: 0.8677\n",
      "Epoch 226: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.7672 - accuracy: 0.8677 - val_loss: 1.6140 - val_accuracy: 0.7242 - lr: 0.1000\n",
      "Epoch 227/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7698 - accuracy: 0.8666\n",
      "Epoch 227: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.7698 - accuracy: 0.8666 - val_loss: 1.6221 - val_accuracy: 0.7200 - lr: 0.1000\n",
      "Epoch 228/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7700 - accuracy: 0.8660\n",
      "Epoch 228: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 0.7700 - accuracy: 0.8660 - val_loss: 1.6426 - val_accuracy: 0.7142 - lr: 0.1000\n",
      "Epoch 229/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7491 - accuracy: 0.8736\n",
      "Epoch 229: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.7491 - accuracy: 0.8736 - val_loss: 1.6526 - val_accuracy: 0.7224 - lr: 0.1000\n",
      "Epoch 230/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7550 - accuracy: 0.8702\n",
      "Epoch 230: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.7550 - accuracy: 0.8702 - val_loss: 1.6477 - val_accuracy: 0.7160 - lr: 0.1000\n",
      "Epoch 231/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7485 - accuracy: 0.8723\n",
      "Epoch 231: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 46s 129ms/step - loss: 0.7485 - accuracy: 0.8723 - val_loss: 1.6042 - val_accuracy: 0.7192 - lr: 0.1000\n",
      "Epoch 232/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7655 - accuracy: 0.8657\n",
      "Epoch 232: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.7655 - accuracy: 0.8657 - val_loss: 1.6133 - val_accuracy: 0.7176 - lr: 0.1000\n",
      "Epoch 233/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.8718\n",
      "Epoch 233: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7501 - accuracy: 0.8718 - val_loss: 1.6010 - val_accuracy: 0.7250 - lr: 0.1000\n",
      "Epoch 234/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7566 - accuracy: 0.8708\n",
      "Epoch 234: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.7566 - accuracy: 0.8708 - val_loss: 1.6591 - val_accuracy: 0.7126 - lr: 0.1000\n",
      "Epoch 235/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7566 - accuracy: 0.8687\n",
      "Epoch 235: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7566 - accuracy: 0.8687 - val_loss: 1.6117 - val_accuracy: 0.7222 - lr: 0.1000\n",
      "Epoch 236/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7461 - accuracy: 0.8719\n",
      "Epoch 236: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.7461 - accuracy: 0.8719 - val_loss: 1.5968 - val_accuracy: 0.7194 - lr: 0.1000\n",
      "Epoch 237/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7544 - accuracy: 0.8707\n",
      "Epoch 237: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.7544 - accuracy: 0.8707 - val_loss: 1.7086 - val_accuracy: 0.7060 - lr: 0.1000\n",
      "Epoch 238/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7416 - accuracy: 0.8727\n",
      "Epoch 238: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.7416 - accuracy: 0.8727 - val_loss: 1.6645 - val_accuracy: 0.7128 - lr: 0.1000\n",
      "Epoch 239/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7378 - accuracy: 0.8748\n",
      "Epoch 239: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7378 - accuracy: 0.8748 - val_loss: 1.6178 - val_accuracy: 0.7244 - lr: 0.1000\n",
      "Epoch 240/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7480 - accuracy: 0.8716\n",
      "Epoch 240: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7480 - accuracy: 0.8716 - val_loss: 1.6812 - val_accuracy: 0.7082 - lr: 0.1000\n",
      "Epoch 241/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7370 - accuracy: 0.8754\n",
      "Epoch 241: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 46s 129ms/step - loss: 0.7370 - accuracy: 0.8754 - val_loss: 1.6513 - val_accuracy: 0.7160 - lr: 0.1000\n",
      "Epoch 242/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7511 - accuracy: 0.8697\n",
      "Epoch 242: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.7511 - accuracy: 0.8697 - val_loss: 1.6454 - val_accuracy: 0.7174 - lr: 0.1000\n",
      "Epoch 243/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7313 - accuracy: 0.8763\n",
      "Epoch 243: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.7313 - accuracy: 0.8763 - val_loss: 1.6540 - val_accuracy: 0.7208 - lr: 0.1000\n",
      "Epoch 244/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7252 - accuracy: 0.8776\n",
      "Epoch 244: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7252 - accuracy: 0.8776 - val_loss: 1.6561 - val_accuracy: 0.7174 - lr: 0.1000\n",
      "Epoch 245/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7433 - accuracy: 0.8703\n",
      "Epoch 245: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.7433 - accuracy: 0.8703 - val_loss: 1.6755 - val_accuracy: 0.7118 - lr: 0.1000\n",
      "Epoch 246/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.8788\n",
      "Epoch 246: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.7226 - accuracy: 0.8788 - val_loss: 1.6071 - val_accuracy: 0.7164 - lr: 0.1000\n",
      "Epoch 247/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7211 - accuracy: 0.8793\n",
      "Epoch 247: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.7211 - accuracy: 0.8793 - val_loss: 1.6551 - val_accuracy: 0.7134 - lr: 0.1000\n",
      "Epoch 248/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7157 - accuracy: 0.8793\n",
      "Epoch 248: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.7157 - accuracy: 0.8793 - val_loss: 1.6331 - val_accuracy: 0.7216 - lr: 0.1000\n",
      "Epoch 249/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7167 - accuracy: 0.8801\n",
      "Epoch 249: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 0.7167 - accuracy: 0.8801 - val_loss: 1.6025 - val_accuracy: 0.7282 - lr: 0.1000\n",
      "Epoch 250/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7291 - accuracy: 0.8761\n",
      "Epoch 250: val_loss did not improve from 1.57800\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.7291 - accuracy: 0.8761 - val_loss: 1.6552 - val_accuracy: 0.7158 - lr: 0.1000\n",
      "Epoch 251/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6806 - accuracy: 0.8898\n",
      "Epoch 251: val_loss improved from 1.57800 to 1.54409, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 46s 132ms/step - loss: 0.6806 - accuracy: 0.8898 - val_loss: 1.5441 - val_accuracy: 0.7374 - lr: 0.0100\n",
      "Epoch 252/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.9131\n",
      "Epoch 252: val_loss improved from 1.54409 to 1.52022, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.6148 - accuracy: 0.9131 - val_loss: 1.5202 - val_accuracy: 0.7386 - lr: 0.0100\n",
      "Epoch 253/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6182 - accuracy: 0.9110\n",
      "Epoch 253: val_loss improved from 1.52022 to 1.51294, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.6182 - accuracy: 0.9110 - val_loss: 1.5129 - val_accuracy: 0.7418 - lr: 0.0100\n",
      "Epoch 254/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6144 - accuracy: 0.9137\n",
      "Epoch 254: val_loss improved from 1.51294 to 1.51260, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.6144 - accuracy: 0.9137 - val_loss: 1.5126 - val_accuracy: 0.7448 - lr: 0.0100\n",
      "Epoch 255/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6066 - accuracy: 0.9153\n",
      "Epoch 255: val_loss improved from 1.51260 to 1.50250, saving model to model_resnet110SD_c100_best_2.hdf5\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.6066 - accuracy: 0.9153 - val_loss: 1.5025 - val_accuracy: 0.7466 - lr: 0.0100\n",
      "Epoch 256/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6013 - accuracy: 0.9167\n",
      "Epoch 256: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.6013 - accuracy: 0.9167 - val_loss: 1.5069 - val_accuracy: 0.7474 - lr: 0.0100\n",
      "Epoch 257/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5918 - accuracy: 0.9210\n",
      "Epoch 257: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.5918 - accuracy: 0.9210 - val_loss: 1.5119 - val_accuracy: 0.7450 - lr: 0.0100\n",
      "Epoch 258/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5911 - accuracy: 0.9206\n",
      "Epoch 258: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5911 - accuracy: 0.9206 - val_loss: 1.5095 - val_accuracy: 0.7452 - lr: 0.0100\n",
      "Epoch 259/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5796 - accuracy: 0.9224\n",
      "Epoch 259: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 42s 121ms/step - loss: 0.5796 - accuracy: 0.9224 - val_loss: 1.5155 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 260/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5737 - accuracy: 0.9254\n",
      "Epoch 260: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.5737 - accuracy: 0.9254 - val_loss: 1.5110 - val_accuracy: 0.7458 - lr: 0.0100\n",
      "Epoch 261/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.9233\n",
      "Epoch 261: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 47s 133ms/step - loss: 0.5778 - accuracy: 0.9233 - val_loss: 1.5153 - val_accuracy: 0.7436 - lr: 0.0100\n",
      "Epoch 262/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5826 - accuracy: 0.9237\n",
      "Epoch 262: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5826 - accuracy: 0.9237 - val_loss: 1.5095 - val_accuracy: 0.7480 - lr: 0.0100\n",
      "Epoch 263/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5549 - accuracy: 0.9325\n",
      "Epoch 263: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5549 - accuracy: 0.9325 - val_loss: 1.5154 - val_accuracy: 0.7450 - lr: 0.0100\n",
      "Epoch 264/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5627 - accuracy: 0.9295\n",
      "Epoch 264: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.5627 - accuracy: 0.9295 - val_loss: 1.5083 - val_accuracy: 0.7460 - lr: 0.0100\n",
      "Epoch 265/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5633 - accuracy: 0.9287\n",
      "Epoch 265: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.5633 - accuracy: 0.9287 - val_loss: 1.5092 - val_accuracy: 0.7472 - lr: 0.0100\n",
      "Epoch 266/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5556 - accuracy: 0.9316\n",
      "Epoch 266: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 0.5556 - accuracy: 0.9316 - val_loss: 1.5057 - val_accuracy: 0.7492 - lr: 0.0100\n",
      "Epoch 267/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5612 - accuracy: 0.9292\n",
      "Epoch 267: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.5612 - accuracy: 0.9292 - val_loss: 1.5178 - val_accuracy: 0.7472 - lr: 0.0100\n",
      "Epoch 268/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.9273\n",
      "Epoch 268: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5619 - accuracy: 0.9273 - val_loss: 1.5165 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 269/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.9313\n",
      "Epoch 269: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 0.5558 - accuracy: 0.9313 - val_loss: 1.5085 - val_accuracy: 0.7470 - lr: 0.0100\n",
      "Epoch 270/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5559 - accuracy: 0.9296\n",
      "Epoch 270: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5559 - accuracy: 0.9296 - val_loss: 1.5207 - val_accuracy: 0.7464 - lr: 0.0100\n",
      "Epoch 271/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5401 - accuracy: 0.9368\n",
      "Epoch 271: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 132ms/step - loss: 0.5401 - accuracy: 0.9368 - val_loss: 1.5218 - val_accuracy: 0.7456 - lr: 0.0100\n",
      "Epoch 272/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5421 - accuracy: 0.9346\n",
      "Epoch 272: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.5421 - accuracy: 0.9346 - val_loss: 1.5170 - val_accuracy: 0.7472 - lr: 0.0100\n",
      "Epoch 273/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5414 - accuracy: 0.9349\n",
      "Epoch 273: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.5414 - accuracy: 0.9349 - val_loss: 1.5177 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 274/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5520 - accuracy: 0.9303\n",
      "Epoch 274: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.5520 - accuracy: 0.9303 - val_loss: 1.5215 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 275/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5458 - accuracy: 0.9323\n",
      "Epoch 275: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.5458 - accuracy: 0.9323 - val_loss: 1.5220 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 276/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.9368\n",
      "Epoch 276: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 42s 121ms/step - loss: 0.5356 - accuracy: 0.9368 - val_loss: 1.5207 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 277/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5238 - accuracy: 0.9410\n",
      "Epoch 277: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.5238 - accuracy: 0.9410 - val_loss: 1.5174 - val_accuracy: 0.7518 - lr: 0.0100\n",
      "Epoch 278/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5431 - accuracy: 0.9343\n",
      "Epoch 278: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 42s 121ms/step - loss: 0.5431 - accuracy: 0.9343 - val_loss: 1.5278 - val_accuracy: 0.7492 - lr: 0.0100\n",
      "Epoch 279/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5324 - accuracy: 0.9362\n",
      "Epoch 279: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.5324 - accuracy: 0.9362 - val_loss: 1.5257 - val_accuracy: 0.7494 - lr: 0.0100\n",
      "Epoch 280/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5296 - accuracy: 0.9374\n",
      "Epoch 280: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5296 - accuracy: 0.9374 - val_loss: 1.5354 - val_accuracy: 0.7476 - lr: 0.0100\n",
      "Epoch 281/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.9397\n",
      "Epoch 281: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.5274 - accuracy: 0.9397 - val_loss: 1.5196 - val_accuracy: 0.7478 - lr: 0.0100\n",
      "Epoch 282/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.9411\n",
      "Epoch 282: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.5223 - accuracy: 0.9411 - val_loss: 1.5231 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 283/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.9384\n",
      "Epoch 283: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.5243 - accuracy: 0.9384 - val_loss: 1.5233 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 284/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.9402\n",
      "Epoch 284: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5232 - accuracy: 0.9402 - val_loss: 1.5236 - val_accuracy: 0.7516 - lr: 0.0100\n",
      "Epoch 285/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5181 - accuracy: 0.9398\n",
      "Epoch 285: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5181 - accuracy: 0.9398 - val_loss: 1.5205 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 286/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.9380\n",
      "Epoch 286: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 128ms/step - loss: 0.5291 - accuracy: 0.9380 - val_loss: 1.5132 - val_accuracy: 0.7514 - lr: 0.0100\n",
      "Epoch 287/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5161 - accuracy: 0.9404\n",
      "Epoch 287: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 129ms/step - loss: 0.5161 - accuracy: 0.9404 - val_loss: 1.5145 - val_accuracy: 0.7478 - lr: 0.0100\n",
      "Epoch 288/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5124 - accuracy: 0.9427\n",
      "Epoch 288: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 127ms/step - loss: 0.5124 - accuracy: 0.9427 - val_loss: 1.5185 - val_accuracy: 0.7490 - lr: 0.0100\n",
      "Epoch 289/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.9381\n",
      "Epoch 289: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.5239 - accuracy: 0.9381 - val_loss: 1.5271 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 290/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5147 - accuracy: 0.9425\n",
      "Epoch 290: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5147 - accuracy: 0.9425 - val_loss: 1.5231 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 291/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5217 - accuracy: 0.9392\n",
      "Epoch 291: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.5217 - accuracy: 0.9392 - val_loss: 1.5327 - val_accuracy: 0.7476 - lr: 0.0100\n",
      "Epoch 292/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5249 - accuracy: 0.9378\n",
      "Epoch 292: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.5249 - accuracy: 0.9378 - val_loss: 1.5344 - val_accuracy: 0.7492 - lr: 0.0100\n",
      "Epoch 293/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5089 - accuracy: 0.9423\n",
      "Epoch 293: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5089 - accuracy: 0.9423 - val_loss: 1.5305 - val_accuracy: 0.7528 - lr: 0.0100\n",
      "Epoch 294/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5058 - accuracy: 0.9442\n",
      "Epoch 294: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 130ms/step - loss: 0.5058 - accuracy: 0.9442 - val_loss: 1.5279 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 295/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.9352\n",
      "Epoch 295: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5306 - accuracy: 0.9352 - val_loss: 1.5318 - val_accuracy: 0.7520 - lr: 0.0100\n",
      "Epoch 296/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4986 - accuracy: 0.9459\n",
      "Epoch 296: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.4986 - accuracy: 0.9459 - val_loss: 1.5191 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 297/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.9441\n",
      "Epoch 297: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5064 - accuracy: 0.9441 - val_loss: 1.5206 - val_accuracy: 0.7512 - lr: 0.0100\n",
      "Epoch 298/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.9398\n",
      "Epoch 298: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5189 - accuracy: 0.9398 - val_loss: 1.5428 - val_accuracy: 0.7492 - lr: 0.0100\n",
      "Epoch 299/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5164 - accuracy: 0.9402\n",
      "Epoch 299: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 0.5164 - accuracy: 0.9402 - val_loss: 1.5277 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 300/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.9447\n",
      "Epoch 300: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.5025 - accuracy: 0.9447 - val_loss: 1.5425 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 1/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5612 - accuracy: 0.9248\n",
      "Epoch 1: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 41s 91ms/step - loss: 0.5612 - accuracy: 0.9248 - val_loss: 1.5414 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 2/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5510 - accuracy: 0.9286\n",
      "Epoch 2: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5510 - accuracy: 0.9286 - val_loss: 1.5449 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 3/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5720 - accuracy: 0.9239\n",
      "Epoch 3: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5720 - accuracy: 0.9239 - val_loss: 1.5426 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 4/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.9279\n",
      "Epoch 4: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5582 - accuracy: 0.9279 - val_loss: 1.5446 - val_accuracy: 0.7512 - lr: 0.0100\n",
      "Epoch 5/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5578 - accuracy: 0.9271\n",
      "Epoch 5: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5578 - accuracy: 0.9271 - val_loss: 1.5382 - val_accuracy: 0.7520 - lr: 0.0100\n",
      "Epoch 6/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.9354\n",
      "Epoch 6: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5313 - accuracy: 0.9354 - val_loss: 1.5425 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 7/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5382 - accuracy: 0.9323\n",
      "Epoch 7: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5382 - accuracy: 0.9323 - val_loss: 1.5385 - val_accuracy: 0.7510 - lr: 0.0100\n",
      "Epoch 8/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5574 - accuracy: 0.9267\n",
      "Epoch 8: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5574 - accuracy: 0.9267 - val_loss: 1.5406 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 9/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.9333\n",
      "Epoch 9: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5331 - accuracy: 0.9333 - val_loss: 1.5425 - val_accuracy: 0.7510 - lr: 0.0100\n",
      "Epoch 10/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.9354\n",
      "Epoch 10: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 87ms/step - loss: 0.5300 - accuracy: 0.9354 - val_loss: 1.5424 - val_accuracy: 0.7510 - lr: 0.0100\n",
      "Epoch 11/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5418 - accuracy: 0.9319\n",
      "Epoch 11: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.5418 - accuracy: 0.9319 - val_loss: 1.5428 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 12/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5538 - accuracy: 0.9286\n",
      "Epoch 12: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5538 - accuracy: 0.9286 - val_loss: 1.5444 - val_accuracy: 0.7514 - lr: 0.0100\n",
      "Epoch 13/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5508 - accuracy: 0.9302\n",
      "Epoch 13: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5508 - accuracy: 0.9302 - val_loss: 1.5455 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 14/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5511 - accuracy: 0.9297\n",
      "Epoch 14: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5511 - accuracy: 0.9297 - val_loss: 1.5489 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 15/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5433 - accuracy: 0.9322\n",
      "Epoch 15: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5433 - accuracy: 0.9322 - val_loss: 1.5482 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 16/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5412 - accuracy: 0.9311\n",
      "Epoch 16: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 31s 90ms/step - loss: 0.5412 - accuracy: 0.9311 - val_loss: 1.5456 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 17/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5441 - accuracy: 0.9320\n",
      "Epoch 17: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5441 - accuracy: 0.9320 - val_loss: 1.5471 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 18/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5472 - accuracy: 0.9296\n",
      "Epoch 18: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5472 - accuracy: 0.9296 - val_loss: 1.5501 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 19/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.9310\n",
      "Epoch 19: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5424 - accuracy: 0.9310 - val_loss: 1.5500 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 20/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.9353\n",
      "Epoch 20: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5299 - accuracy: 0.9353 - val_loss: 1.5475 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 21/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5393 - accuracy: 0.9346\n",
      "Epoch 21: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.5393 - accuracy: 0.9346 - val_loss: 1.5536 - val_accuracy: 0.7494 - lr: 0.0100\n",
      "Epoch 22/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5443 - accuracy: 0.9316\n",
      "Epoch 22: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5443 - accuracy: 0.9316 - val_loss: 1.5506 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 23/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.9343\n",
      "Epoch 23: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5343 - accuracy: 0.9343 - val_loss: 1.5469 - val_accuracy: 0.7516 - lr: 0.0100\n",
      "Epoch 24/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.9328\n",
      "Epoch 24: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5356 - accuracy: 0.9328 - val_loss: 1.5506 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 25/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.9358\n",
      "Epoch 25: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5248 - accuracy: 0.9358 - val_loss: 1.5521 - val_accuracy: 0.7512 - lr: 0.0100\n",
      "Epoch 26/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5308 - accuracy: 0.9350\n",
      "Epoch 26: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5308 - accuracy: 0.9350 - val_loss: 1.5505 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 27/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.9313\n",
      "Epoch 27: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5416 - accuracy: 0.9313 - val_loss: 1.5482 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 28/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5178 - accuracy: 0.9382\n",
      "Epoch 28: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 31s 90ms/step - loss: 0.5178 - accuracy: 0.9382 - val_loss: 1.5493 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 29/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5369 - accuracy: 0.9329\n",
      "Epoch 29: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5369 - accuracy: 0.9329 - val_loss: 1.5517 - val_accuracy: 0.7522 - lr: 0.0100\n",
      "Epoch 30/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5443 - accuracy: 0.9298\n",
      "Epoch 30: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5443 - accuracy: 0.9298 - val_loss: 1.5525 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 31/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5373 - accuracy: 0.9333\n",
      "Epoch 31: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 29s 81ms/step - loss: 0.5373 - accuracy: 0.9333 - val_loss: 1.5506 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 32/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.9370\n",
      "Epoch 32: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5245 - accuracy: 0.9370 - val_loss: 1.5535 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 33/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5420 - accuracy: 0.9307\n",
      "Epoch 33: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5420 - accuracy: 0.9307 - val_loss: 1.5554 - val_accuracy: 0.7516 - lr: 0.0100\n",
      "Epoch 34/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.9272\n",
      "Epoch 34: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 84ms/step - loss: 0.5541 - accuracy: 0.9272 - val_loss: 1.5547 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 35/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5309 - accuracy: 0.9343\n",
      "Epoch 35: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5309 - accuracy: 0.9343 - val_loss: 1.5570 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 36/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.9349\n",
      "Epoch 36: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5278 - accuracy: 0.9349 - val_loss: 1.5562 - val_accuracy: 0.7510 - lr: 0.0100\n",
      "Epoch 37/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5339 - accuracy: 0.9338\n",
      "Epoch 37: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5339 - accuracy: 0.9338 - val_loss: 1.5531 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 38/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5280 - accuracy: 0.9353\n",
      "Epoch 38: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5280 - accuracy: 0.9353 - val_loss: 1.5539 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 39/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.9342\n",
      "Epoch 39: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5350 - accuracy: 0.9342 - val_loss: 1.5563 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 40/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5413 - accuracy: 0.9314\n",
      "Epoch 40: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 34s 96ms/step - loss: 0.5413 - accuracy: 0.9314 - val_loss: 1.5569 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 41/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5388 - accuracy: 0.9324\n",
      "Epoch 41: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.5388 - accuracy: 0.9324 - val_loss: 1.5582 - val_accuracy: 0.7516 - lr: 0.0100\n",
      "Epoch 42/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5318 - accuracy: 0.9331\n",
      "Epoch 42: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5318 - accuracy: 0.9331 - val_loss: 1.5588 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 43/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.9344\n",
      "Epoch 43: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5335 - accuracy: 0.9344 - val_loss: 1.5572 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 44/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.9290\n",
      "Epoch 44: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5447 - accuracy: 0.9290 - val_loss: 1.5585 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 45/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.9320\n",
      "Epoch 45: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.5356 - accuracy: 0.9320 - val_loss: 1.5626 - val_accuracy: 0.7494 - lr: 0.0100\n",
      "Epoch 46/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5330 - accuracy: 0.9338\n",
      "Epoch 46: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 84ms/step - loss: 0.5330 - accuracy: 0.9338 - val_loss: 1.5582 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 47/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5449 - accuracy: 0.9300\n",
      "Epoch 47: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5449 - accuracy: 0.9300 - val_loss: 1.5624 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 48/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.9353\n",
      "Epoch 48: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 84ms/step - loss: 0.5283 - accuracy: 0.9353 - val_loss: 1.5582 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 49/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.9352\n",
      "Epoch 49: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5228 - accuracy: 0.9352 - val_loss: 1.5598 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 50/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5321 - accuracy: 0.9339\n",
      "Epoch 50: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5321 - accuracy: 0.9339 - val_loss: 1.5635 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 51/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.9352\n",
      "Epoch 51: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.5293 - accuracy: 0.9352 - val_loss: 1.5562 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 52/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5287 - accuracy: 0.9348\n",
      "Epoch 52: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5287 - accuracy: 0.9348 - val_loss: 1.5624 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 53/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5376 - accuracy: 0.9331\n",
      "Epoch 53: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5376 - accuracy: 0.9331 - val_loss: 1.5608 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 54/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5260 - accuracy: 0.9358\n",
      "Epoch 54: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5260 - accuracy: 0.9358 - val_loss: 1.5586 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 55/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.9358\n",
      "Epoch 55: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5251 - accuracy: 0.9358 - val_loss: 1.5606 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 56/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.9340\n",
      "Epoch 56: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5288 - accuracy: 0.9340 - val_loss: 1.5613 - val_accuracy: 0.7514 - lr: 0.0100\n",
      "Epoch 57/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5207 - accuracy: 0.9378\n",
      "Epoch 57: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 29s 82ms/step - loss: 0.5207 - accuracy: 0.9378 - val_loss: 1.5606 - val_accuracy: 0.7510 - lr: 0.0100\n",
      "Epoch 58/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.9389\n",
      "Epoch 58: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5165 - accuracy: 0.9389 - val_loss: 1.5612 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 59/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5233 - accuracy: 0.9362\n",
      "Epoch 59: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5233 - accuracy: 0.9362 - val_loss: 1.5638 - val_accuracy: 0.7512 - lr: 0.0100\n",
      "Epoch 60/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5194 - accuracy: 0.9388\n",
      "Epoch 60: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5194 - accuracy: 0.9388 - val_loss: 1.5646 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 61/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.9371\n",
      "Epoch 61: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.5212 - accuracy: 0.9371 - val_loss: 1.5651 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 62/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.9360\n",
      "Epoch 62: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5228 - accuracy: 0.9360 - val_loss: 1.5651 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 63/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.9364\n",
      "Epoch 63: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5283 - accuracy: 0.9364 - val_loss: 1.5652 - val_accuracy: 0.7510 - lr: 0.0100\n",
      "Epoch 64/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5310 - accuracy: 0.9351\n",
      "Epoch 64: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5310 - accuracy: 0.9351 - val_loss: 1.5672 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 65/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5421 - accuracy: 0.9309\n",
      "Epoch 65: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5421 - accuracy: 0.9309 - val_loss: 1.5641 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 66/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5231 - accuracy: 0.9357\n",
      "Epoch 66: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5231 - accuracy: 0.9357 - val_loss: 1.5621 - val_accuracy: 0.7480 - lr: 0.0100\n",
      "Epoch 67/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5216 - accuracy: 0.9370\n",
      "Epoch 67: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5216 - accuracy: 0.9370 - val_loss: 1.5602 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 68/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.9340\n",
      "Epoch 68: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5295 - accuracy: 0.9340 - val_loss: 1.5641 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 69/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5303 - accuracy: 0.9340\n",
      "Epoch 69: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5303 - accuracy: 0.9340 - val_loss: 1.5605 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 70/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.9316\n",
      "Epoch 70: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5398 - accuracy: 0.9316 - val_loss: 1.5665 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 71/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.9347\n",
      "Epoch 71: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 34s 96ms/step - loss: 0.5306 - accuracy: 0.9347 - val_loss: 1.5612 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 72/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.9321\n",
      "Epoch 72: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5364 - accuracy: 0.9321 - val_loss: 1.5629 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 73/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.9341\n",
      "Epoch 73: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5255 - accuracy: 0.9341 - val_loss: 1.5676 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 74/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5348 - accuracy: 0.9339\n",
      "Epoch 74: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5348 - accuracy: 0.9339 - val_loss: 1.5675 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 75/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.9346\n",
      "Epoch 75: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.5259 - accuracy: 0.9346 - val_loss: 1.5684 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 76/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.9354\n",
      "Epoch 76: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5298 - accuracy: 0.9354 - val_loss: 1.5650 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 77/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.9324\n",
      "Epoch 77: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5392 - accuracy: 0.9324 - val_loss: 1.5676 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 78/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5233 - accuracy: 0.9361\n",
      "Epoch 78: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5233 - accuracy: 0.9361 - val_loss: 1.5673 - val_accuracy: 0.7476 - lr: 0.0010\n",
      "Epoch 79/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5359 - accuracy: 0.9325\n",
      "Epoch 79: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5359 - accuracy: 0.9325 - val_loss: 1.5668 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 80/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.9319\n",
      "Epoch 80: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5392 - accuracy: 0.9319 - val_loss: 1.5645 - val_accuracy: 0.7478 - lr: 0.0010\n",
      "Epoch 81/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.9346\n",
      "Epoch 81: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.5305 - accuracy: 0.9346 - val_loss: 1.5688 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 82/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.9386\n",
      "Epoch 82: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.5174 - accuracy: 0.9386 - val_loss: 1.5663 - val_accuracy: 0.7480 - lr: 0.0010\n",
      "Epoch 83/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5391 - accuracy: 0.9316\n",
      "Epoch 83: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5391 - accuracy: 0.9316 - val_loss: 1.5678 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 84/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.9338\n",
      "Epoch 84: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5288 - accuracy: 0.9338 - val_loss: 1.5695 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 85/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.9319\n",
      "Epoch 85: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.5379 - accuracy: 0.9319 - val_loss: 1.5688 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 86/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.9390\n",
      "Epoch 86: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.5156 - accuracy: 0.9390 - val_loss: 1.5645 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 87/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5355 - accuracy: 0.9324\n",
      "Epoch 87: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5355 - accuracy: 0.9324 - val_loss: 1.5683 - val_accuracy: 0.7480 - lr: 0.0010\n",
      "Epoch 88/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5197 - accuracy: 0.9384\n",
      "Epoch 88: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5197 - accuracy: 0.9384 - val_loss: 1.5654 - val_accuracy: 0.7480 - lr: 0.0010\n",
      "Epoch 89/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5250 - accuracy: 0.9349\n",
      "Epoch 89: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 84ms/step - loss: 0.5250 - accuracy: 0.9349 - val_loss: 1.5665 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 90/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.9363\n",
      "Epoch 90: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.5213 - accuracy: 0.9363 - val_loss: 1.5622 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 91/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5303 - accuracy: 0.9350\n",
      "Epoch 91: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.5303 - accuracy: 0.9350 - val_loss: 1.5667 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 92/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5387 - accuracy: 0.9327\n",
      "Epoch 92: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5387 - accuracy: 0.9327 - val_loss: 1.5634 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 93/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.9396\n",
      "Epoch 93: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5125 - accuracy: 0.9396 - val_loss: 1.5616 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 94/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.9399\n",
      "Epoch 94: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.5141 - accuracy: 0.9399 - val_loss: 1.5650 - val_accuracy: 0.7480 - lr: 0.0010\n",
      "Epoch 95/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.9312\n",
      "Epoch 95: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5429 - accuracy: 0.9312 - val_loss: 1.5703 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 96/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.9336\n",
      "Epoch 96: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.5293 - accuracy: 0.9336 - val_loss: 1.5677 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 97/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5197 - accuracy: 0.9377\n",
      "Epoch 97: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5197 - accuracy: 0.9377 - val_loss: 1.5658 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 98/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.9356\n",
      "Epoch 98: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.5230 - accuracy: 0.9356 - val_loss: 1.5668 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 99/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.9358\n",
      "Epoch 99: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5256 - accuracy: 0.9358 - val_loss: 1.5695 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 100/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.9356\n",
      "Epoch 100: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5278 - accuracy: 0.9356 - val_loss: 1.5692 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 101/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5199 - accuracy: 0.9371\n",
      "Epoch 101: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.5199 - accuracy: 0.9371 - val_loss: 1.5656 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 102/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5574 - accuracy: 0.9265\n",
      "Epoch 102: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5574 - accuracy: 0.9265 - val_loss: 1.5757 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 103/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5354 - accuracy: 0.9323\n",
      "Epoch 103: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5354 - accuracy: 0.9323 - val_loss: 1.5664 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 104/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5381 - accuracy: 0.9317\n",
      "Epoch 104: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5381 - accuracy: 0.9317 - val_loss: 1.5715 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 105/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.9353\n",
      "Epoch 105: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 34s 98ms/step - loss: 0.5262 - accuracy: 0.9353 - val_loss: 1.5638 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 106/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.9384\n",
      "Epoch 106: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5166 - accuracy: 0.9384 - val_loss: 1.5653 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 107/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5548 - accuracy: 0.9287\n",
      "Epoch 107: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5548 - accuracy: 0.9287 - val_loss: 1.5679 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 108/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.9385\n",
      "Epoch 108: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5138 - accuracy: 0.9385 - val_loss: 1.5638 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 109/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.9378\n",
      "Epoch 109: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5189 - accuracy: 0.9378 - val_loss: 1.5636 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 110/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5275 - accuracy: 0.9354\n",
      "Epoch 110: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.5275 - accuracy: 0.9354 - val_loss: 1.5664 - val_accuracy: 0.7494 - lr: 0.0010\n",
      "Epoch 111/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5215 - accuracy: 0.9362\n",
      "Epoch 111: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 29s 82ms/step - loss: 0.5215 - accuracy: 0.9362 - val_loss: 1.5677 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 112/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5240 - accuracy: 0.9363\n",
      "Epoch 112: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5240 - accuracy: 0.9363 - val_loss: 1.5630 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 113/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.9387\n",
      "Epoch 113: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.5163 - accuracy: 0.9387 - val_loss: 1.5671 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 114/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5264 - accuracy: 0.9354\n",
      "Epoch 114: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5264 - accuracy: 0.9354 - val_loss: 1.5661 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 115/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.9361\n",
      "Epoch 115: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.5230 - accuracy: 0.9361 - val_loss: 1.5671 - val_accuracy: 0.7480 - lr: 0.0010\n",
      "Epoch 116/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5161 - accuracy: 0.9390\n",
      "Epoch 116: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5161 - accuracy: 0.9390 - val_loss: 1.5672 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 117/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.9431\n",
      "Epoch 117: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5050 - accuracy: 0.9431 - val_loss: 1.5648 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 118/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.9348\n",
      "Epoch 118: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5331 - accuracy: 0.9348 - val_loss: 1.5690 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 119/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.9352\n",
      "Epoch 119: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5255 - accuracy: 0.9352 - val_loss: 1.5679 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 120/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5186 - accuracy: 0.9364\n",
      "Epoch 120: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5186 - accuracy: 0.9364 - val_loss: 1.5642 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 121/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.9362\n",
      "Epoch 121: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 34s 97ms/step - loss: 0.5245 - accuracy: 0.9362 - val_loss: 1.5643 - val_accuracy: 0.7494 - lr: 0.0010\n",
      "Epoch 122/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5216 - accuracy: 0.9377\n",
      "Epoch 122: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5216 - accuracy: 0.9377 - val_loss: 1.5684 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 123/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5399 - accuracy: 0.9320\n",
      "Epoch 123: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5399 - accuracy: 0.9320 - val_loss: 1.5697 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 124/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.9373\n",
      "Epoch 124: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.5198 - accuracy: 0.9373 - val_loss: 1.5609 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 125/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5222 - accuracy: 0.9369\n",
      "Epoch 125: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5222 - accuracy: 0.9369 - val_loss: 1.5650 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 126/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5249 - accuracy: 0.9361\n",
      "Epoch 126: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5249 - accuracy: 0.9361 - val_loss: 1.5631 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 127/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.9352\n",
      "Epoch 127: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5271 - accuracy: 0.9352 - val_loss: 1.5628 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 128/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.9355\n",
      "Epoch 128: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.5247 - accuracy: 0.9355 - val_loss: 1.5663 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 129/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.9392\n",
      "Epoch 129: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5156 - accuracy: 0.9392 - val_loss: 1.5653 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 130/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5386 - accuracy: 0.9317\n",
      "Epoch 130: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5386 - accuracy: 0.9317 - val_loss: 1.5690 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 131/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5253 - accuracy: 0.9360\n",
      "Epoch 131: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 29s 82ms/step - loss: 0.5253 - accuracy: 0.9360 - val_loss: 1.5662 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 132/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.9360\n",
      "Epoch 132: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5259 - accuracy: 0.9360 - val_loss: 1.5635 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 133/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5339 - accuracy: 0.9324\n",
      "Epoch 133: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5339 - accuracy: 0.9324 - val_loss: 1.5706 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 134/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5202 - accuracy: 0.9372\n",
      "Epoch 134: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.5202 - accuracy: 0.9372 - val_loss: 1.5673 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 135/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.9375\n",
      "Epoch 135: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5213 - accuracy: 0.9375 - val_loss: 1.5686 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 136/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5275 - accuracy: 0.9361\n",
      "Epoch 136: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 35s 98ms/step - loss: 0.5275 - accuracy: 0.9361 - val_loss: 1.5673 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 137/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5157 - accuracy: 0.9386\n",
      "Epoch 137: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5157 - accuracy: 0.9386 - val_loss: 1.5671 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 138/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.9319\n",
      "Epoch 138: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5342 - accuracy: 0.9319 - val_loss: 1.5669 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 139/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.9372\n",
      "Epoch 139: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.5221 - accuracy: 0.9372 - val_loss: 1.5660 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 140/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.9349\n",
      "Epoch 140: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 34s 96ms/step - loss: 0.5258 - accuracy: 0.9349 - val_loss: 1.5684 - val_accuracy: 0.7494 - lr: 0.0010\n",
      "Epoch 141/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.9394\n",
      "Epoch 141: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 29s 83ms/step - loss: 0.5125 - accuracy: 0.9394 - val_loss: 1.5643 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 142/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.9335\n",
      "Epoch 142: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5288 - accuracy: 0.9335 - val_loss: 1.5686 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 143/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5368 - accuracy: 0.9316\n",
      "Epoch 143: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5368 - accuracy: 0.9316 - val_loss: 1.5705 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 144/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5361 - accuracy: 0.9330\n",
      "Epoch 144: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 34s 95ms/step - loss: 0.5361 - accuracy: 0.9330 - val_loss: 1.5681 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 145/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.9369\n",
      "Epoch 145: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5247 - accuracy: 0.9369 - val_loss: 1.5664 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 146/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5253 - accuracy: 0.9351\n",
      "Epoch 146: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5253 - accuracy: 0.9351 - val_loss: 1.5653 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 147/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5336 - accuracy: 0.9326\n",
      "Epoch 147: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5336 - accuracy: 0.9326 - val_loss: 1.5680 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 148/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5227 - accuracy: 0.9382\n",
      "Epoch 148: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5227 - accuracy: 0.9382 - val_loss: 1.5671 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 149/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.9350\n",
      "Epoch 149: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5279 - accuracy: 0.9350 - val_loss: 1.5675 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 150/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5119 - accuracy: 0.9383\n",
      "Epoch 150: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5119 - accuracy: 0.9383 - val_loss: 1.5663 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 151/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.9390\n",
      "Epoch 151: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5151 - accuracy: 0.9390 - val_loss: 1.5653 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 152/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5275 - accuracy: 0.9341\n",
      "Epoch 152: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.5275 - accuracy: 0.9341 - val_loss: 1.5689 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 153/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.9337\n",
      "Epoch 153: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5326 - accuracy: 0.9337 - val_loss: 1.5700 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 154/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5192 - accuracy: 0.9380\n",
      "Epoch 154: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5192 - accuracy: 0.9380 - val_loss: 1.5663 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 155/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.9356\n",
      "Epoch 155: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5295 - accuracy: 0.9356 - val_loss: 1.5681 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 156/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5246 - accuracy: 0.9364\n",
      "Epoch 156: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5246 - accuracy: 0.9364 - val_loss: 1.5679 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 157/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.9327\n",
      "Epoch 157: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5378 - accuracy: 0.9327 - val_loss: 1.5715 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 158/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.9356\n",
      "Epoch 158: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5223 - accuracy: 0.9356 - val_loss: 1.5666 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 159/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.9406\n",
      "Epoch 159: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5073 - accuracy: 0.9406 - val_loss: 1.5652 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 160/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5205 - accuracy: 0.9360\n",
      "Epoch 160: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5205 - accuracy: 0.9360 - val_loss: 1.5677 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 161/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.9401\n",
      "Epoch 161: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 29s 82ms/step - loss: 0.5123 - accuracy: 0.9401 - val_loss: 1.5687 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 162/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.9331\n",
      "Epoch 162: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5358 - accuracy: 0.9331 - val_loss: 1.5649 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 163/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5178 - accuracy: 0.9393\n",
      "Epoch 163: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5178 - accuracy: 0.9393 - val_loss: 1.5657 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 164/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5369 - accuracy: 0.9330\n",
      "Epoch 164: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5369 - accuracy: 0.9330 - val_loss: 1.5687 - val_accuracy: 0.7498 - lr: 0.0010\n",
      "Epoch 165/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.9358\n",
      "Epoch 165: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5228 - accuracy: 0.9358 - val_loss: 1.5681 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 166/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5377 - accuracy: 0.9319\n",
      "Epoch 166: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5377 - accuracy: 0.9319 - val_loss: 1.5668 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 167/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.9349\n",
      "Epoch 167: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5271 - accuracy: 0.9349 - val_loss: 1.5692 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 168/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.9372\n",
      "Epoch 168: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.5212 - accuracy: 0.9372 - val_loss: 1.5654 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 169/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5246 - accuracy: 0.9377\n",
      "Epoch 169: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5246 - accuracy: 0.9377 - val_loss: 1.5629 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 170/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.9342\n",
      "Epoch 170: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5313 - accuracy: 0.9342 - val_loss: 1.5681 - val_accuracy: 0.7494 - lr: 0.0010\n",
      "Epoch 171/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5227 - accuracy: 0.9370\n",
      "Epoch 171: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 29s 81ms/step - loss: 0.5227 - accuracy: 0.9370 - val_loss: 1.5712 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 172/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5154 - accuracy: 0.9375\n",
      "Epoch 172: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5154 - accuracy: 0.9375 - val_loss: 1.5669 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 173/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.9375\n",
      "Epoch 173: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5176 - accuracy: 0.9375 - val_loss: 1.5664 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 174/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.9342\n",
      "Epoch 174: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 34s 96ms/step - loss: 0.5279 - accuracy: 0.9342 - val_loss: 1.5722 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 175/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.9347\n",
      "Epoch 175: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5270 - accuracy: 0.9347 - val_loss: 1.5686 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 176/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5318 - accuracy: 0.9341\n",
      "Epoch 176: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5318 - accuracy: 0.9341 - val_loss: 1.5737 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 177/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.9389\n",
      "Epoch 177: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5130 - accuracy: 0.9389 - val_loss: 1.5674 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 178/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5226 - accuracy: 0.9369\n",
      "Epoch 178: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 34s 96ms/step - loss: 0.5226 - accuracy: 0.9369 - val_loss: 1.5693 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 179/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5135 - accuracy: 0.9404\n",
      "Epoch 179: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5135 - accuracy: 0.9404 - val_loss: 1.5683 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 180/180\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.9314\n",
      "Epoch 180: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.5379 - accuracy: 0.9314 - val_loss: 1.5664 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Current:  350\n",
      "Epoch 1/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.9317\n",
      "Epoch 1: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 44s 101ms/step - loss: 0.5358 - accuracy: 0.9317 - val_loss: 1.5669 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.9317\n",
      "Epoch 2: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5428 - accuracy: 0.9317 - val_loss: 1.5662 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5462 - accuracy: 0.9291\n",
      "Epoch 3: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5462 - accuracy: 0.9291 - val_loss: 1.5662 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5518 - accuracy: 0.9265\n",
      "Epoch 4: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5518 - accuracy: 0.9265 - val_loss: 1.5662 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.9333\n",
      "Epoch 5: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5356 - accuracy: 0.9333 - val_loss: 1.5664 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5224 - accuracy: 0.9382\n",
      "Epoch 6: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5224 - accuracy: 0.9382 - val_loss: 1.5664 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5264 - accuracy: 0.9353\n",
      "Epoch 7: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5264 - accuracy: 0.9353 - val_loss: 1.5665 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5573 - accuracy: 0.9271\n",
      "Epoch 8: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 29s 83ms/step - loss: 0.5573 - accuracy: 0.9271 - val_loss: 1.5673 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.9336\n",
      "Epoch 9: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.5358 - accuracy: 0.9336 - val_loss: 1.5664 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.9336\n",
      "Epoch 10: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5298 - accuracy: 0.9336 - val_loss: 1.5665 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5370 - accuracy: 0.9323\n",
      "Epoch 11: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5370 - accuracy: 0.9323 - val_loss: 1.5667 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5439 - accuracy: 0.9292\n",
      "Epoch 12: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5439 - accuracy: 0.9292 - val_loss: 1.5667 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5385 - accuracy: 0.9316\n",
      "Epoch 13: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.5385 - accuracy: 0.9316 - val_loss: 1.5662 - val_accuracy: 0.7484 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.9341\n",
      "Epoch 14: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5299 - accuracy: 0.9341 - val_loss: 1.5663 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.9298\n",
      "Epoch 15: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5481 - accuracy: 0.9298 - val_loss: 1.5665 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5469 - accuracy: 0.9294\n",
      "Epoch 16: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5469 - accuracy: 0.9294 - val_loss: 1.5669 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5699 - accuracy: 0.9225\n",
      "Epoch 17: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.5699 - accuracy: 0.9225 - val_loss: 1.5664 - val_accuracy: 0.7482 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5224 - accuracy: 0.9362\n",
      "Epoch 18: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.5224 - accuracy: 0.9362 - val_loss: 1.5663 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.9359\n",
      "Epoch 19: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.5212 - accuracy: 0.9359 - val_loss: 1.5667 - val_accuracy: 0.7486 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5500 - accuracy: 0.9286\n",
      "Epoch 20: val_loss did not improve from 1.50250\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5500 - accuracy: 0.9286 - val_loss: 1.5675 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Current:  381\n",
      "313/313 [==============================] - 7s 15ms/step\n",
      "Accuracy: 74.46000000000001\n",
      "Error: 25.539999999999992\n",
      "ECE: 0.14255762116760012\n",
      "MCE: 0.3067331643514736\n",
      "Loss: 1.2519651429346974\n",
      "brier: 0.22999444347230427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[25.539999999999992,\n",
       " 0.14255762116760012,\n",
       " 0.3067331643514736,\n",
       " 1.2519651429346974,\n",
       " 0.22999444347230427]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, img_gen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list, batch_size=128,lr_schedule = [[0, 0.1],[nb_epochs*0.5,0.01],[nb_epochs*0.75,0.001]],cbks=[checkpointer], name='resnet_sd_cifar100_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df772bca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T04:45:05.451741Z",
     "iopub.status.busy": "2023-04-25T04:45:05.450691Z",
     "iopub.status.idle": "2023-04-25T04:45:20.171455Z",
     "shell.execute_reply": "2023-04-25T04:45:20.170394Z"
    },
    "papermill": {
     "duration": 25.565696,
     "end_time": "2023-04-25T04:45:20.173908",
     "exception": false,
     "start_time": "2023-04-25T04:44:54.608212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack_n = 18\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "batch_size = 128\n",
    "epochs = 200 \n",
    "iterations = 45000 // batch_size\n",
    "weight_decay = 0.0001\n",
    "seed = 333\n",
    "\n",
    "\n",
    "def residual_network(img_input, classes_num=10, stack_n=5):\n",
    "    def residual_block(intput, out_channel, increase=False):\n",
    "        if increase:\n",
    "            stride = (2, 2)\n",
    "        else:\n",
    "            stride = (1, 1)\n",
    "\n",
    "        pre_bn = BatchNormalization()(intput)\n",
    "        pre_relu = Activation('relu')(pre_bn)\n",
    "\n",
    "        conv_1 = Conv2D(out_channel, kernel_size=(3, 3), strides=stride, padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(pre_relu)\n",
    "        bn_1 = BatchNormalization()(conv_1)\n",
    "        relu1 = Activation('relu')(bn_1)\n",
    "        conv_2 = Conv2D(out_channel, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                        kernel_regularizer=regularizers.l2(weight_decay))(relu1)\n",
    "        if increase:\n",
    "            projection = Conv2D(out_channel,\n",
    "                                kernel_size=(1, 1),\n",
    "                                strides=(2, 2),\n",
    "                                padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(weight_decay))(intput)\n",
    "            block = add([conv_2, projection])\n",
    "        else:\n",
    "            block = add([intput, conv_2])\n",
    "        return block\n",
    "\n",
    "    # build model\n",
    "    # total layers = stack_n * 3 * 2 + 2\n",
    "    # stack_n = 5 by default, total layers = 32\n",
    "    # input: 32x32x3 output: 32x32x16\n",
    "    x = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "\n",
    "    # input: 32x32x16 output: 32x32x16\n",
    "    for _ in range(stack_n):\n",
    "        x = residual_block(x, 16, False)\n",
    "\n",
    "    # input: 32x32x16 output: 16x16x32\n",
    "    x = residual_block(x, 32, True)\n",
    "    for _ in range(1, stack_n):\n",
    "        x = residual_block(x, 32, False)\n",
    "\n",
    "    # input: 16x16x32 output: 8x8x64\n",
    "    x = residual_block(x, 64, True)\n",
    "    for _ in range(1, stack_n):\n",
    "        x = residual_block(x, 64, False)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # input: 64 output: 10\n",
    "    x = Dense(classes_num, activation='softmax',\n",
    "              kernel_initializer=\"he_normal\",\n",
    "              kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1,\n",
    "                                                      random_state=seed)  # random_state = seed\n",
    "\n",
    "img_mean = x_train45.mean(axis=0)  # per-pixel mean, what about std?\n",
    "img_std = x_train45.std(axis=0)\n",
    "x_train45 = (x_train45 - img_mean) / img_std\n",
    "x_val = (x_val - img_mean) / img_std\n",
    "x_test = (x_test - img_mean) / img_std\n",
    "\n",
    "# build network\n",
    "img_input = Input(shape=(img_rows, img_cols, img_channels))\n",
    "output = residual_network(img_input, num_classes, stack_n)\n",
    "model = Model(img_input, output)\n",
    "print(model.summary())\n",
    "\n",
    "# set optimizer\n",
    "sgd = optimizers.SGD(learning_rate=.1, momentum=0.9, nesterov=True)\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant', cval=0.)\n",
    "datagen.fit(x_train45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3364a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T04:45:41.206152Z",
     "iopub.status.busy": "2023-04-25T04:45:41.205750Z",
     "iopub.status.idle": "2023-04-25T04:45:41.390789Z",
     "shell.execute_reply": "2023-04-25T04:45:41.389761Z"
    },
    "papermill": {
     "duration": 10.71856,
     "end_time": "2023-04-25T04:45:41.393233",
     "exception": false,
     "start_time": "2023-04-25T04:45:30.674673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.9:\n",
    "    freezing_list.append(int(epochs*0.6))\n",
    "  elif i < len(model.layers) * 0.98:\n",
    "    freezing_list.append(int(epochs*0.96))\n",
    "freezing_list.append(epochs)\n",
    "checkpointer = ModelCheckpoint('model_resnet_c10_best_2.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3562df2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T04:46:02.080701Z",
     "iopub.status.busy": "2023-04-25T04:46:02.080338Z",
     "iopub.status.idle": "2023-04-25T07:33:51.640430Z",
     "shell.execute_reply": "2023-04-25T07:33:51.639263Z"
    },
    "papermill": {
     "duration": 10079.799472,
     "end_time": "2023-04-25T07:33:51.642674",
     "exception": false,
     "start_time": "2023-04-25T04:45:51.843202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_100\n",
      ".........vars\n",
      "......activation_101\n",
      ".........vars\n",
      "......activation_102\n",
      ".........vars\n",
      "......activation_103\n",
      ".........vars\n",
      "......activation_104\n",
      ".........vars\n",
      "......activation_105\n",
      ".........vars\n",
      "......activation_106\n",
      ".........vars\n",
      "......activation_107\n",
      ".........vars\n",
      "......activation_108\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_55\n",
      ".........vars\n",
      "......activation_56\n",
      ".........vars\n",
      "......activation_57\n",
      ".........vars\n",
      "......activation_58\n",
      ".........vars\n",
      "......activation_59\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_60\n",
      ".........vars\n",
      "......activation_61\n",
      ".........vars\n",
      "......activation_62\n",
      ".........vars\n",
      "......activation_63\n",
      ".........vars\n",
      "......activation_64\n",
      ".........vars\n",
      "......activation_65\n",
      ".........vars\n",
      "......activation_66\n",
      ".........vars\n",
      "......activation_67\n",
      ".........vars\n",
      "......activation_68\n",
      ".........vars\n",
      "......activation_69\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_70\n",
      ".........vars\n",
      "......activation_71\n",
      ".........vars\n",
      "......activation_72\n",
      ".........vars\n",
      "......activation_73\n",
      ".........vars\n",
      "......activation_74\n",
      ".........vars\n",
      "......activation_75\n",
      ".........vars\n",
      "......activation_76\n",
      ".........vars\n",
      "......activation_77\n",
      ".........vars\n",
      "......activation_78\n",
      ".........vars\n",
      "......activation_79\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_80\n",
      ".........vars\n",
      "......activation_81\n",
      ".........vars\n",
      "......activation_82\n",
      ".........vars\n",
      "......activation_83\n",
      ".........vars\n",
      "......activation_84\n",
      ".........vars\n",
      "......activation_85\n",
      ".........vars\n",
      "......activation_86\n",
      ".........vars\n",
      "......activation_87\n",
      ".........vars\n",
      "......activation_88\n",
      ".........vars\n",
      "......activation_89\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......activation_90\n",
      ".........vars\n",
      "......activation_91\n",
      ".........vars\n",
      "......activation_92\n",
      ".........vars\n",
      "......activation_93\n",
      ".........vars\n",
      "......activation_94\n",
      ".........vars\n",
      "......activation_95\n",
      ".........vars\n",
      "......activation_96\n",
      ".........vars\n",
      "......activation_97\n",
      ".........vars\n",
      "......activation_98\n",
      ".........vars\n",
      "......activation_99\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_109\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_110\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-25 04:46:02       180502\n",
      "variables.h5                                   2023-04-25 04:46:03      7938896\n",
      "metadata.json                                  2023-04-25 04:46:02           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-25 04:46:02       180502\n",
      "variables.h5                                   2023-04-25 04:46:02      7938896\n",
      "metadata.json                                  2023-04-25 04:46:02           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_100\n",
      ".........vars\n",
      "......activation_101\n",
      ".........vars\n",
      "......activation_102\n",
      ".........vars\n",
      "......activation_103\n",
      ".........vars\n",
      "......activation_104\n",
      ".........vars\n",
      "......activation_105\n",
      ".........vars\n",
      "......activation_106\n",
      ".........vars\n",
      "......activation_107\n",
      ".........vars\n",
      "......activation_108\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_55\n",
      ".........vars\n",
      "......activation_56\n",
      ".........vars\n",
      "......activation_57\n",
      ".........vars\n",
      "......activation_58\n",
      ".........vars\n",
      "......activation_59\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_60\n",
      ".........vars\n",
      "......activation_61\n",
      ".........vars\n",
      "......activation_62\n",
      ".........vars\n",
      "......activation_63\n",
      ".........vars\n",
      "......activation_64\n",
      ".........vars\n",
      "......activation_65\n",
      ".........vars\n",
      "......activation_66\n",
      ".........vars\n",
      "......activation_67\n",
      ".........vars\n",
      "......activation_68\n",
      ".........vars\n",
      "......activation_69\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_70\n",
      ".........vars\n",
      "......activation_71\n",
      ".........vars\n",
      "......activation_72\n",
      ".........vars\n",
      "......activation_73\n",
      ".........vars\n",
      "......activation_74\n",
      ".........vars\n",
      "......activation_75\n",
      ".........vars\n",
      "......activation_76\n",
      ".........vars\n",
      "......activation_77\n",
      ".........vars\n",
      "......activation_78\n",
      ".........vars\n",
      "......activation_79\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_80\n",
      ".........vars\n",
      "......activation_81\n",
      ".........vars\n",
      "......activation_82\n",
      ".........vars\n",
      "......activation_83\n",
      ".........vars\n",
      "......activation_84\n",
      ".........vars\n",
      "......activation_85\n",
      ".........vars\n",
      "......activation_86\n",
      ".........vars\n",
      "......activation_87\n",
      ".........vars\n",
      "......activation_88\n",
      ".........vars\n",
      "......activation_89\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......activation_90\n",
      ".........vars\n",
      "......activation_91\n",
      ".........vars\n",
      "......activation_92\n",
      ".........vars\n",
      "......activation_93\n",
      ".........vars\n",
      "......activation_94\n",
      ".........vars\n",
      "......activation_95\n",
      ".........vars\n",
      "......activation_96\n",
      ".........vars\n",
      "......activation_97\n",
      ".........vars\n",
      "......activation_98\n",
      ".........vars\n",
      "......activation_99\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_109\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_110\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3233 - accuracy: 0.4339\n",
      "Epoch 1: val_loss improved from inf to 3.63014, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 58s 126ms/step - loss: 2.3233 - accuracy: 0.4339 - val_loss: 3.6301 - val_accuracy: 0.3354 - lr: 0.1000\n",
      "Epoch 2/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7998 - accuracy: 0.6076\n",
      "Epoch 2: val_loss improved from 3.63014 to 2.03768, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 1.7998 - accuracy: 0.6076 - val_loss: 2.0377 - val_accuracy: 0.4750 - lr: 0.1000\n",
      "Epoch 3/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5389 - accuracy: 0.6789\n",
      "Epoch 3: val_loss improved from 2.03768 to 1.85001, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 1.5389 - accuracy: 0.6789 - val_loss: 1.8500 - val_accuracy: 0.6028 - lr: 0.1000\n",
      "Epoch 4/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3460 - accuracy: 0.7302\n",
      "Epoch 4: val_loss improved from 1.85001 to 1.51720, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 45s 129ms/step - loss: 1.3460 - accuracy: 0.7302 - val_loss: 1.5172 - val_accuracy: 0.6678 - lr: 0.1000\n",
      "Epoch 5/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2147 - accuracy: 0.7623\n",
      "Epoch 5: val_loss did not improve from 1.51720\n",
      "351/351 [==============================] - 42s 121ms/step - loss: 1.2147 - accuracy: 0.7623 - val_loss: 1.6829 - val_accuracy: 0.6364 - lr: 0.1000\n",
      "Epoch 6/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1152 - accuracy: 0.7792\n",
      "Epoch 6: val_loss did not improve from 1.51720\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 1.1152 - accuracy: 0.7792 - val_loss: 1.8717 - val_accuracy: 0.5802 - lr: 0.1000\n",
      "Epoch 7/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0335 - accuracy: 0.7961\n",
      "Epoch 7: val_loss did not improve from 1.51720\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 1.0335 - accuracy: 0.7961 - val_loss: 1.6789 - val_accuracy: 0.6632 - lr: 0.1000\n",
      "Epoch 8/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9671 - accuracy: 0.8090\n",
      "Epoch 8: val_loss did not improve from 1.51720\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.9671 - accuracy: 0.8090 - val_loss: 3.3520 - val_accuracy: 0.4002 - lr: 0.1000\n",
      "Epoch 9/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9107 - accuracy: 0.8187\n",
      "Epoch 9: val_loss improved from 1.51720 to 1.25243, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 0.9107 - accuracy: 0.8187 - val_loss: 1.2524 - val_accuracy: 0.7238 - lr: 0.1000\n",
      "Epoch 10/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8590 - accuracy: 0.8279\n",
      "Epoch 10: val_loss improved from 1.25243 to 1.24342, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 0.8590 - accuracy: 0.8279 - val_loss: 1.2434 - val_accuracy: 0.7080 - lr: 0.1000\n",
      "Epoch 11/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8208 - accuracy: 0.8348\n",
      "Epoch 11: val_loss improved from 1.24342 to 1.17535, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 44s 124ms/step - loss: 0.8208 - accuracy: 0.8348 - val_loss: 1.1753 - val_accuracy: 0.7482 - lr: 0.1000\n",
      "Epoch 12/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7807 - accuracy: 0.8415\n",
      "Epoch 12: val_loss did not improve from 1.17535\n",
      "351/351 [==============================] - 47s 134ms/step - loss: 0.7807 - accuracy: 0.8415 - val_loss: 1.2975 - val_accuracy: 0.7170 - lr: 0.1000\n",
      "Epoch 13/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7519 - accuracy: 0.8484\n",
      "Epoch 13: val_loss improved from 1.17535 to 0.88748, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 0.7519 - accuracy: 0.8484 - val_loss: 0.8875 - val_accuracy: 0.8040 - lr: 0.1000\n",
      "Epoch 14/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7262 - accuracy: 0.8534\n",
      "Epoch 14: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 0.7262 - accuracy: 0.8534 - val_loss: 1.1536 - val_accuracy: 0.7338 - lr: 0.1000\n",
      "Epoch 15/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6986 - accuracy: 0.8582\n",
      "Epoch 15: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.6986 - accuracy: 0.8582 - val_loss: 1.1498 - val_accuracy: 0.7434 - lr: 0.1000\n",
      "Epoch 16/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6784 - accuracy: 0.8615\n",
      "Epoch 16: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 0.6784 - accuracy: 0.8615 - val_loss: 1.3153 - val_accuracy: 0.6826 - lr: 0.1000\n",
      "Epoch 17/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6621 - accuracy: 0.8633\n",
      "Epoch 17: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 0.6621 - accuracy: 0.8633 - val_loss: 1.1340 - val_accuracy: 0.7398 - lr: 0.1000\n",
      "Epoch 18/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.8689\n",
      "Epoch 18: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 0.6435 - accuracy: 0.8689 - val_loss: 1.3534 - val_accuracy: 0.6960 - lr: 0.1000\n",
      "Epoch 19/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6262 - accuracy: 0.8732\n",
      "Epoch 19: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.6262 - accuracy: 0.8732 - val_loss: 0.9516 - val_accuracy: 0.7824 - lr: 0.1000\n",
      "Epoch 20/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6170 - accuracy: 0.8752\n",
      "Epoch 20: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.6170 - accuracy: 0.8752 - val_loss: 1.0854 - val_accuracy: 0.7402 - lr: 0.1000\n",
      "Epoch 21/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6062 - accuracy: 0.8764\n",
      "Epoch 21: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 0.6062 - accuracy: 0.8764 - val_loss: 1.1765 - val_accuracy: 0.7124 - lr: 0.1000\n",
      "Epoch 22/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5976 - accuracy: 0.8813\n",
      "Epoch 22: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.5976 - accuracy: 0.8813 - val_loss: 0.9011 - val_accuracy: 0.7928 - lr: 0.1000\n",
      "Epoch 23/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.8837\n",
      "Epoch 23: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 0.5838 - accuracy: 0.8837 - val_loss: 0.9012 - val_accuracy: 0.7802 - lr: 0.1000\n",
      "Epoch 24/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5785 - accuracy: 0.8841\n",
      "Epoch 24: val_loss did not improve from 0.88748\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.5785 - accuracy: 0.8841 - val_loss: 1.3201 - val_accuracy: 0.7262 - lr: 0.1000\n",
      "Epoch 25/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.8874\n",
      "Epoch 25: val_loss improved from 0.88748 to 0.70393, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 44s 126ms/step - loss: 0.5688 - accuracy: 0.8874 - val_loss: 0.7039 - val_accuracy: 0.8536 - lr: 0.1000\n",
      "Epoch 26/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5594 - accuracy: 0.8892\n",
      "Epoch 26: val_loss did not improve from 0.70393\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.5594 - accuracy: 0.8892 - val_loss: 1.1877 - val_accuracy: 0.7336 - lr: 0.1000\n",
      "Epoch 27/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.8905\n",
      "Epoch 27: val_loss did not improve from 0.70393\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.5563 - accuracy: 0.8905 - val_loss: 0.7582 - val_accuracy: 0.8334 - lr: 0.1000\n",
      "Epoch 28/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5513 - accuracy: 0.8933\n",
      "Epoch 28: val_loss did not improve from 0.70393\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 0.5513 - accuracy: 0.8933 - val_loss: 0.9675 - val_accuracy: 0.7732 - lr: 0.1000\n",
      "Epoch 29/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5490 - accuracy: 0.8924\n",
      "Epoch 29: val_loss did not improve from 0.70393\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.5490 - accuracy: 0.8924 - val_loss: 0.8234 - val_accuracy: 0.8054 - lr: 0.1000\n",
      "Epoch 30/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.8985\n",
      "Epoch 30: val_loss did not improve from 0.70393\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 0.5335 - accuracy: 0.8985 - val_loss: 0.7271 - val_accuracy: 0.8446 - lr: 0.1000\n",
      "Epoch 31/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5368 - accuracy: 0.8968\n",
      "Epoch 31: val_loss did not improve from 0.70393\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 0.5368 - accuracy: 0.8968 - val_loss: 0.7588 - val_accuracy: 0.8354 - lr: 0.1000\n",
      "Epoch 32/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5355 - accuracy: 0.8988\n",
      "Epoch 32: val_loss did not improve from 0.70393\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.5355 - accuracy: 0.8988 - val_loss: 0.7105 - val_accuracy: 0.8494 - lr: 0.1000\n",
      "Epoch 33/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.9005\n",
      "Epoch 33: val_loss did not improve from 0.70393\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 0.5332 - accuracy: 0.9005 - val_loss: 0.9302 - val_accuracy: 0.7990 - lr: 0.1000\n",
      "Epoch 34/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5249 - accuracy: 0.9020\n",
      "Epoch 34: val_loss improved from 0.70393 to 0.69720, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 42s 121ms/step - loss: 0.5249 - accuracy: 0.9020 - val_loss: 0.6972 - val_accuracy: 0.8560 - lr: 0.1000\n",
      "Epoch 35/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5182 - accuracy: 0.9055\n",
      "Epoch 35: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 0.5182 - accuracy: 0.9055 - val_loss: 1.5553 - val_accuracy: 0.6538 - lr: 0.1000\n",
      "Epoch 36/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5233 - accuracy: 0.9029\n",
      "Epoch 36: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.5233 - accuracy: 0.9029 - val_loss: 0.9428 - val_accuracy: 0.7750 - lr: 0.1000\n",
      "Epoch 37/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5177 - accuracy: 0.9055\n",
      "Epoch 37: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.5177 - accuracy: 0.9055 - val_loss: 0.9931 - val_accuracy: 0.7796 - lr: 0.1000\n",
      "Epoch 38/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5153 - accuracy: 0.9058\n",
      "Epoch 38: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.5153 - accuracy: 0.9058 - val_loss: 0.9603 - val_accuracy: 0.7970 - lr: 0.1000\n",
      "Epoch 39/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5154 - accuracy: 0.9069\n",
      "Epoch 39: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.5154 - accuracy: 0.9069 - val_loss: 0.7381 - val_accuracy: 0.8394 - lr: 0.1000\n",
      "Epoch 40/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.9086\n",
      "Epoch 40: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 47s 134ms/step - loss: 0.5141 - accuracy: 0.9086 - val_loss: 0.7220 - val_accuracy: 0.8486 - lr: 0.1000\n",
      "Epoch 41/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.9109\n",
      "Epoch 41: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 0.5078 - accuracy: 0.9109 - val_loss: 0.9250 - val_accuracy: 0.8050 - lr: 0.1000\n",
      "Epoch 42/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.9111\n",
      "Epoch 42: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 47s 134ms/step - loss: 0.5070 - accuracy: 0.9111 - val_loss: 0.8941 - val_accuracy: 0.8116 - lr: 0.1000\n",
      "Epoch 43/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.9098\n",
      "Epoch 43: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.5070 - accuracy: 0.9098 - val_loss: 0.8719 - val_accuracy: 0.8172 - lr: 0.1000\n",
      "Epoch 44/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.9128\n",
      "Epoch 44: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 47s 134ms/step - loss: 0.5031 - accuracy: 0.9128 - val_loss: 0.7202 - val_accuracy: 0.8404 - lr: 0.1000\n",
      "Epoch 45/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5006 - accuracy: 0.9130\n",
      "Epoch 45: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.5006 - accuracy: 0.9130 - val_loss: 0.7387 - val_accuracy: 0.8512 - lr: 0.1000\n",
      "Epoch 46/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.9152\n",
      "Epoch 46: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 41s 116ms/step - loss: 0.4982 - accuracy: 0.9152 - val_loss: 0.8709 - val_accuracy: 0.8232 - lr: 0.1000\n",
      "Epoch 47/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.9138\n",
      "Epoch 47: val_loss did not improve from 0.69720\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.5031 - accuracy: 0.9138 - val_loss: 0.7427 - val_accuracy: 0.8510 - lr: 0.1000\n",
      "Epoch 48/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4980 - accuracy: 0.9162\n",
      "Epoch 48: val_loss improved from 0.69720 to 0.69479, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 0.4980 - accuracy: 0.9162 - val_loss: 0.6948 - val_accuracy: 0.8634 - lr: 0.1000\n",
      "Epoch 49/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5004 - accuracy: 0.9152\n",
      "Epoch 49: val_loss did not improve from 0.69479\n",
      "351/351 [==============================] - 41s 116ms/step - loss: 0.5004 - accuracy: 0.9152 - val_loss: 0.8324 - val_accuracy: 0.8272 - lr: 0.1000\n",
      "Epoch 50/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4942 - accuracy: 0.9180\n",
      "Epoch 50: val_loss did not improve from 0.69479\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.4942 - accuracy: 0.9180 - val_loss: 0.7722 - val_accuracy: 0.8334 - lr: 0.1000\n",
      "Epoch 51/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.9152\n",
      "Epoch 51: val_loss did not improve from 0.69479\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.5013 - accuracy: 0.9152 - val_loss: 0.7472 - val_accuracy: 0.8488 - lr: 0.1000\n",
      "Epoch 52/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.9154\n",
      "Epoch 52: val_loss did not improve from 0.69479\n",
      "351/351 [==============================] - 41s 116ms/step - loss: 0.4969 - accuracy: 0.9154 - val_loss: 0.9482 - val_accuracy: 0.8008 - lr: 0.1000\n",
      "Epoch 53/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4911 - accuracy: 0.9191\n",
      "Epoch 53: val_loss did not improve from 0.69479\n",
      "351/351 [==============================] - 47s 133ms/step - loss: 0.4911 - accuracy: 0.9191 - val_loss: 0.7258 - val_accuracy: 0.8452 - lr: 0.1000\n",
      "Epoch 54/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4936 - accuracy: 0.9183\n",
      "Epoch 54: val_loss did not improve from 0.69479\n",
      "351/351 [==============================] - 41s 116ms/step - loss: 0.4936 - accuracy: 0.9183 - val_loss: 0.7096 - val_accuracy: 0.8616 - lr: 0.1000\n",
      "Epoch 55/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4896 - accuracy: 0.9212\n",
      "Epoch 55: val_loss did not improve from 0.69479\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.4896 - accuracy: 0.9212 - val_loss: 0.9761 - val_accuracy: 0.7956 - lr: 0.1000\n",
      "Epoch 56/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.9207\n",
      "Epoch 56: val_loss improved from 0.69479 to 0.68819, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 0.4870 - accuracy: 0.9207 - val_loss: 0.6882 - val_accuracy: 0.8696 - lr: 0.1000\n",
      "Epoch 57/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.9193\n",
      "Epoch 57: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 116ms/step - loss: 0.4947 - accuracy: 0.9193 - val_loss: 0.7498 - val_accuracy: 0.8454 - lr: 0.1000\n",
      "Epoch 58/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4889 - accuracy: 0.9220\n",
      "Epoch 58: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.4889 - accuracy: 0.9220 - val_loss: 0.7988 - val_accuracy: 0.8408 - lr: 0.1000\n",
      "Epoch 59/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.9217\n",
      "Epoch 59: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 47s 133ms/step - loss: 0.4885 - accuracy: 0.9217 - val_loss: 0.6988 - val_accuracy: 0.8588 - lr: 0.1000\n",
      "Epoch 60/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4849 - accuracy: 0.9226\n",
      "Epoch 60: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 116ms/step - loss: 0.4849 - accuracy: 0.9226 - val_loss: 0.8046 - val_accuracy: 0.8406 - lr: 0.1000\n",
      "Epoch 61/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4912 - accuracy: 0.9199\n",
      "Epoch 61: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 0.4912 - accuracy: 0.9199 - val_loss: 0.7644 - val_accuracy: 0.8474 - lr: 0.1000\n",
      "Epoch 62/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4822 - accuracy: 0.9242\n",
      "Epoch 62: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 0.4822 - accuracy: 0.9242 - val_loss: 0.9295 - val_accuracy: 0.8150 - lr: 0.1000\n",
      "Epoch 63/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.9247\n",
      "Epoch 63: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.4847 - accuracy: 0.9247 - val_loss: 0.8161 - val_accuracy: 0.8312 - lr: 0.1000\n",
      "Epoch 64/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4861 - accuracy: 0.9233\n",
      "Epoch 64: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 0.4861 - accuracy: 0.9233 - val_loss: 0.8067 - val_accuracy: 0.8294 - lr: 0.1000\n",
      "Epoch 65/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.9245\n",
      "Epoch 65: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 116ms/step - loss: 0.4850 - accuracy: 0.9245 - val_loss: 0.9353 - val_accuracy: 0.8096 - lr: 0.1000\n",
      "Epoch 66/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4867 - accuracy: 0.9232\n",
      "Epoch 66: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 0.4867 - accuracy: 0.9232 - val_loss: 0.8507 - val_accuracy: 0.8192 - lr: 0.1000\n",
      "Epoch 67/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.9265\n",
      "Epoch 67: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 115ms/step - loss: 0.4786 - accuracy: 0.9265 - val_loss: 0.8058 - val_accuracy: 0.8486 - lr: 0.1000\n",
      "Epoch 68/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.9252\n",
      "Epoch 68: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 116ms/step - loss: 0.4834 - accuracy: 0.9252 - val_loss: 0.7803 - val_accuracy: 0.8396 - lr: 0.1000\n",
      "Epoch 69/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.9247\n",
      "Epoch 69: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.4844 - accuracy: 0.9247 - val_loss: 0.7787 - val_accuracy: 0.8414 - lr: 0.1000\n",
      "Epoch 70/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4793 - accuracy: 0.9270\n",
      "Epoch 70: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.4793 - accuracy: 0.9270 - val_loss: 0.7930 - val_accuracy: 0.8562 - lr: 0.1000\n",
      "Epoch 71/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4831 - accuracy: 0.9256\n",
      "Epoch 71: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 0.4831 - accuracy: 0.9256 - val_loss: 0.7519 - val_accuracy: 0.8588 - lr: 0.1000\n",
      "Epoch 72/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.9256\n",
      "Epoch 72: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 116ms/step - loss: 0.4844 - accuracy: 0.9256 - val_loss: 0.7308 - val_accuracy: 0.8624 - lr: 0.1000\n",
      "Epoch 73/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4782 - accuracy: 0.9265\n",
      "Epoch 73: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.4782 - accuracy: 0.9265 - val_loss: 0.7016 - val_accuracy: 0.8624 - lr: 0.1000\n",
      "Epoch 74/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.9280\n",
      "Epoch 74: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.4813 - accuracy: 0.9280 - val_loss: 0.7553 - val_accuracy: 0.8554 - lr: 0.1000\n",
      "Epoch 75/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4840 - accuracy: 0.9257\n",
      "Epoch 75: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 0.4840 - accuracy: 0.9257 - val_loss: 0.8009 - val_accuracy: 0.8454 - lr: 0.1000\n",
      "Epoch 76/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.9266\n",
      "Epoch 76: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.4815 - accuracy: 0.9266 - val_loss: 0.7434 - val_accuracy: 0.8578 - lr: 0.1000\n",
      "Epoch 77/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.9285\n",
      "Epoch 77: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 0.4785 - accuracy: 0.9285 - val_loss: 0.8731 - val_accuracy: 0.8332 - lr: 0.1000\n",
      "Epoch 78/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.9277\n",
      "Epoch 78: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.4808 - accuracy: 0.9277 - val_loss: 0.7179 - val_accuracy: 0.8632 - lr: 0.1000\n",
      "Epoch 79/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.9291\n",
      "Epoch 79: val_loss did not improve from 0.68819\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.4775 - accuracy: 0.9291 - val_loss: 0.8199 - val_accuracy: 0.8346 - lr: 0.1000\n",
      "Epoch 80/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4857 - accuracy: 0.9272\n",
      "Epoch 80: val_loss improved from 0.68819 to 0.64311, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 50s 141ms/step - loss: 0.4857 - accuracy: 0.9272 - val_loss: 0.6431 - val_accuracy: 0.8780 - lr: 0.1000\n",
      "Epoch 81/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.9550\n",
      "Epoch 81: val_loss improved from 0.64311 to 0.49787, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.4046 - accuracy: 0.9550 - val_loss: 0.4979 - val_accuracy: 0.9262 - lr: 0.0100\n",
      "Epoch 82/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.9732\n",
      "Epoch 82: val_loss improved from 0.49787 to 0.48818, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 49s 141ms/step - loss: 0.3536 - accuracy: 0.9732 - val_loss: 0.4882 - val_accuracy: 0.9298 - lr: 0.0100\n",
      "Epoch 83/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.9783\n",
      "Epoch 83: val_loss improved from 0.48818 to 0.48479, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 0.3354 - accuracy: 0.9783 - val_loss: 0.4848 - val_accuracy: 0.9352 - lr: 0.0100\n",
      "Epoch 84/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.9809\n",
      "Epoch 84: val_loss did not improve from 0.48479\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 0.3239 - accuracy: 0.9809 - val_loss: 0.4879 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 85/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.9832\n",
      "Epoch 85: val_loss did not improve from 0.48479\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.3145 - accuracy: 0.9832 - val_loss: 0.4902 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 86/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.9840\n",
      "Epoch 86: val_loss improved from 0.48479 to 0.47818, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 43s 122ms/step - loss: 0.3067 - accuracy: 0.9840 - val_loss: 0.4782 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 87/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.9864\n",
      "Epoch 87: val_loss did not improve from 0.47818\n",
      "351/351 [==============================] - 44s 127ms/step - loss: 0.2971 - accuracy: 0.9864 - val_loss: 0.4913 - val_accuracy: 0.9356 - lr: 0.0100\n",
      "Epoch 88/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9870\n",
      "Epoch 88: val_loss improved from 0.47818 to 0.47800, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 0.2903 - accuracy: 0.9870 - val_loss: 0.4780 - val_accuracy: 0.9378 - lr: 0.0100\n",
      "Epoch 89/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2840 - accuracy: 0.9883\n",
      "Epoch 89: val_loss did not improve from 0.47800\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 0.2840 - accuracy: 0.9883 - val_loss: 0.4849 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 90/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9891\n",
      "Epoch 90: val_loss improved from 0.47800 to 0.47699, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 0.2792 - accuracy: 0.9891 - val_loss: 0.4770 - val_accuracy: 0.9338 - lr: 0.0100\n",
      "Epoch 91/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.9899\n",
      "Epoch 91: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 50s 141ms/step - loss: 0.2734 - accuracy: 0.9899 - val_loss: 0.4879 - val_accuracy: 0.9338 - lr: 0.0100\n",
      "Epoch 92/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2688 - accuracy: 0.9900\n",
      "Epoch 92: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.2688 - accuracy: 0.9900 - val_loss: 0.4877 - val_accuracy: 0.9378 - lr: 0.0100\n",
      "Epoch 93/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9916\n",
      "Epoch 93: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 49s 140ms/step - loss: 0.2624 - accuracy: 0.9916 - val_loss: 0.4790 - val_accuracy: 0.9378 - lr: 0.0100\n",
      "Epoch 94/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.9918\n",
      "Epoch 94: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.2588 - accuracy: 0.9918 - val_loss: 0.4982 - val_accuracy: 0.9354 - lr: 0.0100\n",
      "Epoch 95/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.9928\n",
      "Epoch 95: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.2529 - accuracy: 0.9928 - val_loss: 0.4811 - val_accuracy: 0.9336 - lr: 0.0100\n",
      "Epoch 96/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.9921\n",
      "Epoch 96: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 0.2517 - accuracy: 0.9921 - val_loss: 0.5027 - val_accuracy: 0.9334 - lr: 0.0100\n",
      "Epoch 97/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.9936\n",
      "Epoch 97: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.2445 - accuracy: 0.9936 - val_loss: 0.4840 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 98/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9935\n",
      "Epoch 98: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.2421 - accuracy: 0.9935 - val_loss: 0.4928 - val_accuracy: 0.9306 - lr: 0.0100\n",
      "Epoch 99/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9934\n",
      "Epoch 99: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 0.2394 - accuracy: 0.9934 - val_loss: 0.5021 - val_accuracy: 0.9310 - lr: 0.0100\n",
      "Epoch 100/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2348 - accuracy: 0.9943\n",
      "Epoch 100: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.2348 - accuracy: 0.9943 - val_loss: 0.4900 - val_accuracy: 0.9356 - lr: 0.0100\n",
      "Epoch 101/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9941\n",
      "Epoch 101: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 49s 140ms/step - loss: 0.2316 - accuracy: 0.9941 - val_loss: 0.4895 - val_accuracy: 0.9376 - lr: 0.0100\n",
      "Epoch 102/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9950\n",
      "Epoch 102: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.2274 - accuracy: 0.9950 - val_loss: 0.4979 - val_accuracy: 0.9332 - lr: 0.0100\n",
      "Epoch 103/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9954\n",
      "Epoch 103: val_loss did not improve from 0.47699\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 0.2228 - accuracy: 0.9954 - val_loss: 0.4886 - val_accuracy: 0.9352 - lr: 0.0100\n",
      "Epoch 104/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2198 - accuracy: 0.9955\n",
      "Epoch 104: val_loss improved from 0.47699 to 0.47349, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 0.2198 - accuracy: 0.9955 - val_loss: 0.4735 - val_accuracy: 0.9376 - lr: 0.0100\n",
      "Epoch 105/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9950\n",
      "Epoch 105: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.2189 - accuracy: 0.9950 - val_loss: 0.4863 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 106/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2153 - accuracy: 0.9956\n",
      "Epoch 106: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 0.2153 - accuracy: 0.9956 - val_loss: 0.5075 - val_accuracy: 0.9332 - lr: 0.0100\n",
      "Epoch 107/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9950\n",
      "Epoch 107: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.2134 - accuracy: 0.9950 - val_loss: 0.4954 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 108/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9948\n",
      "Epoch 108: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 49s 140ms/step - loss: 0.2116 - accuracy: 0.9948 - val_loss: 0.4895 - val_accuracy: 0.9356 - lr: 0.0100\n",
      "Epoch 109/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2081 - accuracy: 0.9953\n",
      "Epoch 109: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.2081 - accuracy: 0.9953 - val_loss: 0.4936 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 110/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9957\n",
      "Epoch 110: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 0.2047 - accuracy: 0.9957 - val_loss: 0.4986 - val_accuracy: 0.9322 - lr: 0.0100\n",
      "Epoch 111/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9951\n",
      "Epoch 111: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 43s 121ms/step - loss: 0.2039 - accuracy: 0.9951 - val_loss: 0.4871 - val_accuracy: 0.9318 - lr: 0.0100\n",
      "Epoch 112/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9964\n",
      "Epoch 112: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.1987 - accuracy: 0.9964 - val_loss: 0.4770 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 113/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9958\n",
      "Epoch 113: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 0.1965 - accuracy: 0.9958 - val_loss: 0.5237 - val_accuracy: 0.9326 - lr: 0.0100\n",
      "Epoch 114/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9960\n",
      "Epoch 114: val_loss did not improve from 0.47349\n",
      "351/351 [==============================] - 41s 118ms/step - loss: 0.1941 - accuracy: 0.9960 - val_loss: 0.4911 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 115/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9961\n",
      "Epoch 115: val_loss improved from 0.47349 to 0.47033, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 42s 120ms/step - loss: 0.1918 - accuracy: 0.9961 - val_loss: 0.4703 - val_accuracy: 0.9386 - lr: 0.0100\n",
      "Epoch 116/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9954\n",
      "Epoch 116: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 0.1909 - accuracy: 0.9954 - val_loss: 0.5090 - val_accuracy: 0.9312 - lr: 0.0100\n",
      "Epoch 117/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.9957\n",
      "Epoch 117: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 41s 117ms/step - loss: 0.1880 - accuracy: 0.9957 - val_loss: 0.4746 - val_accuracy: 0.9364 - lr: 0.0100\n",
      "Epoch 118/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9959\n",
      "Epoch 118: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 42s 119ms/step - loss: 0.1853 - accuracy: 0.9959 - val_loss: 0.4799 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 119/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9965\n",
      "Epoch 119: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 44s 125ms/step - loss: 0.1815 - accuracy: 0.9965 - val_loss: 0.4934 - val_accuracy: 0.9336 - lr: 0.0100\n",
      "Epoch 120/120\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1805 - accuracy: 0.9962\n",
      "Epoch 120: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 43s 123ms/step - loss: 0.1805 - accuracy: 0.9962 - val_loss: 0.4835 - val_accuracy: 0.9346 - lr: 0.0100\n",
      "Epoch 1/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9969\n",
      "Epoch 1: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 39s 91ms/step - loss: 0.1773 - accuracy: 0.9969 - val_loss: 0.4761 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 2/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9972\n",
      "Epoch 2: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.1768 - accuracy: 0.9972 - val_loss: 0.4771 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 3/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9972\n",
      "Epoch 3: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 29s 81ms/step - loss: 0.1760 - accuracy: 0.9972 - val_loss: 0.4759 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 4/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9980\n",
      "Epoch 4: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.1744 - accuracy: 0.9980 - val_loss: 0.4778 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 5/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.9979\n",
      "Epoch 5: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.1750 - accuracy: 0.9979 - val_loss: 0.4739 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 6/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9977\n",
      "Epoch 6: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.1744 - accuracy: 0.9977 - val_loss: 0.4787 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 7/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9980\n",
      "Epoch 7: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 29s 84ms/step - loss: 0.1746 - accuracy: 0.9980 - val_loss: 0.4754 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 8/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9978\n",
      "Epoch 8: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.1747 - accuracy: 0.9978 - val_loss: 0.4705 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 9/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9977\n",
      "Epoch 9: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.1746 - accuracy: 0.9977 - val_loss: 0.4819 - val_accuracy: 0.9362 - lr: 0.0100\n",
      "Epoch 10/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1737 - accuracy: 0.9980\n",
      "Epoch 10: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.1737 - accuracy: 0.9980 - val_loss: 0.4758 - val_accuracy: 0.9354 - lr: 0.0100\n",
      "Epoch 11/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9979\n",
      "Epoch 11: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.1740 - accuracy: 0.9979 - val_loss: 0.4752 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 12/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9977\n",
      "Epoch 12: val_loss did not improve from 0.47033\n",
      "351/351 [==============================] - 29s 81ms/step - loss: 0.1741 - accuracy: 0.9977 - val_loss: 0.4740 - val_accuracy: 0.9354 - lr: 0.0100\n",
      "Epoch 13/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9981\n",
      "Epoch 13: val_loss improved from 0.47033 to 0.46786, saving model to model_resnet_c10_best_2.hdf5\n",
      "351/351 [==============================] - 29s 81ms/step - loss: 0.1735 - accuracy: 0.9981 - val_loss: 0.4679 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 14/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9979\n",
      "Epoch 14: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 35s 98ms/step - loss: 0.1732 - accuracy: 0.9979 - val_loss: 0.4735 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 15/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9979\n",
      "Epoch 15: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.1731 - accuracy: 0.9979 - val_loss: 0.4696 - val_accuracy: 0.9346 - lr: 0.0100\n",
      "Epoch 16/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.9979\n",
      "Epoch 16: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.1728 - accuracy: 0.9979 - val_loss: 0.4754 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 17/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9977\n",
      "Epoch 17: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.1732 - accuracy: 0.9977 - val_loss: 0.4790 - val_accuracy: 0.9358 - lr: 0.0100\n",
      "Epoch 18/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9983\n",
      "Epoch 18: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.1721 - accuracy: 0.9983 - val_loss: 0.4743 - val_accuracy: 0.9360 - lr: 0.0100\n",
      "Epoch 19/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9984\n",
      "Epoch 19: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.1718 - accuracy: 0.9984 - val_loss: 0.4743 - val_accuracy: 0.9374 - lr: 0.0100\n",
      "Epoch 20/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9979\n",
      "Epoch 20: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.1720 - accuracy: 0.9979 - val_loss: 0.4881 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 21/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9983\n",
      "Epoch 21: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.1718 - accuracy: 0.9983 - val_loss: 0.4771 - val_accuracy: 0.9368 - lr: 0.0100\n",
      "Epoch 22/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9982\n",
      "Epoch 22: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.1715 - accuracy: 0.9982 - val_loss: 0.4761 - val_accuracy: 0.9368 - lr: 0.0100\n",
      "Epoch 23/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9979\n",
      "Epoch 23: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.1720 - accuracy: 0.9979 - val_loss: 0.4733 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 24/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9978\n",
      "Epoch 24: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.1721 - accuracy: 0.9978 - val_loss: 0.4831 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 25/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9980\n",
      "Epoch 25: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.1721 - accuracy: 0.9980 - val_loss: 0.4786 - val_accuracy: 0.9356 - lr: 0.0100\n",
      "Epoch 26/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9979\n",
      "Epoch 26: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.1717 - accuracy: 0.9979 - val_loss: 0.4781 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 27/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9981\n",
      "Epoch 27: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.1713 - accuracy: 0.9981 - val_loss: 0.4825 - val_accuracy: 0.9348 - lr: 0.0100\n",
      "Epoch 28/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9980\n",
      "Epoch 28: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 0.1714 - accuracy: 0.9980 - val_loss: 0.4767 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 29/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9981\n",
      "Epoch 29: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.1712 - accuracy: 0.9981 - val_loss: 0.4730 - val_accuracy: 0.9366 - lr: 0.0100\n",
      "Epoch 30/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9981\n",
      "Epoch 30: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.1707 - accuracy: 0.9981 - val_loss: 0.4729 - val_accuracy: 0.9378 - lr: 0.0100\n",
      "Epoch 31/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1710 - accuracy: 0.9984\n",
      "Epoch 31: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.1710 - accuracy: 0.9984 - val_loss: 0.4760 - val_accuracy: 0.9382 - lr: 0.0010\n",
      "Epoch 32/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9980\n",
      "Epoch 32: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 0.1711 - accuracy: 0.9980 - val_loss: 0.4765 - val_accuracy: 0.9380 - lr: 0.0010\n",
      "Epoch 33/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9984\n",
      "Epoch 33: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.1706 - accuracy: 0.9984 - val_loss: 0.4757 - val_accuracy: 0.9378 - lr: 0.0010\n",
      "Epoch 34/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9982\n",
      "Epoch 34: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.1708 - accuracy: 0.9982 - val_loss: 0.4760 - val_accuracy: 0.9372 - lr: 0.0010\n",
      "Epoch 35/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9982\n",
      "Epoch 35: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.1700 - accuracy: 0.9982 - val_loss: 0.4775 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 36/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9982\n",
      "Epoch 36: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.1708 - accuracy: 0.9982 - val_loss: 0.4761 - val_accuracy: 0.9374 - lr: 0.0010\n",
      "Epoch 37/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9986\n",
      "Epoch 37: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 29s 84ms/step - loss: 0.1703 - accuracy: 0.9986 - val_loss: 0.4760 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 38/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9985\n",
      "Epoch 38: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.1701 - accuracy: 0.9985 - val_loss: 0.4764 - val_accuracy: 0.9374 - lr: 0.0010\n",
      "Epoch 39/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9983\n",
      "Epoch 39: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.1698 - accuracy: 0.9983 - val_loss: 0.4758 - val_accuracy: 0.9378 - lr: 0.0010\n",
      "Epoch 40/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9982\n",
      "Epoch 40: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.1705 - accuracy: 0.9982 - val_loss: 0.4759 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 41/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9983\n",
      "Epoch 41: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 36s 104ms/step - loss: 0.1700 - accuracy: 0.9983 - val_loss: 0.4753 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 42/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9981\n",
      "Epoch 42: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.1699 - accuracy: 0.9981 - val_loss: 0.4768 - val_accuracy: 0.9378 - lr: 0.0010\n",
      "Epoch 43/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9986\n",
      "Epoch 43: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.1698 - accuracy: 0.9986 - val_loss: 0.4764 - val_accuracy: 0.9378 - lr: 0.0010\n",
      "Epoch 44/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9982\n",
      "Epoch 44: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.1701 - accuracy: 0.9982 - val_loss: 0.4766 - val_accuracy: 0.9378 - lr: 0.0010\n",
      "Epoch 45/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9983\n",
      "Epoch 45: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 0.1700 - accuracy: 0.9983 - val_loss: 0.4759 - val_accuracy: 0.9384 - lr: 0.0010\n",
      "Epoch 46/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9977\n",
      "Epoch 46: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 26s 75ms/step - loss: 0.1711 - accuracy: 0.9977 - val_loss: 0.4742 - val_accuracy: 0.9380 - lr: 0.0010\n",
      "Epoch 47/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9984\n",
      "Epoch 47: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.1699 - accuracy: 0.9984 - val_loss: 0.4755 - val_accuracy: 0.9382 - lr: 0.0010\n",
      "Epoch 48/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9982\n",
      "Epoch 48: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.1700 - accuracy: 0.9982 - val_loss: 0.4751 - val_accuracy: 0.9372 - lr: 0.0010\n",
      "Epoch 49/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9982\n",
      "Epoch 49: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.1704 - accuracy: 0.9982 - val_loss: 0.4750 - val_accuracy: 0.9382 - lr: 0.0010\n",
      "Epoch 50/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9982\n",
      "Epoch 50: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 26s 75ms/step - loss: 0.1702 - accuracy: 0.9982 - val_loss: 0.4757 - val_accuracy: 0.9376 - lr: 0.0010\n",
      "Epoch 51/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9980\n",
      "Epoch 51: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 80ms/step - loss: 0.1705 - accuracy: 0.9980 - val_loss: 0.4766 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 52/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9982\n",
      "Epoch 52: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 78ms/step - loss: 0.1703 - accuracy: 0.9982 - val_loss: 0.4760 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 53/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9984\n",
      "Epoch 53: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.1702 - accuracy: 0.9984 - val_loss: 0.4746 - val_accuracy: 0.9376 - lr: 0.0010\n",
      "Epoch 54/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9981\n",
      "Epoch 54: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.1703 - accuracy: 0.9981 - val_loss: 0.4767 - val_accuracy: 0.9372 - lr: 0.0010\n",
      "Epoch 55/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9982\n",
      "Epoch 55: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.1701 - accuracy: 0.9982 - val_loss: 0.4755 - val_accuracy: 0.9372 - lr: 0.0010\n",
      "Epoch 56/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9979\n",
      "Epoch 56: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.1711 - accuracy: 0.9979 - val_loss: 0.4761 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 57/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1695 - accuracy: 0.9986\n",
      "Epoch 57: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 78ms/step - loss: 0.1695 - accuracy: 0.9986 - val_loss: 0.4784 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 58/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9983\n",
      "Epoch 58: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 0.1704 - accuracy: 0.9983 - val_loss: 0.4783 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 59/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9980\n",
      "Epoch 59: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.1703 - accuracy: 0.9980 - val_loss: 0.4796 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 60/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9983\n",
      "Epoch 60: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.1702 - accuracy: 0.9983 - val_loss: 0.4778 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 61/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9983\n",
      "Epoch 61: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 81ms/step - loss: 0.1700 - accuracy: 0.9983 - val_loss: 0.4786 - val_accuracy: 0.9364 - lr: 0.0010\n",
      "Epoch 62/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9984\n",
      "Epoch 62: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.1700 - accuracy: 0.9984 - val_loss: 0.4785 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 63/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9986\n",
      "Epoch 63: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 76ms/step - loss: 0.1692 - accuracy: 0.9986 - val_loss: 0.4775 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 64/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9985\n",
      "Epoch 64: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 26s 74ms/step - loss: 0.1697 - accuracy: 0.9985 - val_loss: 0.4775 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 65/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9984\n",
      "Epoch 65: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.1698 - accuracy: 0.9984 - val_loss: 0.4761 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 66/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9982\n",
      "Epoch 66: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 29s 81ms/step - loss: 0.1700 - accuracy: 0.9982 - val_loss: 0.4762 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 67/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9984\n",
      "Epoch 67: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.1702 - accuracy: 0.9984 - val_loss: 0.4769 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 68/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9984\n",
      "Epoch 68: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 26s 75ms/step - loss: 0.1699 - accuracy: 0.9984 - val_loss: 0.4771 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 69/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1693 - accuracy: 0.9986\n",
      "Epoch 69: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 75ms/step - loss: 0.1693 - accuracy: 0.9986 - val_loss: 0.4770 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 70/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9986\n",
      "Epoch 70: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 27s 77ms/step - loss: 0.1698 - accuracy: 0.9986 - val_loss: 0.4764 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 71/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.9987\n",
      "Epoch 71: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 28s 79ms/step - loss: 0.1694 - accuracy: 0.9987 - val_loss: 0.4763 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 72/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9982\n",
      "Epoch 72: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 0.1702 - accuracy: 0.9982 - val_loss: 0.4762 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Current:  347\n",
      "Epoch 1/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9987\n",
      "Epoch 1: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 37s 84ms/step - loss: 0.1688 - accuracy: 0.9987 - val_loss: 0.4761 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 2/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9986\n",
      "Epoch 2: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 26s 75ms/step - loss: 0.1692 - accuracy: 0.9986 - val_loss: 0.4756 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 3/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9987\n",
      "Epoch 3: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 26s 74ms/step - loss: 0.1689 - accuracy: 0.9987 - val_loss: 0.4761 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 4/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9987\n",
      "Epoch 4: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 35s 99ms/step - loss: 0.1689 - accuracy: 0.9987 - val_loss: 0.4761 - val_accuracy: 0.9366 - lr: 0.0010\n",
      "Epoch 5/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9990\n",
      "Epoch 5: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 26s 74ms/step - loss: 0.1684 - accuracy: 0.9990 - val_loss: 0.4760 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 6/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.9988\n",
      "Epoch 6: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 25s 73ms/step - loss: 0.1685 - accuracy: 0.9988 - val_loss: 0.4759 - val_accuracy: 0.9372 - lr: 0.0010\n",
      "Epoch 7/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9989\n",
      "Epoch 7: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 26s 74ms/step - loss: 0.1683 - accuracy: 0.9989 - val_loss: 0.4760 - val_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 8/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9987\n",
      "Epoch 8: val_loss did not improve from 0.46786\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.1689 - accuracy: 0.9987 - val_loss: 0.4759 - val_accuracy: 0.9368 - lr: 0.0010\n",
      "Current:  378\n",
      "313/313 [==============================] - 8s 17ms/step\n",
      "Accuracy: 93.07\n",
      "Error: 6.930000000000007\n",
      "ECE: 0.04833229455351825\n",
      "MCE: 0.3222891390323639\n",
      "Loss: 0.32020646726851515\n",
      "brier: 0.06022057364990712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.930000000000007,\n",
       " 0.04833229455351825,\n",
       " 0.3222891390323639,\n",
       " 0.32020646726851515,\n",
       " 0.06022057364990712]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, datagen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list, batch_size=128,lr_schedule = [[0, 0.1],[80,0.01],[150,0.001]],cbks=[checkpointer], name='resnet_cifar10_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36807.440469,
   "end_time": "2023-04-25T07:34:09.850688",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-24T21:20:42.410219",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
