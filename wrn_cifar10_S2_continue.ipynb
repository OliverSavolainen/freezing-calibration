{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca65cef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-23T21:53:16.408415Z",
     "iopub.status.busy": "2023-04-23T21:53:16.407872Z",
     "iopub.status.idle": "2023-04-23T21:53:24.106456Z",
     "shell.execute_reply": "2023-04-23T21:53:24.105293Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1682109576846,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "bpCwuvLginNe",
    "papermill": {
     "duration": 7.707679,
     "end_time": "2023-04-23T21:53:24.109231",
     "exception": false,
     "start_time": "2023-04-23T21:53:16.401552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import Input\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99b1a62",
   "metadata": {
    "_cell_guid": "d714c626-b790-420e-b76b-1f7aab627dc7",
    "_uuid": "46386508-5d86-48fc-86f1-04e2626bb2f5",
    "execution": {
     "iopub.execute_input": "2023-04-23T21:53:24.207868Z",
     "iopub.status.busy": "2023-04-23T21:53:24.207195Z",
     "iopub.status.idle": "2023-04-23T21:53:24.250020Z",
     "shell.execute_reply": "2023-04-23T21:53:24.249100Z"
    },
    "executionInfo": {
     "elapsed": 6101,
     "status": "ok",
     "timestamp": 1682109576845,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "4vOgQ5fvinNh",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.049656,
     "end_time": "2023-04-23T21:53:24.252130",
     "exception": false,
     "start_time": "2023-04-23T21:53:24.202474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import freezing\n",
    "import wrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e6233d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-23T21:53:24.296393Z",
     "iopub.status.busy": "2023-04-23T21:53:24.296124Z",
     "iopub.status.idle": "2023-04-23T21:53:24.304206Z",
     "shell.execute_reply": "2023-04-23T21:53:24.303301Z"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682109576847,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "eFW7ePv8inNn",
    "papermill": {
     "duration": 0.014604,
     "end_time": "2023-04-23T21:53:24.306222",
     "exception": false,
     "start_time": "2023-04-23T21:53:24.291618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "depth              = 34  # 32, if ignoring conv layers carrying residuals, which are needed for increasing filter size.\n",
    "growth_rate        = 10  # Growth factor\n",
    "n                  = (depth-4)//6\n",
    "num_classes        = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels       = 3\n",
    "batch_size         = 128\n",
    "epochs             = 200\n",
    "iterations         = 45000 // batch_size\n",
    "weight_decay       = 0.0005\n",
    "seed = 333\n",
    "def color_preprocessing(x_train, x_val, x_test):\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')    \n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    mean = np.mean(x_train, axis=(0,1,2))  # Per channel mean\n",
    "    std = np.std(x_train, axis=(0,1,2))\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_val = (x_val - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "    \n",
    "    return x_train, x_val, x_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc265b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-23T21:53:24.315023Z",
     "iopub.status.busy": "2023-04-23T21:53:24.314161Z",
     "iopub.status.idle": "2023-04-23T21:53:43.220099Z",
     "shell.execute_reply": "2023-04-23T21:53:43.218647Z"
    },
    "executionInfo": {
     "elapsed": 10712,
     "status": "ok",
     "timestamp": 1682109777593,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "oghEV45AinNo",
    "outputId": "a00812e9-f46c-4548-dec2-5af843f1ec31",
    "papermill": {
     "duration": 18.956796,
     "end_time": "2023-04-23T21:53:43.266571",
     "exception": false,
     "start_time": "2023-04-23T21:53:24.309775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# color preprocessing\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "x_train45, x_val, x_test = color_preprocessing(x_train45, x_val, x_test)\n",
    "\n",
    "y_train45 = keras.utils.to_categorical(y_train45, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# build network\n",
    "img_input = Input(shape=(img_rows,img_cols,img_channels))    \n",
    "model = wrn.create_wide_residual_network(img_input, nb_classes=num_classes, N=n, k=growth_rate, dropout=0.0)\n",
    "print(model.summary())\n",
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "\n",
    "# set callback\n",
    "checkpointer = ModelCheckpoint('model_cont_wide_28_10_c10_best_2.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "# set data augmentation\n",
    "print('Using real-time data augmentation.')\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "        width_shift_range=0.125,height_shift_range=0.125,fill_mode='reflect') # Missing pixels replaced with reflections\n",
    "\n",
    "datagen.fit(x_train45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8b3619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-23T21:53:43.317049Z",
     "iopub.status.busy": "2023-04-23T21:53:43.316683Z",
     "iopub.status.idle": "2023-04-23T21:53:43.340432Z",
     "shell.execute_reply": "2023-04-23T21:53:43.339332Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1682109600331,
     "user": {
      "displayName": "Oliver Savolainen",
      "userId": "11456779327234974123"
     },
     "user_tz": -180
    },
    "id": "f-lVxqBhinNp",
    "papermill": {
     "duration": 0.050791,
     "end_time": "2023-04-23T21:53:43.342655",
     "exception": false,
     "start_time": "2023-04-23T21:53:43.291864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.9:\n",
    "    freezing_list.append(int(epochs*0.6))\n",
    "  elif i < len(model.layers) * 0.98:\n",
    "    freezing_list.append(int(epochs*0.96))\n",
    "freezing_list.append(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f468be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-23T21:53:43.392703Z",
     "iopub.status.busy": "2023-04-23T21:53:43.392355Z",
     "iopub.status.idle": "2023-04-24T02:44:12.437483Z",
     "shell.execute_reply": "2023-04-24T02:44:12.436429Z"
    },
    "id": "5WwxjRTUinNp",
    "outputId": "a00ab3dd-b414-487f-9622-0896a8674e61",
    "papermill": {
     "duration": 17429.072342,
     "end_time": "2023-04-24T02:44:12.439766",
     "exception": false,
     "start_time": "2023-04-23T21:53:43.367424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-23 21:53:43        53367\n",
      "variables.h5                                   2023-04-23 21:53:44    185018824\n",
      "metadata.json                                  2023-04-23 21:53:43           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-23 21:53:42        53367\n",
      "variables.h5                                   2023-04-23 21:53:44    185018824\n",
      "metadata.json                                  2023-04-23 21:53:42           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Loaded weights\n",
      "Epoch 1/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9830\n",
      "Epoch 1: val_loss improved from inf to 0.32011, saving model to model_cont_wide_28_10_c10_best_2.hdf5\n",
      "351/351 [==============================] - 138s 369ms/step - loss: 0.0507 - accuracy: 0.9830 - val_loss: 0.3201 - val_accuracy: 0.9230 - lr: 0.1000\n",
      "Epoch 2/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9814\n",
      "Epoch 2: val_loss did not improve from 0.32011\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 0.0535 - accuracy: 0.9814 - val_loss: 0.3734 - val_accuracy: 0.9132 - lr: 0.1000\n",
      "Epoch 3/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9838\n",
      "Epoch 3: val_loss did not improve from 0.32011\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0481 - accuracy: 0.9838 - val_loss: 0.3318 - val_accuracy: 0.9208 - lr: 0.1000\n",
      "Epoch 4/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9828\n",
      "Epoch 4: val_loss improved from 0.32011 to 0.30962, saving model to model_cont_wide_28_10_c10_best_2.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0497 - accuracy: 0.9828 - val_loss: 0.3096 - val_accuracy: 0.9228 - lr: 0.1000\n",
      "Epoch 5/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9844\n",
      "Epoch 5: val_loss improved from 0.30962 to 0.28008, saving model to model_cont_wide_28_10_c10_best_2.hdf5\n",
      "351/351 [==============================] - 128s 364ms/step - loss: 0.0439 - accuracy: 0.9844 - val_loss: 0.2801 - val_accuracy: 0.9336 - lr: 0.1000\n",
      "Epoch 6/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9846\n",
      "Epoch 6: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 0.0430 - accuracy: 0.9846 - val_loss: 0.3286 - val_accuracy: 0.9258 - lr: 0.1000\n",
      "Epoch 7/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9851\n",
      "Epoch 7: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0422 - accuracy: 0.9851 - val_loss: 0.3102 - val_accuracy: 0.9284 - lr: 0.1000\n",
      "Epoch 8/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9865\n",
      "Epoch 8: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 0.0388 - accuracy: 0.9865 - val_loss: 0.3465 - val_accuracy: 0.9236 - lr: 0.1000\n",
      "Epoch 9/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9877\n",
      "Epoch 9: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 357ms/step - loss: 0.0363 - accuracy: 0.9877 - val_loss: 0.3258 - val_accuracy: 0.9276 - lr: 0.1000\n",
      "Epoch 10/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9878\n",
      "Epoch 10: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0351 - accuracy: 0.9878 - val_loss: 0.3212 - val_accuracy: 0.9288 - lr: 0.1000\n",
      "Epoch 11/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9879\n",
      "Epoch 11: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0345 - accuracy: 0.9879 - val_loss: 0.3152 - val_accuracy: 0.9300 - lr: 0.1000\n",
      "Epoch 12/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9866\n",
      "Epoch 12: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0385 - accuracy: 0.9866 - val_loss: 0.3073 - val_accuracy: 0.9278 - lr: 0.1000\n",
      "Epoch 13/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9904\n",
      "Epoch 13: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0274 - accuracy: 0.9904 - val_loss: 0.3195 - val_accuracy: 0.9278 - lr: 0.1000\n",
      "Epoch 14/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9902\n",
      "Epoch 14: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0279 - accuracy: 0.9902 - val_loss: 0.3046 - val_accuracy: 0.9252 - lr: 0.1000\n",
      "Epoch 15/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9903\n",
      "Epoch 15: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0273 - accuracy: 0.9903 - val_loss: 0.3656 - val_accuracy: 0.9234 - lr: 0.1000\n",
      "Epoch 16/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9916\n",
      "Epoch 16: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0248 - accuracy: 0.9916 - val_loss: 0.3187 - val_accuracy: 0.9330 - lr: 0.1000\n",
      "Epoch 17/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9909\n",
      "Epoch 17: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0255 - accuracy: 0.9909 - val_loss: 0.3328 - val_accuracy: 0.9294 - lr: 0.1000\n",
      "Epoch 18/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9915\n",
      "Epoch 18: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0257 - accuracy: 0.9915 - val_loss: 0.3233 - val_accuracy: 0.9294 - lr: 0.1000\n",
      "Epoch 19/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9909\n",
      "Epoch 19: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0255 - accuracy: 0.9909 - val_loss: 0.3303 - val_accuracy: 0.9260 - lr: 0.1000\n",
      "Epoch 20/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9918\n",
      "Epoch 20: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0240 - accuracy: 0.9918 - val_loss: 0.2990 - val_accuracy: 0.9336 - lr: 0.1000\n",
      "Epoch 21/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9926\n",
      "Epoch 21: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0216 - accuracy: 0.9926 - val_loss: 0.3510 - val_accuracy: 0.9264 - lr: 0.1000\n",
      "Epoch 22/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9923\n",
      "Epoch 22: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0223 - accuracy: 0.9923 - val_loss: 0.3262 - val_accuracy: 0.9364 - lr: 0.1000\n",
      "Epoch 23/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9924\n",
      "Epoch 23: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0228 - accuracy: 0.9924 - val_loss: 0.3305 - val_accuracy: 0.9312 - lr: 0.1000\n",
      "Epoch 24/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9934\n",
      "Epoch 24: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0203 - accuracy: 0.9934 - val_loss: 0.3096 - val_accuracy: 0.9342 - lr: 0.1000\n",
      "Epoch 25/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9932\n",
      "Epoch 25: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0202 - accuracy: 0.9932 - val_loss: 0.3078 - val_accuracy: 0.9358 - lr: 0.1000\n",
      "Epoch 26/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9934\n",
      "Epoch 26: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 357ms/step - loss: 0.0196 - accuracy: 0.9934 - val_loss: 0.3279 - val_accuracy: 0.9298 - lr: 0.1000\n",
      "Epoch 27/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9932\n",
      "Epoch 27: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.3243 - val_accuracy: 0.9340 - lr: 0.1000\n",
      "Epoch 28/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9945\n",
      "Epoch 28: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 0.3563 - val_accuracy: 0.9320 - lr: 0.1000\n",
      "Epoch 29/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9928\n",
      "Epoch 29: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0198 - accuracy: 0.9928 - val_loss: 0.3288 - val_accuracy: 0.9328 - lr: 0.1000\n",
      "Epoch 30/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9942\n",
      "Epoch 30: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0160 - accuracy: 0.9942 - val_loss: 0.3062 - val_accuracy: 0.9378 - lr: 0.1000\n",
      "Epoch 31/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9969\n",
      "Epoch 31: val_loss did not improve from 0.28008\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.2827 - val_accuracy: 0.9384 - lr: 0.0200\n",
      "Epoch 32/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 32: val_loss improved from 0.28008 to 0.27542, saving model to model_cont_wide_28_10_c10_best_2.hdf5\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.2754 - val_accuracy: 0.9406 - lr: 0.0200\n",
      "Epoch 33/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 33: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.2783 - val_accuracy: 0.9406 - lr: 0.0200\n",
      "Epoch 34/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 34: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.2898 - val_accuracy: 0.9418 - lr: 0.0200\n",
      "Epoch 35/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 35: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 357ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.2848 - val_accuracy: 0.9446 - lr: 0.0200\n",
      "Epoch 36/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 36: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.2847 - val_accuracy: 0.9432 - lr: 0.0200\n",
      "Epoch 37/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9993\n",
      "Epoch 37: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.3313 - val_accuracy: 0.9336 - lr: 0.0200\n",
      "Epoch 38/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 38: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.2945 - val_accuracy: 0.9416 - lr: 0.0200\n",
      "Epoch 39/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 39: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.2873 - val_accuracy: 0.9444 - lr: 0.0200\n",
      "Epoch 40/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 40: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3067 - val_accuracy: 0.9404 - lr: 0.0200\n",
      "Epoch 41/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 41: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 359ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2856 - val_accuracy: 0.9436 - lr: 0.0200\n",
      "Epoch 42/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 42: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2904 - val_accuracy: 0.9426 - lr: 0.0200\n",
      "Epoch 43/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 43: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2920 - val_accuracy: 0.9432 - lr: 0.0200\n",
      "Epoch 44/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 44: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3085 - val_accuracy: 0.9416 - lr: 0.0200\n",
      "Epoch 45/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 45: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2908 - val_accuracy: 0.9444 - lr: 0.0200\n",
      "Epoch 46/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 46: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.3021 - val_accuracy: 0.9424 - lr: 0.0200\n",
      "Epoch 47/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 47: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.3054 - val_accuracy: 0.9424 - lr: 0.0200\n",
      "Epoch 48/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 48: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2949 - val_accuracy: 0.9442 - lr: 0.0200\n",
      "Epoch 49/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 49: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2896 - val_accuracy: 0.9444 - lr: 0.0200\n",
      "Epoch 50/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.5481e-04 - accuracy: 0.9998\n",
      "Epoch 50: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 9.5481e-04 - accuracy: 0.9998 - val_loss: 0.2952 - val_accuracy: 0.9442 - lr: 0.0200\n",
      "Epoch 51/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.9954e-04 - accuracy: 0.9998\n",
      "Epoch 51: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 8.9954e-04 - accuracy: 0.9998 - val_loss: 0.3104 - val_accuracy: 0.9410 - lr: 0.0200\n",
      "Epoch 52/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 52: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 357ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2951 - val_accuracy: 0.9428 - lr: 0.0200\n",
      "Epoch 53/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.0330e-04 - accuracy: 0.9998\n",
      "Epoch 53: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 9.0330e-04 - accuracy: 0.9998 - val_loss: 0.3088 - val_accuracy: 0.9410 - lr: 0.0200\n",
      "Epoch 54/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.9821e-04 - accuracy: 0.9998\n",
      "Epoch 54: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 8.9821e-04 - accuracy: 0.9998 - val_loss: 0.2956 - val_accuracy: 0.9446 - lr: 0.0200\n",
      "Epoch 55/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 55: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.2982 - val_accuracy: 0.9444 - lr: 0.0200\n",
      "Epoch 56/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.1374e-04 - accuracy: 0.9998\n",
      "Epoch 56: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 357ms/step - loss: 9.1374e-04 - accuracy: 0.9998 - val_loss: 0.3075 - val_accuracy: 0.9424 - lr: 0.0200\n",
      "Epoch 57/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.4947e-04 - accuracy: 0.9997\n",
      "Epoch 57: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 9.4947e-04 - accuracy: 0.9997 - val_loss: 0.3316 - val_accuracy: 0.9396 - lr: 0.0200\n",
      "Epoch 58/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.8259e-04 - accuracy: 0.9998\n",
      "Epoch 58: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 7.8259e-04 - accuracy: 0.9998 - val_loss: 0.2972 - val_accuracy: 0.9440 - lr: 0.0200\n",
      "Epoch 59/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.1706e-04 - accuracy: 0.9998\n",
      "Epoch 59: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 9.1706e-04 - accuracy: 0.9998 - val_loss: 0.3092 - val_accuracy: 0.9456 - lr: 0.0200\n",
      "Epoch 60/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.6299e-04 - accuracy: 0.9998\n",
      "Epoch 60: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 7.6299e-04 - accuracy: 0.9998 - val_loss: 0.3332 - val_accuracy: 0.9386 - lr: 0.0200\n",
      "Epoch 61/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.4668e-04 - accuracy: 0.9998\n",
      "Epoch 61: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 7.4668e-04 - accuracy: 0.9998 - val_loss: 0.3152 - val_accuracy: 0.9436 - lr: 0.0200\n",
      "Epoch 62/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 62: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.2960 - val_accuracy: 0.9466 - lr: 0.0200\n",
      "Epoch 63/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9103e-04 - accuracy: 0.9999\n",
      "Epoch 63: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 7.9103e-04 - accuracy: 0.9999 - val_loss: 0.3126 - val_accuracy: 0.9440 - lr: 0.0200\n",
      "Epoch 64/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.1908e-04 - accuracy: 0.9999\n",
      "Epoch 64: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 6.1908e-04 - accuracy: 0.9999 - val_loss: 0.3118 - val_accuracy: 0.9428 - lr: 0.0200\n",
      "Epoch 65/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.4568e-04 - accuracy: 0.9998\n",
      "Epoch 65: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 8.4568e-04 - accuracy: 0.9998 - val_loss: 0.3130 - val_accuracy: 0.9450 - lr: 0.0200\n",
      "Epoch 66/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.4047e-04 - accuracy: 0.9999\n",
      "Epoch 66: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 5.4047e-04 - accuracy: 0.9999 - val_loss: 0.3070 - val_accuracy: 0.9420 - lr: 0.0200\n",
      "Epoch 67/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.8327e-04 - accuracy: 0.9999\n",
      "Epoch 67: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 6.8327e-04 - accuracy: 0.9999 - val_loss: 0.3252 - val_accuracy: 0.9414 - lr: 0.0200\n",
      "Epoch 68/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.8234e-04 - accuracy: 0.9999\n",
      "Epoch 68: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 6.8234e-04 - accuracy: 0.9999 - val_loss: 0.3247 - val_accuracy: 0.9436 - lr: 0.0200\n",
      "Epoch 69/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.3390e-04 - accuracy: 0.9999\n",
      "Epoch 69: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 6.3390e-04 - accuracy: 0.9999 - val_loss: 0.3133 - val_accuracy: 0.9434 - lr: 0.0200\n",
      "Epoch 70/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 9.0150e-04 - accuracy: 0.9997\n",
      "Epoch 70: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 9.0150e-04 - accuracy: 0.9997 - val_loss: 0.3116 - val_accuracy: 0.9436 - lr: 0.0200\n",
      "Epoch 71/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.4186e-04 - accuracy: 0.9998\n",
      "Epoch 71: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 362ms/step - loss: 6.4186e-04 - accuracy: 0.9998 - val_loss: 0.3011 - val_accuracy: 0.9460 - lr: 0.0200\n",
      "Epoch 72/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.2906e-04 - accuracy: 0.9998\n",
      "Epoch 72: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 357ms/step - loss: 8.2906e-04 - accuracy: 0.9998 - val_loss: 0.3172 - val_accuracy: 0.9404 - lr: 0.0200\n",
      "Epoch 73/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.3118e-04 - accuracy: 0.9998\n",
      "Epoch 73: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 357ms/step - loss: 7.3118e-04 - accuracy: 0.9998 - val_loss: 0.3220 - val_accuracy: 0.9420 - lr: 0.0200\n",
      "Epoch 74/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.0120e-04 - accuracy: 0.9998\n",
      "Epoch 74: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 7.0120e-04 - accuracy: 0.9998 - val_loss: 0.3275 - val_accuracy: 0.9454 - lr: 0.0200\n",
      "Epoch 75/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.5760e-04 - accuracy: 0.9999\n",
      "Epoch 75: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 5.5760e-04 - accuracy: 0.9999 - val_loss: 0.3551 - val_accuracy: 0.9400 - lr: 0.0200\n",
      "Epoch 76/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.3156e-04 - accuracy: 1.0000\n",
      "Epoch 76: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 5.3156e-04 - accuracy: 1.0000 - val_loss: 0.3203 - val_accuracy: 0.9436 - lr: 0.0200\n",
      "Epoch 77/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.1034e-04 - accuracy: 0.9999\n",
      "Epoch 77: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 5.1034e-04 - accuracy: 0.9999 - val_loss: 0.3173 - val_accuracy: 0.9448 - lr: 0.0200\n",
      "Epoch 78/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.6999e-04 - accuracy: 0.9999\n",
      "Epoch 78: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 6.6999e-04 - accuracy: 0.9999 - val_loss: 0.3176 - val_accuracy: 0.9432 - lr: 0.0200\n",
      "Epoch 79/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 7.9847e-04 - accuracy: 0.9998\n",
      "Epoch 79: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 7.9847e-04 - accuracy: 0.9998 - val_loss: 0.3134 - val_accuracy: 0.9434 - lr: 0.0200\n",
      "Epoch 80/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.9798e-04 - accuracy: 0.9999\n",
      "Epoch 80: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 5.9798e-04 - accuracy: 0.9999 - val_loss: 0.3297 - val_accuracy: 0.9440 - lr: 0.0200\n",
      "Epoch 81/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.6930e-04 - accuracy: 0.9999\n",
      "Epoch 81: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 360ms/step - loss: 5.6930e-04 - accuracy: 0.9999 - val_loss: 0.3108 - val_accuracy: 0.9462 - lr: 0.0200\n",
      "Epoch 82/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.1130e-04 - accuracy: 0.9999\n",
      "Epoch 82: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 5.1130e-04 - accuracy: 0.9999 - val_loss: 0.3151 - val_accuracy: 0.9466 - lr: 0.0200\n",
      "Epoch 83/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 8.0587e-04 - accuracy: 0.9998\n",
      "Epoch 83: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 361ms/step - loss: 8.0587e-04 - accuracy: 0.9998 - val_loss: 0.3185 - val_accuracy: 0.9442 - lr: 0.0200\n",
      "Epoch 84/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.3496e-04 - accuracy: 0.9998\n",
      "Epoch 84: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 5.3496e-04 - accuracy: 0.9998 - val_loss: 0.3122 - val_accuracy: 0.9444 - lr: 0.0200\n",
      "Epoch 85/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 6.2954e-04 - accuracy: 0.9998\n",
      "Epoch 85: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 6.2954e-04 - accuracy: 0.9998 - val_loss: 0.3226 - val_accuracy: 0.9440 - lr: 0.0200\n",
      "Epoch 86/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.3285e-04 - accuracy: 0.9999\n",
      "Epoch 86: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 4.3285e-04 - accuracy: 0.9999 - val_loss: 0.3142 - val_accuracy: 0.9436 - lr: 0.0200\n",
      "Epoch 87/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8172e-04 - accuracy: 0.9999\n",
      "Epoch 87: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 125s 357ms/step - loss: 3.8172e-04 - accuracy: 0.9999 - val_loss: 0.3094 - val_accuracy: 0.9464 - lr: 0.0200\n",
      "Epoch 88/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.3598e-04 - accuracy: 0.9999\n",
      "Epoch 88: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 5.3598e-04 - accuracy: 0.9999 - val_loss: 0.3103 - val_accuracy: 0.9466 - lr: 0.0200\n",
      "Epoch 89/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.7738e-04 - accuracy: 1.0000\n",
      "Epoch 89: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 127s 360ms/step - loss: 3.7738e-04 - accuracy: 1.0000 - val_loss: 0.3429 - val_accuracy: 0.9396 - lr: 0.0200\n",
      "Epoch 90/90\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.7938e-04 - accuracy: 0.9999\n",
      "Epoch 90: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 126s 358ms/step - loss: 5.7938e-04 - accuracy: 0.9999 - val_loss: 0.3193 - val_accuracy: 0.9440 - lr: 0.0200\n",
      "Epoch 1/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1440e-04 - accuracy: 0.9999\n",
      "Epoch 1: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 52s 143ms/step - loss: 3.1440e-04 - accuracy: 0.9999 - val_loss: 0.3151 - val_accuracy: 0.9448 - lr: 0.0040\n",
      "Epoch 2/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.8338e-04 - accuracy: 0.9999\n",
      "Epoch 2: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 4.8338e-04 - accuracy: 0.9999 - val_loss: 0.3114 - val_accuracy: 0.9448 - lr: 0.0040\n",
      "Epoch 3/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8379e-04 - accuracy: 0.9999\n",
      "Epoch 3: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 3.8379e-04 - accuracy: 0.9999 - val_loss: 0.3195 - val_accuracy: 0.9444 - lr: 0.0040\n",
      "Epoch 4/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8552e-04 - accuracy: 0.9999\n",
      "Epoch 4: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 3.8552e-04 - accuracy: 0.9999 - val_loss: 0.3084 - val_accuracy: 0.9466 - lr: 0.0040\n",
      "Epoch 5/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8631e-04 - accuracy: 0.9999\n",
      "Epoch 5: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 3.8631e-04 - accuracy: 0.9999 - val_loss: 0.3176 - val_accuracy: 0.9452 - lr: 0.0040\n",
      "Epoch 6/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.5589e-04 - accuracy: 0.9998\n",
      "Epoch 6: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 5.5589e-04 - accuracy: 0.9998 - val_loss: 0.3213 - val_accuracy: 0.9444 - lr: 0.0040\n",
      "Epoch 7/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.2737e-04 - accuracy: 0.9998\n",
      "Epoch 7: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 4.2737e-04 - accuracy: 0.9998 - val_loss: 0.3198 - val_accuracy: 0.9444 - lr: 0.0040\n",
      "Epoch 8/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.6371e-04 - accuracy: 0.9998\n",
      "Epoch 8: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 4.6371e-04 - accuracy: 0.9998 - val_loss: 0.3203 - val_accuracy: 0.9440 - lr: 0.0040\n",
      "Epoch 9/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6215e-04 - accuracy: 0.9999\n",
      "Epoch 9: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 3.6215e-04 - accuracy: 0.9999 - val_loss: 0.3161 - val_accuracy: 0.9466 - lr: 0.0040\n",
      "Epoch 10/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1530e-04 - accuracy: 1.0000\n",
      "Epoch 10: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 3.1530e-04 - accuracy: 1.0000 - val_loss: 0.3177 - val_accuracy: 0.9452 - lr: 0.0040\n",
      "Epoch 11/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.4465e-04 - accuracy: 0.9999\n",
      "Epoch 11: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 4.4465e-04 - accuracy: 0.9999 - val_loss: 0.3143 - val_accuracy: 0.9450 - lr: 0.0040\n",
      "Epoch 12/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1528e-04 - accuracy: 0.9999\n",
      "Epoch 12: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 3.1528e-04 - accuracy: 0.9999 - val_loss: 0.3188 - val_accuracy: 0.9438 - lr: 0.0040\n",
      "Epoch 13/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4236e-04 - accuracy: 0.9999\n",
      "Epoch 13: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 3.4236e-04 - accuracy: 0.9999 - val_loss: 0.3321 - val_accuracy: 0.9440 - lr: 0.0040\n",
      "Epoch 14/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.5922e-04 - accuracy: 1.0000\n",
      "Epoch 14: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 2.5922e-04 - accuracy: 1.0000 - val_loss: 0.3198 - val_accuracy: 0.9458 - lr: 0.0040\n",
      "Epoch 15/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.3227e-04 - accuracy: 0.9999\n",
      "Epoch 15: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 134ms/step - loss: 4.3227e-04 - accuracy: 0.9999 - val_loss: 0.3231 - val_accuracy: 0.9454 - lr: 0.0040\n",
      "Epoch 16/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8892e-04 - accuracy: 1.0000\n",
      "Epoch 16: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 3.8892e-04 - accuracy: 1.0000 - val_loss: 0.3166 - val_accuracy: 0.9456 - lr: 0.0040\n",
      "Epoch 17/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2427e-04 - accuracy: 1.0000\n",
      "Epoch 17: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 3.2427e-04 - accuracy: 1.0000 - val_loss: 0.3163 - val_accuracy: 0.9454 - lr: 0.0040\n",
      "Epoch 18/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9412e-04 - accuracy: 1.0000\n",
      "Epoch 18: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 2.9412e-04 - accuracy: 1.0000 - val_loss: 0.3259 - val_accuracy: 0.9442 - lr: 0.0040\n",
      "Epoch 19/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9913e-04 - accuracy: 1.0000\n",
      "Epoch 19: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 52s 149ms/step - loss: 2.9913e-04 - accuracy: 1.0000 - val_loss: 0.3189 - val_accuracy: 0.9460 - lr: 0.0040\n",
      "Epoch 20/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4868e-04 - accuracy: 1.0000\n",
      "Epoch 20: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 2.4868e-04 - accuracy: 1.0000 - val_loss: 0.3283 - val_accuracy: 0.9424 - lr: 0.0040\n",
      "Epoch 21/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9109e-04 - accuracy: 0.9999\n",
      "Epoch 21: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 2.9109e-04 - accuracy: 0.9999 - val_loss: 0.3194 - val_accuracy: 0.9462 - lr: 0.0040\n",
      "Epoch 22/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.8752e-04 - accuracy: 1.0000\n",
      "Epoch 22: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 2.8752e-04 - accuracy: 1.0000 - val_loss: 0.3194 - val_accuracy: 0.9446 - lr: 0.0040\n",
      "Epoch 23/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.6446e-04 - accuracy: 1.0000\n",
      "Epoch 23: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 2.6446e-04 - accuracy: 1.0000 - val_loss: 0.3243 - val_accuracy: 0.9448 - lr: 0.0040\n",
      "Epoch 24/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3981e-04 - accuracy: 1.0000\n",
      "Epoch 24: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 2.3981e-04 - accuracy: 1.0000 - val_loss: 0.3192 - val_accuracy: 0.9462 - lr: 0.0040\n",
      "Epoch 25/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1715e-04 - accuracy: 1.0000\n",
      "Epoch 25: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 3.1715e-04 - accuracy: 1.0000 - val_loss: 0.3192 - val_accuracy: 0.9444 - lr: 0.0040\n",
      "Epoch 26/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6625e-04 - accuracy: 0.9999\n",
      "Epoch 26: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 3.6625e-04 - accuracy: 0.9999 - val_loss: 0.3308 - val_accuracy: 0.9426 - lr: 0.0040\n",
      "Epoch 27/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 5.1885e-04 - accuracy: 0.9999\n",
      "Epoch 27: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 5.1885e-04 - accuracy: 0.9999 - val_loss: 0.3286 - val_accuracy: 0.9428 - lr: 0.0040\n",
      "Epoch 28/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.0959e-04 - accuracy: 0.9999\n",
      "Epoch 28: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 47s 135ms/step - loss: 3.0959e-04 - accuracy: 0.9999 - val_loss: 0.3274 - val_accuracy: 0.9452 - lr: 0.0040\n",
      "Epoch 29/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.7789e-04 - accuracy: 1.0000\n",
      "Epoch 29: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 3.7789e-04 - accuracy: 1.0000 - val_loss: 0.3204 - val_accuracy: 0.9444 - lr: 0.0040\n",
      "Epoch 30/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.9826e-04 - accuracy: 0.9999\n",
      "Epoch 30: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 3.9826e-04 - accuracy: 0.9999 - val_loss: 0.3243 - val_accuracy: 0.9448 - lr: 0.0040\n",
      "Epoch 31/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1360e-04 - accuracy: 0.9999\n",
      "Epoch 31: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 3.1360e-04 - accuracy: 0.9999 - val_loss: 0.3151 - val_accuracy: 0.9448 - lr: 0.0040\n",
      "Epoch 32/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.2436e-04 - accuracy: 0.9999\n",
      "Epoch 32: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 4.2436e-04 - accuracy: 0.9999 - val_loss: 0.3139 - val_accuracy: 0.9458 - lr: 0.0040\n",
      "Epoch 33/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4324e-04 - accuracy: 0.9999\n",
      "Epoch 33: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 136ms/step - loss: 3.4324e-04 - accuracy: 0.9999 - val_loss: 0.3230 - val_accuracy: 0.9446 - lr: 0.0040\n",
      "Epoch 34/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4723e-04 - accuracy: 0.9999\n",
      "Epoch 34: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 53s 150ms/step - loss: 3.4723e-04 - accuracy: 0.9999 - val_loss: 0.3251 - val_accuracy: 0.9454 - lr: 0.0040\n",
      "Epoch 35/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5280e-04 - accuracy: 0.9999\n",
      "Epoch 35: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 135ms/step - loss: 3.5280e-04 - accuracy: 0.9999 - val_loss: 0.3273 - val_accuracy: 0.9438 - lr: 0.0040\n",
      "Epoch 36/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2819e-04 - accuracy: 0.9999\n",
      "Epoch 36: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 50s 142ms/step - loss: 3.2819e-04 - accuracy: 0.9999 - val_loss: 0.3170 - val_accuracy: 0.9442 - lr: 0.0040\n",
      "Epoch 37/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9933e-04 - accuracy: 1.0000\n",
      "Epoch 37: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 2.9933e-04 - accuracy: 1.0000 - val_loss: 0.3216 - val_accuracy: 0.9438 - lr: 0.0040\n",
      "Epoch 38/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.7845e-04 - accuracy: 1.0000\n",
      "Epoch 38: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 2.7845e-04 - accuracy: 1.0000 - val_loss: 0.3295 - val_accuracy: 0.9458 - lr: 0.0040\n",
      "Epoch 39/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.7383e-04 - accuracy: 1.0000\n",
      "Epoch 39: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 2.7383e-04 - accuracy: 1.0000 - val_loss: 0.3274 - val_accuracy: 0.9440 - lr: 0.0040\n",
      "Epoch 40/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8814e-04 - accuracy: 0.9999\n",
      "Epoch 40: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 3.8814e-04 - accuracy: 0.9999 - val_loss: 0.3143 - val_accuracy: 0.9456 - lr: 0.0040\n",
      "Epoch 41/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.4283e-04 - accuracy: 0.9999\n",
      "Epoch 41: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 4.4283e-04 - accuracy: 0.9999 - val_loss: 0.3265 - val_accuracy: 0.9458 - lr: 8.0000e-04\n",
      "Epoch 42/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.3284e-04 - accuracy: 1.0000\n",
      "Epoch 42: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 3.3284e-04 - accuracy: 1.0000 - val_loss: 0.3260 - val_accuracy: 0.9442 - lr: 8.0000e-04\n",
      "Epoch 43/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1953e-04 - accuracy: 1.0000\n",
      "Epoch 43: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 3.1953e-04 - accuracy: 1.0000 - val_loss: 0.3178 - val_accuracy: 0.9440 - lr: 8.0000e-04\n",
      "Epoch 44/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1041e-04 - accuracy: 1.0000\n",
      "Epoch 44: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 3.1041e-04 - accuracy: 1.0000 - val_loss: 0.3182 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Epoch 45/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6903e-04 - accuracy: 0.9999\n",
      "Epoch 45: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 53s 151ms/step - loss: 3.6903e-04 - accuracy: 0.9999 - val_loss: 0.3192 - val_accuracy: 0.9434 - lr: 8.0000e-04\n",
      "Epoch 46/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5772e-04 - accuracy: 0.9999\n",
      "Epoch 46: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 138ms/step - loss: 3.5772e-04 - accuracy: 0.9999 - val_loss: 0.3251 - val_accuracy: 0.9442 - lr: 8.0000e-04\n",
      "Epoch 47/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2908e-04 - accuracy: 0.9999\n",
      "Epoch 47: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 3.2908e-04 - accuracy: 0.9999 - val_loss: 0.3268 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Epoch 48/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.7545e-04 - accuracy: 0.9999\n",
      "Epoch 48: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 3.7545e-04 - accuracy: 0.9999 - val_loss: 0.3265 - val_accuracy: 0.9440 - lr: 8.0000e-04\n",
      "Epoch 49/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6494e-04 - accuracy: 0.9999\n",
      "Epoch 49: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 152ms/step - loss: 3.6494e-04 - accuracy: 0.9999 - val_loss: 0.3171 - val_accuracy: 0.9442 - lr: 8.0000e-04\n",
      "Epoch 50/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.7162e-04 - accuracy: 0.9999\n",
      "Epoch 50: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 4.7162e-04 - accuracy: 0.9999 - val_loss: 0.3162 - val_accuracy: 0.9466 - lr: 8.0000e-04\n",
      "Epoch 51/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.8518e-04 - accuracy: 1.0000\n",
      "Epoch 51: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 2.8518e-04 - accuracy: 1.0000 - val_loss: 0.3193 - val_accuracy: 0.9458 - lr: 8.0000e-04\n",
      "Epoch 52/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9836e-04 - accuracy: 1.0000\n",
      "Epoch 52: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 2.9836e-04 - accuracy: 1.0000 - val_loss: 0.3188 - val_accuracy: 0.9442 - lr: 8.0000e-04\n",
      "Epoch 53/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3871e-04 - accuracy: 1.0000\n",
      "Epoch 53: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 155ms/step - loss: 2.3871e-04 - accuracy: 1.0000 - val_loss: 0.3178 - val_accuracy: 0.9434 - lr: 8.0000e-04\n",
      "Epoch 54/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.9059e-04 - accuracy: 0.9999\n",
      "Epoch 54: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 152ms/step - loss: 3.9059e-04 - accuracy: 0.9999 - val_loss: 0.3210 - val_accuracy: 0.9448 - lr: 8.0000e-04\n",
      "Epoch 55/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1995e-04 - accuracy: 1.0000\n",
      "Epoch 55: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 137ms/step - loss: 3.1995e-04 - accuracy: 1.0000 - val_loss: 0.3176 - val_accuracy: 0.9440 - lr: 8.0000e-04\n",
      "Epoch 56/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.6831e-04 - accuracy: 1.0000\n",
      "Epoch 56: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 2.6831e-04 - accuracy: 1.0000 - val_loss: 0.3299 - val_accuracy: 0.9450 - lr: 8.0000e-04\n",
      "Epoch 57/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5953e-04 - accuracy: 0.9999\n",
      "Epoch 57: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 53s 152ms/step - loss: 3.5953e-04 - accuracy: 0.9999 - val_loss: 0.3198 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Epoch 58/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.5140e-04 - accuracy: 0.9999\n",
      "Epoch 58: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 55s 155ms/step - loss: 4.5140e-04 - accuracy: 0.9999 - val_loss: 0.3446 - val_accuracy: 0.9420 - lr: 8.0000e-04\n",
      "Epoch 59/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1601e-04 - accuracy: 1.0000\n",
      "Epoch 59: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 152ms/step - loss: 3.1601e-04 - accuracy: 1.0000 - val_loss: 0.3243 - val_accuracy: 0.9450 - lr: 8.0000e-04\n",
      "Epoch 60/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8373e-04 - accuracy: 0.9999\n",
      "Epoch 60: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 3.8373e-04 - accuracy: 0.9999 - val_loss: 0.3310 - val_accuracy: 0.9420 - lr: 8.0000e-04\n",
      "Epoch 61/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.2181e-04 - accuracy: 0.9999\n",
      "Epoch 61: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 50s 141ms/step - loss: 3.2181e-04 - accuracy: 0.9999 - val_loss: 0.3184 - val_accuracy: 0.9448 - lr: 8.0000e-04\n",
      "Epoch 62/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.8905e-04 - accuracy: 1.0000\n",
      "Epoch 62: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 140ms/step - loss: 2.8905e-04 - accuracy: 1.0000 - val_loss: 0.3200 - val_accuracy: 0.9456 - lr: 8.0000e-04\n",
      "Epoch 63/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4937e-04 - accuracy: 1.0000\n",
      "Epoch 63: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 141ms/step - loss: 3.4937e-04 - accuracy: 1.0000 - val_loss: 0.3212 - val_accuracy: 0.9458 - lr: 8.0000e-04\n",
      "Epoch 64/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9914e-04 - accuracy: 1.0000\n",
      "Epoch 64: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 2.9914e-04 - accuracy: 1.0000 - val_loss: 0.3238 - val_accuracy: 0.9466 - lr: 8.0000e-04\n",
      "Epoch 65/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1567e-04 - accuracy: 0.9999\n",
      "Epoch 65: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 141ms/step - loss: 4.1567e-04 - accuracy: 0.9999 - val_loss: 0.3239 - val_accuracy: 0.9440 - lr: 8.0000e-04\n",
      "Epoch 66/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1785e-04 - accuracy: 1.0000\n",
      "Epoch 66: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 138ms/step - loss: 3.1785e-04 - accuracy: 1.0000 - val_loss: 0.3266 - val_accuracy: 0.9450 - lr: 8.0000e-04\n",
      "Epoch 67/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.8900e-04 - accuracy: 1.0000\n",
      "Epoch 67: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 2.8900e-04 - accuracy: 1.0000 - val_loss: 0.3273 - val_accuracy: 0.9438 - lr: 8.0000e-04\n",
      "Epoch 68/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.7738e-04 - accuracy: 1.0000\n",
      "Epoch 68: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 153ms/step - loss: 2.7738e-04 - accuracy: 1.0000 - val_loss: 0.3157 - val_accuracy: 0.9448 - lr: 8.0000e-04\n",
      "Epoch 69/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4006e-04 - accuracy: 0.9999\n",
      "Epoch 69: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 49s 139ms/step - loss: 3.4006e-04 - accuracy: 0.9999 - val_loss: 0.3371 - val_accuracy: 0.9422 - lr: 8.0000e-04\n",
      "Epoch 70/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.9541e-04 - accuracy: 0.9999\n",
      "Epoch 70: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 3.9541e-04 - accuracy: 0.9999 - val_loss: 0.3202 - val_accuracy: 0.9440 - lr: 8.0000e-04\n",
      "Epoch 71/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.6449e-04 - accuracy: 0.9999\n",
      "Epoch 71: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 54s 154ms/step - loss: 4.6449e-04 - accuracy: 0.9999 - val_loss: 0.3207 - val_accuracy: 0.9440 - lr: 8.0000e-04\n",
      "Epoch 72/72\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.7657e-04 - accuracy: 0.9999\n",
      "Epoch 72: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 53s 151ms/step - loss: 4.7657e-04 - accuracy: 0.9999 - val_loss: 0.3161 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Current:  103\n",
      "Epoch 1/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.8837e-04 - accuracy: 0.9999\n",
      "Epoch 1: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 48s 133ms/step - loss: 2.8837e-04 - accuracy: 0.9999 - val_loss: 0.3161 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Epoch 2/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1503e-04 - accuracy: 1.0000\n",
      "Epoch 2: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 50s 144ms/step - loss: 3.1503e-04 - accuracy: 1.0000 - val_loss: 0.3160 - val_accuracy: 0.9444 - lr: 8.0000e-04\n",
      "Epoch 3/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9374e-04 - accuracy: 1.0000\n",
      "Epoch 3: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 51s 146ms/step - loss: 2.9374e-04 - accuracy: 1.0000 - val_loss: 0.3160 - val_accuracy: 0.9444 - lr: 8.0000e-04\n",
      "Epoch 4/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4125e-04 - accuracy: 1.0000\n",
      "Epoch 4: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 46s 131ms/step - loss: 3.4125e-04 - accuracy: 1.0000 - val_loss: 0.3159 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Epoch 5/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4771e-04 - accuracy: 1.0000\n",
      "Epoch 5: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 50s 144ms/step - loss: 2.4771e-04 - accuracy: 1.0000 - val_loss: 0.3159 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Epoch 6/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.6879e-04 - accuracy: 1.0000\n",
      "Epoch 6: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 51s 144ms/step - loss: 2.6879e-04 - accuracy: 1.0000 - val_loss: 0.3158 - val_accuracy: 0.9446 - lr: 8.0000e-04\n",
      "Epoch 7/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.4384e-04 - accuracy: 1.0000\n",
      "Epoch 7: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 51s 144ms/step - loss: 3.4384e-04 - accuracy: 1.0000 - val_loss: 0.3158 - val_accuracy: 0.9444 - lr: 8.0000e-04\n",
      "Epoch 8/8\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.6698e-04 - accuracy: 1.0000\n",
      "Epoch 8: val_loss did not improve from 0.27542\n",
      "351/351 [==============================] - 51s 146ms/step - loss: 3.6698e-04 - accuracy: 1.0000 - val_loss: 0.3157 - val_accuracy: 0.9444 - lr: 8.0000e-04\n",
      "Current:  112\n",
      "313/313 [==============================] - 11s 32ms/step\n",
      "Accuracy: 93.96\n",
      "Error: 6.040000000000006\n",
      "ECE: 0.044643579581379905\n",
      "MCE: 0.35186373194058734\n",
      "Loss: 0.34076034955963314\n",
      "brier: 0.05374559426638748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.040000000000006,\n",
       " 0.044643579581379905,\n",
       " 0.35186373194058734,\n",
       " 0.34076034955963314,\n",
       " 0.05374559426638748]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.continue_training(model, datagen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list,lr_schedule = [[0, 0.1],[epochs*0.3,0.02],[epochs*0.6,0.004],[epochs*0.8,0.0008]],cbks=[checkpointer], all_epochs=30,weights='/kaggle/input/weights/resnet_wide_cifar10_2_30 (1).h5', name='resnet_wide_cifar10_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17471.47793,
   "end_time": "2023-04-24T02:44:19.054660",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-23T21:53:07.576730",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
