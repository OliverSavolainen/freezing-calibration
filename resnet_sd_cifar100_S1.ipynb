{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a46b33-c975-47bd-b1f6-003f65dfe9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d0067-996e-4f7d-ac57-e4aa0b275fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet_sd\n",
    "import freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0d43c3-74f3-4473-82b4-bef0baace9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per channel mean and std normalization\n",
    "def color_preprocessing(x_train, x_val, x_test):\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')    \n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    mean = np.mean(x_train, axis=(0,1,2))  # Per channel mean\n",
    "    std = np.std(x_train, axis=(0,1,2))\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_val = (x_val - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "    \n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57207f36-e520-4d19-898c-7067f7f19cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "nb_epochs = 500\n",
    "batch_size = 128\n",
    "nb_classes = 100\n",
    "seed = 333\n",
    "\n",
    "\n",
    "# data\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "\n",
    "# Data splitting (get additional 5k validation set)\n",
    "# Sklearn to split\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "x_train45, x_val, x_test = color_preprocessing(x_train45, x_val, x_test)\n",
    "\n",
    "\n",
    "img_gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    data_format=\"channels_last\",\n",
    "    width_shift_range=0.125,  # 0.125*32 = 4 so max padding of 4 pixels, as described in paper.\n",
    "    height_shift_range=0.125,\n",
    "    fill_mode=\"constant\",\n",
    "    cval = 0\n",
    ")\n",
    "\n",
    "img_gen.fit(x_train45)\n",
    "\n",
    "y_train45 = np_utils.to_categorical(y_train45, nb_classes)  # 1-hot vector\n",
    "y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "# building and training net\n",
    "model = resnet_sd.resnet_sd_model(img_shape = (32,32), img_channels = 3,\n",
    "                        layers = 110, nb_classes = nb_classes, verbose = True)\n",
    "\n",
    "sgd = SGD(learning_rate=0.1, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "checkpointer = ModelCheckpoint('model_resnet110SD_c100_best.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a72167-bda9-4925-b114-0c7261756e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.8:\n",
    "    freezing_list.append(int(nb_epochs*0.6))\n",
    "freezing_list.append(nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b445ff8-db75-4999-9160-ae73916b289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......average_pooling2d_2\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......lambda\n",
      ".........vars\n",
      "......lambda_1\n",
      ".........vars\n",
      "......lambda_10\n",
      ".........vars\n",
      "......lambda_11\n",
      ".........vars\n",
      "......lambda_12\n",
      ".........vars\n",
      "......lambda_13\n",
      ".........vars\n",
      "......lambda_14\n",
      ".........vars\n",
      "......lambda_15\n",
      ".........vars\n",
      "......lambda_16\n",
      ".........vars\n",
      "......lambda_17\n",
      ".........vars\n",
      "......lambda_18\n",
      ".........vars\n",
      "......lambda_19\n",
      ".........vars\n",
      "......lambda_2\n",
      ".........vars\n",
      "......lambda_20\n",
      ".........vars\n",
      "......lambda_21\n",
      ".........vars\n",
      "......lambda_22\n",
      ".........vars\n",
      "......lambda_23\n",
      ".........vars\n",
      "......lambda_24\n",
      ".........vars\n",
      "......lambda_25\n",
      ".........vars\n",
      "......lambda_26\n",
      ".........vars\n",
      "......lambda_27\n",
      ".........vars\n",
      "......lambda_28\n",
      ".........vars\n",
      "......lambda_29\n",
      ".........vars\n",
      "......lambda_3\n",
      ".........vars\n",
      "......lambda_30\n",
      ".........vars\n",
      "......lambda_31\n",
      ".........vars\n",
      "......lambda_32\n",
      ".........vars\n",
      "......lambda_33\n",
      ".........vars\n",
      "......lambda_34\n",
      ".........vars\n",
      "......lambda_35\n",
      ".........vars\n",
      "......lambda_36\n",
      ".........vars\n",
      "......lambda_37\n",
      ".........vars\n",
      "......lambda_38\n",
      ".........vars\n",
      "......lambda_39\n",
      ".........vars\n",
      "......lambda_4\n",
      ".........vars\n",
      "......lambda_40\n",
      ".........vars\n",
      "......lambda_41\n",
      ".........vars\n",
      "......lambda_42\n",
      ".........vars\n",
      "......lambda_43\n",
      ".........vars\n",
      "......lambda_44\n",
      ".........vars\n",
      "......lambda_45\n",
      ".........vars\n",
      "......lambda_46\n",
      ".........vars\n",
      "......lambda_47\n",
      ".........vars\n",
      "......lambda_48\n",
      ".........vars\n",
      "......lambda_49\n",
      ".........vars\n",
      "......lambda_5\n",
      ".........vars\n",
      "......lambda_50\n",
      ".........vars\n",
      "......lambda_51\n",
      ".........vars\n",
      "......lambda_52\n",
      ".........vars\n",
      "......lambda_53\n",
      ".........vars\n",
      "......lambda_54\n",
      ".........vars\n",
      "......lambda_55\n",
      ".........vars\n",
      "......lambda_6\n",
      ".........vars\n",
      "......lambda_7\n",
      ".........vars\n",
      "......lambda_8\n",
      ".........vars\n",
      "......lambda_9\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-20 11:12:56           64\n",
      "config.json                                    2023-04-20 11:12:56       210071\n",
      "variables.h5                                   2023-04-20 11:12:57      7954080\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-04-20 11:12:56           64\n",
      "config.json                                    2023-04-20 11:12:56       210071\n",
      "variables.h5                                   2023-04-20 11:12:56      7954080\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_39\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_40\n",
      ".........vars\n",
      "......activation_41\n",
      ".........vars\n",
      "......activation_42\n",
      ".........vars\n",
      "......activation_43\n",
      ".........vars\n",
      "......activation_44\n",
      ".........vars\n",
      "......activation_45\n",
      ".........vars\n",
      "......activation_46\n",
      ".........vars\n",
      "......activation_47\n",
      ".........vars\n",
      "......activation_48\n",
      ".........vars\n",
      "......activation_49\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_50\n",
      ".........vars\n",
      "......activation_51\n",
      ".........vars\n",
      "......activation_52\n",
      ".........vars\n",
      "......activation_53\n",
      ".........vars\n",
      "......activation_54\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......add\n",
      ".........vars\n",
      "......add_1\n",
      ".........vars\n",
      "......add_10\n",
      ".........vars\n",
      "......add_11\n",
      ".........vars\n",
      "......add_12\n",
      ".........vars\n",
      "......add_13\n",
      ".........vars\n",
      "......add_14\n",
      ".........vars\n",
      "......add_15\n",
      ".........vars\n",
      "......add_16\n",
      ".........vars\n",
      "......add_17\n",
      ".........vars\n",
      "......add_18\n",
      ".........vars\n",
      "......add_19\n",
      ".........vars\n",
      "......add_2\n",
      ".........vars\n",
      "......add_20\n",
      ".........vars\n",
      "......add_21\n",
      ".........vars\n",
      "......add_22\n",
      ".........vars\n",
      "......add_23\n",
      ".........vars\n",
      "......add_24\n",
      ".........vars\n",
      "......add_25\n",
      ".........vars\n",
      "......add_26\n",
      ".........vars\n",
      "......add_27\n",
      ".........vars\n",
      "......add_28\n",
      ".........vars\n",
      "......add_29\n",
      ".........vars\n",
      "......add_3\n",
      ".........vars\n",
      "......add_30\n",
      ".........vars\n",
      "......add_31\n",
      ".........vars\n",
      "......add_32\n",
      ".........vars\n",
      "......add_33\n",
      ".........vars\n",
      "......add_34\n",
      ".........vars\n",
      "......add_35\n",
      ".........vars\n",
      "......add_36\n",
      ".........vars\n",
      "......add_37\n",
      ".........vars\n",
      "......add_38\n",
      ".........vars\n",
      "......add_39\n",
      ".........vars\n",
      "......add_4\n",
      ".........vars\n",
      "......add_40\n",
      ".........vars\n",
      "......add_41\n",
      ".........vars\n",
      "......add_42\n",
      ".........vars\n",
      "......add_43\n",
      ".........vars\n",
      "......add_44\n",
      ".........vars\n",
      "......add_45\n",
      ".........vars\n",
      "......add_46\n",
      ".........vars\n",
      "......add_47\n",
      ".........vars\n",
      "......add_48\n",
      ".........vars\n",
      "......add_49\n",
      ".........vars\n",
      "......add_5\n",
      ".........vars\n",
      "......add_50\n",
      ".........vars\n",
      "......add_51\n",
      ".........vars\n",
      "......add_52\n",
      ".........vars\n",
      "......add_53\n",
      ".........vars\n",
      "......add_6\n",
      ".........vars\n",
      "......add_7\n",
      ".........vars\n",
      "......add_8\n",
      ".........vars\n",
      "......add_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......average_pooling2d_2\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_100\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_101\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_102\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_103\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_104\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_105\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_106\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_107\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_108\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_39\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_40\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_41\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_42\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_43\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_44\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_45\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_46\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_47\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_48\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_49\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_50\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_51\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_52\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_53\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_54\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_55\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_56\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_57\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_58\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_59\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_60\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_61\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_62\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_63\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_64\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_65\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_66\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_67\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_68\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_69\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_70\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_71\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_72\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_73\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_74\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_75\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_76\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_77\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_78\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_79\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_80\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_81\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_82\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_83\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_84\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_85\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_86\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_87\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_88\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_89\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_90\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_91\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_92\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_93\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_94\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_95\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_96\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_97\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_98\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_99\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......lambda\n",
      ".........vars\n",
      "......lambda_1\n",
      ".........vars\n",
      "......lambda_10\n",
      ".........vars\n",
      "......lambda_11\n",
      ".........vars\n",
      "......lambda_12\n",
      ".........vars\n",
      "......lambda_13\n",
      ".........vars\n",
      "......lambda_14\n",
      ".........vars\n",
      "......lambda_15\n",
      ".........vars\n",
      "......lambda_16\n",
      ".........vars\n",
      "......lambda_17\n",
      ".........vars\n",
      "......lambda_18\n",
      ".........vars\n",
      "......lambda_19\n",
      ".........vars\n",
      "......lambda_2\n",
      ".........vars\n",
      "......lambda_20\n",
      ".........vars\n",
      "......lambda_21\n",
      ".........vars\n",
      "......lambda_22\n",
      ".........vars\n",
      "......lambda_23\n",
      ".........vars\n",
      "......lambda_24\n",
      ".........vars\n",
      "......lambda_25\n",
      ".........vars\n",
      "......lambda_26\n",
      ".........vars\n",
      "......lambda_27\n",
      ".........vars\n",
      "......lambda_28\n",
      ".........vars\n",
      "......lambda_29\n",
      ".........vars\n",
      "......lambda_3\n",
      ".........vars\n",
      "......lambda_30\n",
      ".........vars\n",
      "......lambda_31\n",
      ".........vars\n",
      "......lambda_32\n",
      ".........vars\n",
      "......lambda_33\n",
      ".........vars\n",
      "......lambda_34\n",
      ".........vars\n",
      "......lambda_35\n",
      ".........vars\n",
      "......lambda_36\n",
      ".........vars\n",
      "......lambda_37\n",
      ".........vars\n",
      "......lambda_38\n",
      ".........vars\n",
      "......lambda_39\n",
      ".........vars\n",
      "......lambda_4\n",
      ".........vars\n",
      "......lambda_40\n",
      ".........vars\n",
      "......lambda_41\n",
      ".........vars\n",
      "......lambda_42\n",
      ".........vars\n",
      "......lambda_43\n",
      ".........vars\n",
      "......lambda_44\n",
      ".........vars\n",
      "......lambda_45\n",
      ".........vars\n",
      "......lambda_46\n",
      ".........vars\n",
      "......lambda_47\n",
      ".........vars\n",
      "......lambda_48\n",
      ".........vars\n",
      "......lambda_49\n",
      ".........vars\n",
      "......lambda_5\n",
      ".........vars\n",
      "......lambda_50\n",
      ".........vars\n",
      "......lambda_51\n",
      ".........vars\n",
      "......lambda_52\n",
      ".........vars\n",
      "......lambda_53\n",
      ".........vars\n",
      "......lambda_54\n",
      ".........vars\n",
      "......lambda_55\n",
      ".........vars\n",
      "......lambda_6\n",
      ".........vars\n",
      "......lambda_7\n",
      ".........vars\n",
      "......lambda_8\n",
      ".........vars\n",
      "......lambda_9\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 11:13:15.963916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - ETA: 0s - loss: 4.7726 - accuracy: 0.0984\n",
      "Epoch 1: val_loss improved from inf to 4.91106, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 62s 122ms/step - loss: 4.7726 - accuracy: 0.0984 - val_loss: 4.9111 - val_accuracy: 0.1148 - lr: 0.1000\n",
      "Epoch 2/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 4.1542 - accuracy: 0.1769\n",
      "Epoch 2: val_loss improved from 4.91106 to 4.00222, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 4.1542 - accuracy: 0.1769 - val_loss: 4.0022 - val_accuracy: 0.2108 - lr: 0.1000\n",
      "Epoch 3/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.8330 - accuracy: 0.2261\n",
      "Epoch 3: val_loss improved from 4.00222 to 3.91068, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 3.8330 - accuracy: 0.2261 - val_loss: 3.9107 - val_accuracy: 0.2102 - lr: 0.1000\n",
      "Epoch 4/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.5601 - accuracy: 0.2702\n",
      "Epoch 4: val_loss improved from 3.91068 to 3.53167, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 3.5601 - accuracy: 0.2702 - val_loss: 3.5317 - val_accuracy: 0.2806 - lr: 0.1000\n",
      "Epoch 5/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.3377 - accuracy: 0.3087\n",
      "Epoch 5: val_loss improved from 3.53167 to 3.47042, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 3.3377 - accuracy: 0.3087 - val_loss: 3.4704 - val_accuracy: 0.3076 - lr: 0.1000\n",
      "Epoch 6/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 3.1450 - accuracy: 0.3424\n",
      "Epoch 6: val_loss did not improve from 3.47042\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 3.1450 - accuracy: 0.3424 - val_loss: 3.5104 - val_accuracy: 0.3124 - lr: 0.1000\n",
      "Epoch 7/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.9851 - accuracy: 0.3717\n",
      "Epoch 7: val_loss improved from 3.47042 to 3.09479, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 2.9851 - accuracy: 0.3717 - val_loss: 3.0948 - val_accuracy: 0.3652 - lr: 0.1000\n",
      "Epoch 8/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.8519 - accuracy: 0.3952\n",
      "Epoch 8: val_loss did not improve from 3.09479\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 2.8519 - accuracy: 0.3952 - val_loss: 3.2029 - val_accuracy: 0.3436 - lr: 0.1000\n",
      "Epoch 9/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.7468 - accuracy: 0.4128\n",
      "Epoch 9: val_loss improved from 3.09479 to 2.78479, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 2.7468 - accuracy: 0.4128 - val_loss: 2.7848 - val_accuracy: 0.4210 - lr: 0.1000\n",
      "Epoch 10/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.6252 - accuracy: 0.4364\n",
      "Epoch 10: val_loss improved from 2.78479 to 2.70897, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 2.6252 - accuracy: 0.4364 - val_loss: 2.7090 - val_accuracy: 0.4360 - lr: 0.1000\n",
      "Epoch 11/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.5369 - accuracy: 0.4543\n",
      "Epoch 11: val_loss improved from 2.70897 to 2.50866, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 2.5369 - accuracy: 0.4543 - val_loss: 2.5087 - val_accuracy: 0.4728 - lr: 0.1000\n",
      "Epoch 12/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.4452 - accuracy: 0.4737\n",
      "Epoch 12: val_loss improved from 2.50866 to 2.46583, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 2.4452 - accuracy: 0.4737 - val_loss: 2.4658 - val_accuracy: 0.4778 - lr: 0.1000\n",
      "Epoch 13/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3840 - accuracy: 0.4841\n",
      "Epoch 13: val_loss improved from 2.46583 to 2.34590, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 2.3840 - accuracy: 0.4841 - val_loss: 2.3459 - val_accuracy: 0.5024 - lr: 0.1000\n",
      "Epoch 14/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.3355 - accuracy: 0.4963\n",
      "Epoch 14: val_loss did not improve from 2.34590\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 2.3355 - accuracy: 0.4963 - val_loss: 2.5084 - val_accuracy: 0.4818 - lr: 0.1000\n",
      "Epoch 15/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.2580 - accuracy: 0.5112\n",
      "Epoch 15: val_loss did not improve from 2.34590\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 2.2580 - accuracy: 0.5112 - val_loss: 2.3746 - val_accuracy: 0.4980 - lr: 0.1000\n",
      "Epoch 16/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.2097 - accuracy: 0.5243\n",
      "Epoch 16: val_loss improved from 2.34590 to 2.28435, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 2.2097 - accuracy: 0.5243 - val_loss: 2.2844 - val_accuracy: 0.5166 - lr: 0.1000\n",
      "Epoch 17/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.1636 - accuracy: 0.5327\n",
      "Epoch 17: val_loss did not improve from 2.28435\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 2.1636 - accuracy: 0.5327 - val_loss: 2.2871 - val_accuracy: 0.5114 - lr: 0.1000\n",
      "Epoch 18/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.1120 - accuracy: 0.5432\n",
      "Epoch 18: val_loss improved from 2.28435 to 2.17096, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 2.1120 - accuracy: 0.5432 - val_loss: 2.1710 - val_accuracy: 0.5426 - lr: 0.1000\n",
      "Epoch 19/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0708 - accuracy: 0.5520\n",
      "Epoch 19: val_loss did not improve from 2.17096\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 2.0708 - accuracy: 0.5520 - val_loss: 2.3015 - val_accuracy: 0.5220 - lr: 0.1000\n",
      "Epoch 20/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 2.0414 - accuracy: 0.5595\n",
      "Epoch 20: val_loss improved from 2.17096 to 2.14424, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 2.0414 - accuracy: 0.5595 - val_loss: 2.1442 - val_accuracy: 0.5514 - lr: 0.1000\n",
      "Epoch 21/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9931 - accuracy: 0.5694\n",
      "Epoch 21: val_loss improved from 2.14424 to 2.10522, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 1.9931 - accuracy: 0.5694 - val_loss: 2.1052 - val_accuracy: 0.5652 - lr: 0.1000\n",
      "Epoch 22/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9638 - accuracy: 0.5765\n",
      "Epoch 22: val_loss did not improve from 2.10522\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.9638 - accuracy: 0.5765 - val_loss: 2.1284 - val_accuracy: 0.5546 - lr: 0.1000\n",
      "Epoch 23/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9297 - accuracy: 0.5848\n",
      "Epoch 23: val_loss did not improve from 2.10522\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.9297 - accuracy: 0.5848 - val_loss: 2.1807 - val_accuracy: 0.5408 - lr: 0.1000\n",
      "Epoch 24/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.9016 - accuracy: 0.5908\n",
      "Epoch 24: val_loss did not improve from 2.10522\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.9016 - accuracy: 0.5908 - val_loss: 2.1637 - val_accuracy: 0.5484 - lr: 0.1000\n",
      "Epoch 25/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8723 - accuracy: 0.5979\n",
      "Epoch 25: val_loss improved from 2.10522 to 2.04412, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 1.8723 - accuracy: 0.5979 - val_loss: 2.0441 - val_accuracy: 0.5718 - lr: 0.1000\n",
      "Epoch 26/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8413 - accuracy: 0.6063\n",
      "Epoch 26: val_loss improved from 2.04412 to 1.96878, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.8413 - accuracy: 0.6063 - val_loss: 1.9688 - val_accuracy: 0.5910 - lr: 0.1000\n",
      "Epoch 27/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.8172 - accuracy: 0.6136\n",
      "Epoch 27: val_loss did not improve from 1.96878\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.8172 - accuracy: 0.6136 - val_loss: 2.0550 - val_accuracy: 0.5752 - lr: 0.1000\n",
      "Epoch 28/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7994 - accuracy: 0.6165\n",
      "Epoch 28: val_loss did not improve from 1.96878\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 1.7994 - accuracy: 0.6165 - val_loss: 2.0384 - val_accuracy: 0.5784 - lr: 0.1000\n",
      "Epoch 29/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7691 - accuracy: 0.6221\n",
      "Epoch 29: val_loss improved from 1.96878 to 1.89182, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.7691 - accuracy: 0.6221 - val_loss: 1.8918 - val_accuracy: 0.6148 - lr: 0.1000\n",
      "Epoch 30/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7580 - accuracy: 0.6240\n",
      "Epoch 30: val_loss did not improve from 1.89182\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.7580 - accuracy: 0.6240 - val_loss: 1.9350 - val_accuracy: 0.6036 - lr: 0.1000\n",
      "Epoch 31/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7243 - accuracy: 0.6353\n",
      "Epoch 31: val_loss improved from 1.89182 to 1.82425, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.7243 - accuracy: 0.6353 - val_loss: 1.8242 - val_accuracy: 0.6214 - lr: 0.1000\n",
      "Epoch 32/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.7043 - accuracy: 0.6361\n",
      "Epoch 32: val_loss improved from 1.82425 to 1.78381, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 1.7043 - accuracy: 0.6361 - val_loss: 1.7838 - val_accuracy: 0.6324 - lr: 0.1000\n",
      "Epoch 33/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6932 - accuracy: 0.6413\n",
      "Epoch 33: val_loss did not improve from 1.78381\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.6932 - accuracy: 0.6413 - val_loss: 1.8762 - val_accuracy: 0.6220 - lr: 0.1000\n",
      "Epoch 34/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6749 - accuracy: 0.6455\n",
      "Epoch 34: val_loss did not improve from 1.78381\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.6749 - accuracy: 0.6455 - val_loss: 2.0119 - val_accuracy: 0.5870 - lr: 0.1000\n",
      "Epoch 35/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6605 - accuracy: 0.6482\n",
      "Epoch 35: val_loss improved from 1.78381 to 1.77865, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.6605 - accuracy: 0.6482 - val_loss: 1.7787 - val_accuracy: 0.6338 - lr: 0.1000\n",
      "Epoch 36/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6448 - accuracy: 0.6531\n",
      "Epoch 36: val_loss did not improve from 1.77865\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.6448 - accuracy: 0.6531 - val_loss: 1.9076 - val_accuracy: 0.6102 - lr: 0.1000\n",
      "Epoch 37/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6259 - accuracy: 0.6580\n",
      "Epoch 37: val_loss did not improve from 1.77865\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.6259 - accuracy: 0.6580 - val_loss: 1.7944 - val_accuracy: 0.6396 - lr: 0.1000\n",
      "Epoch 38/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.6212 - accuracy: 0.6599\n",
      "Epoch 38: val_loss did not improve from 1.77865\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.6212 - accuracy: 0.6599 - val_loss: 1.8648 - val_accuracy: 0.6212 - lr: 0.1000\n",
      "Epoch 39/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5897 - accuracy: 0.6680\n",
      "Epoch 39: val_loss did not improve from 1.77865\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5897 - accuracy: 0.6680 - val_loss: 1.9455 - val_accuracy: 0.6066 - lr: 0.1000\n",
      "Epoch 40/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5905 - accuracy: 0.6667\n",
      "Epoch 40: val_loss did not improve from 1.77865\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 1.5905 - accuracy: 0.6667 - val_loss: 1.9052 - val_accuracy: 0.6208 - lr: 0.1000\n",
      "Epoch 41/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5706 - accuracy: 0.6702\n",
      "Epoch 41: val_loss did not improve from 1.77865\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.5706 - accuracy: 0.6702 - val_loss: 1.9934 - val_accuracy: 0.6070 - lr: 0.1000\n",
      "Epoch 42/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5463 - accuracy: 0.6780\n",
      "Epoch 42: val_loss did not improve from 1.77865\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.5463 - accuracy: 0.6780 - val_loss: 1.8388 - val_accuracy: 0.6352 - lr: 0.1000\n",
      "Epoch 43/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5459 - accuracy: 0.6774\n",
      "Epoch 43: val_loss improved from 1.77865 to 1.74442, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 1.5459 - accuracy: 0.6774 - val_loss: 1.7444 - val_accuracy: 0.6534 - lr: 0.1000\n",
      "Epoch 44/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5274 - accuracy: 0.6835\n",
      "Epoch 44: val_loss did not improve from 1.74442\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5274 - accuracy: 0.6835 - val_loss: 1.8127 - val_accuracy: 0.6378 - lr: 0.1000\n",
      "Epoch 45/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5080 - accuracy: 0.6872\n",
      "Epoch 45: val_loss did not improve from 1.74442\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.5080 - accuracy: 0.6872 - val_loss: 1.8756 - val_accuracy: 0.6284 - lr: 0.1000\n",
      "Epoch 46/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.5002 - accuracy: 0.6900\n",
      "Epoch 46: val_loss improved from 1.74442 to 1.68859, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 1.5002 - accuracy: 0.6900 - val_loss: 1.6886 - val_accuracy: 0.6618 - lr: 0.1000\n",
      "Epoch 47/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4955 - accuracy: 0.6902\n",
      "Epoch 47: val_loss did not improve from 1.68859\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.4955 - accuracy: 0.6902 - val_loss: 1.8111 - val_accuracy: 0.6430 - lr: 0.1000\n",
      "Epoch 48/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4773 - accuracy: 0.6950\n",
      "Epoch 48: val_loss did not improve from 1.68859\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.4773 - accuracy: 0.6950 - val_loss: 1.7439 - val_accuracy: 0.6544 - lr: 0.1000\n",
      "Epoch 49/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4731 - accuracy: 0.6954\n",
      "Epoch 49: val_loss did not improve from 1.68859\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.4731 - accuracy: 0.6954 - val_loss: 1.7839 - val_accuracy: 0.6484 - lr: 0.1000\n",
      "Epoch 50/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4586 - accuracy: 0.7030\n",
      "Epoch 50: val_loss did not improve from 1.68859\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 1.4586 - accuracy: 0.7030 - val_loss: 1.7610 - val_accuracy: 0.6536 - lr: 0.1000\n",
      "Epoch 51/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4553 - accuracy: 0.7014\n",
      "Epoch 51: val_loss did not improve from 1.68859\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 1.4553 - accuracy: 0.7014 - val_loss: 1.7092 - val_accuracy: 0.6540 - lr: 0.1000\n",
      "Epoch 52/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4277 - accuracy: 0.7070\n",
      "Epoch 52: val_loss did not improve from 1.68859\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.4277 - accuracy: 0.7070 - val_loss: 1.7045 - val_accuracy: 0.6604 - lr: 0.1000\n",
      "Epoch 53/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4336 - accuracy: 0.7060\n",
      "Epoch 53: val_loss did not improve from 1.68859\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.4336 - accuracy: 0.7060 - val_loss: 1.7611 - val_accuracy: 0.6516 - lr: 0.1000\n",
      "Epoch 54/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4257 - accuracy: 0.7065\n",
      "Epoch 54: val_loss did not improve from 1.68859\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.4257 - accuracy: 0.7065 - val_loss: 1.7670 - val_accuracy: 0.6522 - lr: 0.1000\n",
      "Epoch 55/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.4004 - accuracy: 0.7172\n",
      "Epoch 55: val_loss improved from 1.68859 to 1.68850, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.4004 - accuracy: 0.7172 - val_loss: 1.6885 - val_accuracy: 0.6754 - lr: 0.1000\n",
      "Epoch 56/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3957 - accuracy: 0.7169\n",
      "Epoch 56: val_loss did not improve from 1.68850\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.3957 - accuracy: 0.7169 - val_loss: 1.7204 - val_accuracy: 0.6664 - lr: 0.1000\n",
      "Epoch 57/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3923 - accuracy: 0.7163\n",
      "Epoch 57: val_loss did not improve from 1.68850\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.3923 - accuracy: 0.7163 - val_loss: 1.7531 - val_accuracy: 0.6618 - lr: 0.1000\n",
      "Epoch 58/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3839 - accuracy: 0.7194\n",
      "Epoch 58: val_loss did not improve from 1.68850\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.3839 - accuracy: 0.7194 - val_loss: 1.7493 - val_accuracy: 0.6606 - lr: 0.1000\n",
      "Epoch 59/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3698 - accuracy: 0.7265\n",
      "Epoch 59: val_loss did not improve from 1.68850\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.3698 - accuracy: 0.7265 - val_loss: 1.7489 - val_accuracy: 0.6672 - lr: 0.1000\n",
      "Epoch 60/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3719 - accuracy: 0.7228\n",
      "Epoch 60: val_loss did not improve from 1.68850\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.3719 - accuracy: 0.7228 - val_loss: 1.7575 - val_accuracy: 0.6626 - lr: 0.1000\n",
      "Epoch 61/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3426 - accuracy: 0.7319\n",
      "Epoch 61: val_loss did not improve from 1.68850\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.3426 - accuracy: 0.7319 - val_loss: 1.8209 - val_accuracy: 0.6438 - lr: 0.1000\n",
      "Epoch 62/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3481 - accuracy: 0.7269\n",
      "Epoch 62: val_loss did not improve from 1.68850\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.3481 - accuracy: 0.7269 - val_loss: 1.6925 - val_accuracy: 0.6796 - lr: 0.1000\n",
      "Epoch 63/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3351 - accuracy: 0.7342\n",
      "Epoch 63: val_loss did not improve from 1.68850\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.3351 - accuracy: 0.7342 - val_loss: 1.6889 - val_accuracy: 0.6684 - lr: 0.1000\n",
      "Epoch 64/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3298 - accuracy: 0.7319\n",
      "Epoch 64: val_loss improved from 1.68850 to 1.67617, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 107ms/step - loss: 1.3298 - accuracy: 0.7319 - val_loss: 1.6762 - val_accuracy: 0.6756 - lr: 0.1000\n",
      "Epoch 65/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3155 - accuracy: 0.7368\n",
      "Epoch 65: val_loss did not improve from 1.67617\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.3155 - accuracy: 0.7368 - val_loss: 1.7004 - val_accuracy: 0.6760 - lr: 0.1000\n",
      "Epoch 66/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3122 - accuracy: 0.7373\n",
      "Epoch 66: val_loss did not improve from 1.67617\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.3122 - accuracy: 0.7373 - val_loss: 1.7138 - val_accuracy: 0.6662 - lr: 0.1000\n",
      "Epoch 67/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3043 - accuracy: 0.7378\n",
      "Epoch 67: val_loss improved from 1.67617 to 1.65862, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.3043 - accuracy: 0.7378 - val_loss: 1.6586 - val_accuracy: 0.6850 - lr: 0.1000\n",
      "Epoch 68/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2958 - accuracy: 0.7431\n",
      "Epoch 68: val_loss did not improve from 1.65862\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.2958 - accuracy: 0.7431 - val_loss: 1.8425 - val_accuracy: 0.6534 - lr: 0.1000\n",
      "Epoch 69/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3062 - accuracy: 0.7384\n",
      "Epoch 69: val_loss did not improve from 1.65862\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.3062 - accuracy: 0.7384 - val_loss: 1.7042 - val_accuracy: 0.6890 - lr: 0.1000\n",
      "Epoch 70/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.3111 - accuracy: 0.7368\n",
      "Epoch 70: val_loss did not improve from 1.65862\n",
      "351/351 [==============================] - 36s 104ms/step - loss: 1.3111 - accuracy: 0.7368 - val_loss: 1.6759 - val_accuracy: 0.6808 - lr: 0.1000\n",
      "Epoch 71/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2651 - accuracy: 0.7519\n",
      "Epoch 71: val_loss did not improve from 1.65862\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 1.2651 - accuracy: 0.7519 - val_loss: 1.6813 - val_accuracy: 0.6806 - lr: 0.1000\n",
      "Epoch 72/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2704 - accuracy: 0.7475\n",
      "Epoch 72: val_loss did not improve from 1.65862\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.2704 - accuracy: 0.7475 - val_loss: 1.7269 - val_accuracy: 0.6804 - lr: 0.1000\n",
      "Epoch 73/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2776 - accuracy: 0.7473\n",
      "Epoch 73: val_loss improved from 1.65862 to 1.64510, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.2776 - accuracy: 0.7473 - val_loss: 1.6451 - val_accuracy: 0.6844 - lr: 0.1000\n",
      "Epoch 74/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2637 - accuracy: 0.7505\n",
      "Epoch 74: val_loss did not improve from 1.64510\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 1.2637 - accuracy: 0.7505 - val_loss: 1.7089 - val_accuracy: 0.6752 - lr: 0.1000\n",
      "Epoch 75/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2438 - accuracy: 0.7548\n",
      "Epoch 75: val_loss did not improve from 1.64510\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 1.2438 - accuracy: 0.7548 - val_loss: 1.7009 - val_accuracy: 0.6868 - lr: 0.1000\n",
      "Epoch 76/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2312 - accuracy: 0.7587\n",
      "Epoch 76: val_loss did not improve from 1.64510\n",
      "351/351 [==============================] - 36s 104ms/step - loss: 1.2312 - accuracy: 0.7587 - val_loss: 1.6517 - val_accuracy: 0.6860 - lr: 0.1000\n",
      "Epoch 77/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2207 - accuracy: 0.7612\n",
      "Epoch 77: val_loss did not improve from 1.64510\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.2207 - accuracy: 0.7612 - val_loss: 1.6567 - val_accuracy: 0.6814 - lr: 0.1000\n",
      "Epoch 78/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2341 - accuracy: 0.7585\n",
      "Epoch 78: val_loss improved from 1.64510 to 1.59106, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.2341 - accuracy: 0.7585 - val_loss: 1.5911 - val_accuracy: 0.6990 - lr: 0.1000\n",
      "Epoch 79/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2401 - accuracy: 0.7552\n",
      "Epoch 79: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 35s 101ms/step - loss: 1.2401 - accuracy: 0.7552 - val_loss: 1.7213 - val_accuracy: 0.6860 - lr: 0.1000\n",
      "Epoch 80/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2266 - accuracy: 0.7607\n",
      "Epoch 80: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.2266 - accuracy: 0.7607 - val_loss: 1.6691 - val_accuracy: 0.6910 - lr: 0.1000\n",
      "Epoch 81/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2156 - accuracy: 0.7621\n",
      "Epoch 81: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.2156 - accuracy: 0.7621 - val_loss: 1.6523 - val_accuracy: 0.6868 - lr: 0.1000\n",
      "Epoch 82/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.2073 - accuracy: 0.7665\n",
      "Epoch 82: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.2073 - accuracy: 0.7665 - val_loss: 1.6781 - val_accuracy: 0.6830 - lr: 0.1000\n",
      "Epoch 83/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1993 - accuracy: 0.7672\n",
      "Epoch 83: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.1993 - accuracy: 0.7672 - val_loss: 1.7007 - val_accuracy: 0.6858 - lr: 0.1000\n",
      "Epoch 84/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1910 - accuracy: 0.7676\n",
      "Epoch 84: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.1910 - accuracy: 0.7676 - val_loss: 1.6497 - val_accuracy: 0.6916 - lr: 0.1000\n",
      "Epoch 85/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1851 - accuracy: 0.7699\n",
      "Epoch 85: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 1.1851 - accuracy: 0.7699 - val_loss: 1.6800 - val_accuracy: 0.6852 - lr: 0.1000\n",
      "Epoch 86/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1897 - accuracy: 0.7687\n",
      "Epoch 86: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 1.1897 - accuracy: 0.7687 - val_loss: 1.6336 - val_accuracy: 0.6900 - lr: 0.1000\n",
      "Epoch 87/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1758 - accuracy: 0.7728\n",
      "Epoch 87: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.1758 - accuracy: 0.7728 - val_loss: 1.7106 - val_accuracy: 0.6766 - lr: 0.1000\n",
      "Epoch 88/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1736 - accuracy: 0.7729\n",
      "Epoch 88: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 1.1736 - accuracy: 0.7729 - val_loss: 1.6782 - val_accuracy: 0.6850 - lr: 0.1000\n",
      "Epoch 89/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1692 - accuracy: 0.7747\n",
      "Epoch 89: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.1692 - accuracy: 0.7747 - val_loss: 1.7381 - val_accuracy: 0.6832 - lr: 0.1000\n",
      "Epoch 90/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1799 - accuracy: 0.7718\n",
      "Epoch 90: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.1799 - accuracy: 0.7718 - val_loss: 1.6779 - val_accuracy: 0.6850 - lr: 0.1000\n",
      "Epoch 91/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1632 - accuracy: 0.7747\n",
      "Epoch 91: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 1.1632 - accuracy: 0.7747 - val_loss: 1.7252 - val_accuracy: 0.6782 - lr: 0.1000\n",
      "Epoch 92/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1568 - accuracy: 0.7747\n",
      "Epoch 92: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.1568 - accuracy: 0.7747 - val_loss: 1.6121 - val_accuracy: 0.6990 - lr: 0.1000\n",
      "Epoch 93/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1305 - accuracy: 0.7842\n",
      "Epoch 93: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.1305 - accuracy: 0.7842 - val_loss: 1.6719 - val_accuracy: 0.6846 - lr: 0.1000\n",
      "Epoch 94/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1375 - accuracy: 0.7805\n",
      "Epoch 94: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.1375 - accuracy: 0.7805 - val_loss: 1.6997 - val_accuracy: 0.6910 - lr: 0.1000\n",
      "Epoch 95/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1339 - accuracy: 0.7826\n",
      "Epoch 95: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.1339 - accuracy: 0.7826 - val_loss: 1.6881 - val_accuracy: 0.6828 - lr: 0.1000\n",
      "Epoch 96/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1421 - accuracy: 0.7798\n",
      "Epoch 96: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.1421 - accuracy: 0.7798 - val_loss: 1.6977 - val_accuracy: 0.6986 - lr: 0.1000\n",
      "Epoch 97/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1225 - accuracy: 0.7852\n",
      "Epoch 97: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 107ms/step - loss: 1.1225 - accuracy: 0.7852 - val_loss: 1.6816 - val_accuracy: 0.6974 - lr: 0.1000\n",
      "Epoch 98/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1175 - accuracy: 0.7887\n",
      "Epoch 98: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.1175 - accuracy: 0.7887 - val_loss: 1.5974 - val_accuracy: 0.7104 - lr: 0.1000\n",
      "Epoch 99/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1116 - accuracy: 0.7873\n",
      "Epoch 99: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.1116 - accuracy: 0.7873 - val_loss: 1.6623 - val_accuracy: 0.6904 - lr: 0.1000\n",
      "Epoch 100/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1174 - accuracy: 0.7887\n",
      "Epoch 100: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.1174 - accuracy: 0.7887 - val_loss: 1.7135 - val_accuracy: 0.6888 - lr: 0.1000\n",
      "Epoch 101/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.1080 - accuracy: 0.7887\n",
      "Epoch 101: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 1.1080 - accuracy: 0.7887 - val_loss: 1.6993 - val_accuracy: 0.6846 - lr: 0.1000\n",
      "Epoch 102/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0875 - accuracy: 0.7938\n",
      "Epoch 102: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.0875 - accuracy: 0.7938 - val_loss: 1.5999 - val_accuracy: 0.7044 - lr: 0.1000\n",
      "Epoch 103/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0912 - accuracy: 0.7938\n",
      "Epoch 103: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.0912 - accuracy: 0.7938 - val_loss: 1.7413 - val_accuracy: 0.6856 - lr: 0.1000\n",
      "Epoch 104/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0939 - accuracy: 0.7908\n",
      "Epoch 104: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0939 - accuracy: 0.7908 - val_loss: 1.6846 - val_accuracy: 0.6926 - lr: 0.1000\n",
      "Epoch 105/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0904 - accuracy: 0.7947\n",
      "Epoch 105: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 36s 104ms/step - loss: 1.0904 - accuracy: 0.7947 - val_loss: 1.6930 - val_accuracy: 0.6924 - lr: 0.1000\n",
      "Epoch 106/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0897 - accuracy: 0.7929\n",
      "Epoch 106: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.0897 - accuracy: 0.7929 - val_loss: 1.6790 - val_accuracy: 0.6990 - lr: 0.1000\n",
      "Epoch 107/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0911 - accuracy: 0.7926\n",
      "Epoch 107: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0911 - accuracy: 0.7926 - val_loss: 1.6952 - val_accuracy: 0.6892 - lr: 0.1000\n",
      "Epoch 108/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.7962\n",
      "Epoch 108: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.0763 - accuracy: 0.7962 - val_loss: 1.6669 - val_accuracy: 0.6942 - lr: 0.1000\n",
      "Epoch 109/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0634 - accuracy: 0.7994\n",
      "Epoch 109: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.0634 - accuracy: 0.7994 - val_loss: 1.6571 - val_accuracy: 0.7016 - lr: 0.1000\n",
      "Epoch 110/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0767 - accuracy: 0.7967\n",
      "Epoch 110: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0767 - accuracy: 0.7967 - val_loss: 1.6621 - val_accuracy: 0.6946 - lr: 0.1000\n",
      "Epoch 111/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0649 - accuracy: 0.8013\n",
      "Epoch 111: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 1.0649 - accuracy: 0.8013 - val_loss: 1.6289 - val_accuracy: 0.7042 - lr: 0.1000\n",
      "Epoch 112/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0662 - accuracy: 0.7978\n",
      "Epoch 112: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0662 - accuracy: 0.7978 - val_loss: 1.6462 - val_accuracy: 0.6988 - lr: 0.1000\n",
      "Epoch 113/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0595 - accuracy: 0.7998\n",
      "Epoch 113: val_loss did not improve from 1.59106\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.0595 - accuracy: 0.7998 - val_loss: 1.6441 - val_accuracy: 0.6986 - lr: 0.1000\n",
      "Epoch 114/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0533 - accuracy: 0.8023\n",
      "Epoch 114: val_loss improved from 1.59106 to 1.57717, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 1.0533 - accuracy: 0.8023 - val_loss: 1.5772 - val_accuracy: 0.7130 - lr: 0.1000\n",
      "Epoch 115/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0535 - accuracy: 0.8041\n",
      "Epoch 115: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.0535 - accuracy: 0.8041 - val_loss: 1.6294 - val_accuracy: 0.7078 - lr: 0.1000\n",
      "Epoch 116/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0333 - accuracy: 0.8074\n",
      "Epoch 116: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.0333 - accuracy: 0.8074 - val_loss: 1.6971 - val_accuracy: 0.6986 - lr: 0.1000\n",
      "Epoch 117/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0547 - accuracy: 0.7998\n",
      "Epoch 117: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 1.0547 - accuracy: 0.7998 - val_loss: 1.6432 - val_accuracy: 0.7046 - lr: 0.1000\n",
      "Epoch 118/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0222 - accuracy: 0.8105\n",
      "Epoch 118: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0222 - accuracy: 0.8105 - val_loss: 1.6512 - val_accuracy: 0.7022 - lr: 0.1000\n",
      "Epoch 119/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0206 - accuracy: 0.8105\n",
      "Epoch 119: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 1.0206 - accuracy: 0.8105 - val_loss: 1.6752 - val_accuracy: 0.7018 - lr: 0.1000\n",
      "Epoch 120/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0216 - accuracy: 0.8089\n",
      "Epoch 120: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0216 - accuracy: 0.8089 - val_loss: 1.6935 - val_accuracy: 0.6944 - lr: 0.1000\n",
      "Epoch 121/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0174 - accuracy: 0.8125\n",
      "Epoch 121: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 1.0174 - accuracy: 0.8125 - val_loss: 1.6453 - val_accuracy: 0.7026 - lr: 0.1000\n",
      "Epoch 122/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0381 - accuracy: 0.8028\n",
      "Epoch 122: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.0381 - accuracy: 0.8028 - val_loss: 1.6178 - val_accuracy: 0.7102 - lr: 0.1000\n",
      "Epoch 123/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0177 - accuracy: 0.8094\n",
      "Epoch 123: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0177 - accuracy: 0.8094 - val_loss: 1.6932 - val_accuracy: 0.7060 - lr: 0.1000\n",
      "Epoch 124/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0208 - accuracy: 0.8076\n",
      "Epoch 124: val_loss did not improve from 1.57717\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0208 - accuracy: 0.8076 - val_loss: 1.6377 - val_accuracy: 0.7068 - lr: 0.1000\n",
      "Epoch 125/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0161 - accuracy: 0.8125\n",
      "Epoch 125: val_loss improved from 1.57717 to 1.57077, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 1.0161 - accuracy: 0.8125 - val_loss: 1.5708 - val_accuracy: 0.7154 - lr: 0.1000\n",
      "Epoch 126/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0150 - accuracy: 0.8104\n",
      "Epoch 126: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0150 - accuracy: 0.8104 - val_loss: 1.6697 - val_accuracy: 0.7004 - lr: 0.1000\n",
      "Epoch 127/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0108 - accuracy: 0.8119\n",
      "Epoch 127: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.0108 - accuracy: 0.8119 - val_loss: 1.6577 - val_accuracy: 0.7040 - lr: 0.1000\n",
      "Epoch 128/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0115 - accuracy: 0.8111\n",
      "Epoch 128: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 1.0115 - accuracy: 0.8111 - val_loss: 1.6272 - val_accuracy: 0.7122 - lr: 0.1000\n",
      "Epoch 129/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 1.0115 - accuracy: 0.8110\n",
      "Epoch 129: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 1.0115 - accuracy: 0.8110 - val_loss: 1.6224 - val_accuracy: 0.7026 - lr: 0.1000\n",
      "Epoch 130/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9974 - accuracy: 0.8161\n",
      "Epoch 130: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9974 - accuracy: 0.8161 - val_loss: 1.6528 - val_accuracy: 0.7158 - lr: 0.1000\n",
      "Epoch 131/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9876 - accuracy: 0.8182\n",
      "Epoch 131: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 39s 109ms/step - loss: 0.9876 - accuracy: 0.8182 - val_loss: 1.6461 - val_accuracy: 0.7002 - lr: 0.1000\n",
      "Epoch 132/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9759 - accuracy: 0.8197\n",
      "Epoch 132: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.9759 - accuracy: 0.8197 - val_loss: 1.6469 - val_accuracy: 0.7054 - lr: 0.1000\n",
      "Epoch 133/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9775 - accuracy: 0.8197\n",
      "Epoch 133: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9775 - accuracy: 0.8197 - val_loss: 1.7404 - val_accuracy: 0.6978 - lr: 0.1000\n",
      "Epoch 134/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9698 - accuracy: 0.8222\n",
      "Epoch 134: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.9698 - accuracy: 0.8222 - val_loss: 1.7545 - val_accuracy: 0.6918 - lr: 0.1000\n",
      "Epoch 135/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9832 - accuracy: 0.8195\n",
      "Epoch 135: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9832 - accuracy: 0.8195 - val_loss: 1.6591 - val_accuracy: 0.7106 - lr: 0.1000\n",
      "Epoch 136/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9818 - accuracy: 0.8193\n",
      "Epoch 136: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.9818 - accuracy: 0.8193 - val_loss: 1.6701 - val_accuracy: 0.7026 - lr: 0.1000\n",
      "Epoch 137/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9767 - accuracy: 0.8223\n",
      "Epoch 137: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.9767 - accuracy: 0.8223 - val_loss: 1.6403 - val_accuracy: 0.7092 - lr: 0.1000\n",
      "Epoch 138/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9659 - accuracy: 0.8226\n",
      "Epoch 138: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9659 - accuracy: 0.8226 - val_loss: 1.6148 - val_accuracy: 0.7144 - lr: 0.1000\n",
      "Epoch 139/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9643 - accuracy: 0.8218\n",
      "Epoch 139: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9643 - accuracy: 0.8218 - val_loss: 1.6283 - val_accuracy: 0.7078 - lr: 0.1000\n",
      "Epoch 140/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9649 - accuracy: 0.8229\n",
      "Epoch 140: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.9649 - accuracy: 0.8229 - val_loss: 1.7595 - val_accuracy: 0.6898 - lr: 0.1000\n",
      "Epoch 141/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9723 - accuracy: 0.8219\n",
      "Epoch 141: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.9723 - accuracy: 0.8219 - val_loss: 1.6401 - val_accuracy: 0.7084 - lr: 0.1000\n",
      "Epoch 142/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9584 - accuracy: 0.8229\n",
      "Epoch 142: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.9584 - accuracy: 0.8229 - val_loss: 1.6191 - val_accuracy: 0.7100 - lr: 0.1000\n",
      "Epoch 143/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9478 - accuracy: 0.8283\n",
      "Epoch 143: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 36s 104ms/step - loss: 0.9478 - accuracy: 0.8283 - val_loss: 1.6499 - val_accuracy: 0.7050 - lr: 0.1000\n",
      "Epoch 144/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9469 - accuracy: 0.8274\n",
      "Epoch 144: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.9469 - accuracy: 0.8274 - val_loss: 1.6158 - val_accuracy: 0.7118 - lr: 0.1000\n",
      "Epoch 145/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9347 - accuracy: 0.8309\n",
      "Epoch 145: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 36s 104ms/step - loss: 0.9347 - accuracy: 0.8309 - val_loss: 1.6074 - val_accuracy: 0.7136 - lr: 0.1000\n",
      "Epoch 146/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9289 - accuracy: 0.8320\n",
      "Epoch 146: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9289 - accuracy: 0.8320 - val_loss: 1.6565 - val_accuracy: 0.6968 - lr: 0.1000\n",
      "Epoch 147/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9502 - accuracy: 0.8271\n",
      "Epoch 147: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9502 - accuracy: 0.8271 - val_loss: 1.7150 - val_accuracy: 0.6986 - lr: 0.1000\n",
      "Epoch 148/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9436 - accuracy: 0.8275\n",
      "Epoch 148: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9436 - accuracy: 0.8275 - val_loss: 1.7902 - val_accuracy: 0.6992 - lr: 0.1000\n",
      "Epoch 149/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9311 - accuracy: 0.8305\n",
      "Epoch 149: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.9311 - accuracy: 0.8305 - val_loss: 1.6268 - val_accuracy: 0.7072 - lr: 0.1000\n",
      "Epoch 150/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.8348\n",
      "Epoch 150: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.9138 - accuracy: 0.8348 - val_loss: 1.6196 - val_accuracy: 0.7132 - lr: 0.1000\n",
      "Epoch 151/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9321 - accuracy: 0.8303\n",
      "Epoch 151: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.9321 - accuracy: 0.8303 - val_loss: 1.6060 - val_accuracy: 0.7146 - lr: 0.1000\n",
      "Epoch 152/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9180 - accuracy: 0.8344\n",
      "Epoch 152: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9180 - accuracy: 0.8344 - val_loss: 1.5897 - val_accuracy: 0.7176 - lr: 0.1000\n",
      "Epoch 153/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9145 - accuracy: 0.8342\n",
      "Epoch 153: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9145 - accuracy: 0.8342 - val_loss: 1.6428 - val_accuracy: 0.7094 - lr: 0.1000\n",
      "Epoch 154/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.8333\n",
      "Epoch 154: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9172 - accuracy: 0.8333 - val_loss: 1.6353 - val_accuracy: 0.7074 - lr: 0.1000\n",
      "Epoch 155/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9171 - accuracy: 0.8323\n",
      "Epoch 155: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.9171 - accuracy: 0.8323 - val_loss: 1.6107 - val_accuracy: 0.7182 - lr: 0.1000\n",
      "Epoch 156/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.8332\n",
      "Epoch 156: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.9146 - accuracy: 0.8332 - val_loss: 1.6315 - val_accuracy: 0.7098 - lr: 0.1000\n",
      "Epoch 157/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9060 - accuracy: 0.8344\n",
      "Epoch 157: val_loss did not improve from 1.57077\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.9060 - accuracy: 0.8344 - val_loss: 1.6664 - val_accuracy: 0.7078 - lr: 0.1000\n",
      "Epoch 158/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9129 - accuracy: 0.8353\n",
      "Epoch 158: val_loss improved from 1.57077 to 1.56296, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.9129 - accuracy: 0.8353 - val_loss: 1.5630 - val_accuracy: 0.7180 - lr: 0.1000\n",
      "Epoch 159/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9170 - accuracy: 0.8338\n",
      "Epoch 159: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.9170 - accuracy: 0.8338 - val_loss: 1.6019 - val_accuracy: 0.7088 - lr: 0.1000\n",
      "Epoch 160/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8966 - accuracy: 0.8365\n",
      "Epoch 160: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8966 - accuracy: 0.8365 - val_loss: 1.5999 - val_accuracy: 0.7120 - lr: 0.1000\n",
      "Epoch 161/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9092 - accuracy: 0.8335\n",
      "Epoch 161: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.9092 - accuracy: 0.8335 - val_loss: 1.7128 - val_accuracy: 0.6994 - lr: 0.1000\n",
      "Epoch 162/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.9088 - accuracy: 0.8343\n",
      "Epoch 162: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.9088 - accuracy: 0.8343 - val_loss: 1.6286 - val_accuracy: 0.7130 - lr: 0.1000\n",
      "Epoch 163/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8909 - accuracy: 0.8413\n",
      "Epoch 163: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.8909 - accuracy: 0.8413 - val_loss: 1.6456 - val_accuracy: 0.7112 - lr: 0.1000\n",
      "Epoch 164/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8977 - accuracy: 0.8374\n",
      "Epoch 164: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8977 - accuracy: 0.8374 - val_loss: 1.6532 - val_accuracy: 0.7172 - lr: 0.1000\n",
      "Epoch 165/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8906 - accuracy: 0.8414\n",
      "Epoch 165: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8906 - accuracy: 0.8414 - val_loss: 1.6572 - val_accuracy: 0.7156 - lr: 0.1000\n",
      "Epoch 166/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8715 - accuracy: 0.8471\n",
      "Epoch 166: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.8715 - accuracy: 0.8471 - val_loss: 1.5854 - val_accuracy: 0.7210 - lr: 0.1000\n",
      "Epoch 167/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8926 - accuracy: 0.8392\n",
      "Epoch 167: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8926 - accuracy: 0.8392 - val_loss: 1.6303 - val_accuracy: 0.7134 - lr: 0.1000\n",
      "Epoch 168/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8736 - accuracy: 0.8456\n",
      "Epoch 168: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8736 - accuracy: 0.8456 - val_loss: 1.6019 - val_accuracy: 0.7224 - lr: 0.1000\n",
      "Epoch 169/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8965 - accuracy: 0.8387\n",
      "Epoch 169: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.8965 - accuracy: 0.8387 - val_loss: 1.6239 - val_accuracy: 0.7122 - lr: 0.1000\n",
      "Epoch 170/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8668 - accuracy: 0.8456\n",
      "Epoch 170: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8668 - accuracy: 0.8456 - val_loss: 1.6304 - val_accuracy: 0.7108 - lr: 0.1000\n",
      "Epoch 171/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8640 - accuracy: 0.8462\n",
      "Epoch 171: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.8640 - accuracy: 0.8462 - val_loss: 1.6987 - val_accuracy: 0.7048 - lr: 0.1000\n",
      "Epoch 172/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8778 - accuracy: 0.8428\n",
      "Epoch 172: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8778 - accuracy: 0.8428 - val_loss: 1.6457 - val_accuracy: 0.7046 - lr: 0.1000\n",
      "Epoch 173/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8803 - accuracy: 0.8435\n",
      "Epoch 173: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.8803 - accuracy: 0.8435 - val_loss: 1.6378 - val_accuracy: 0.7122 - lr: 0.1000\n",
      "Epoch 174/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8593 - accuracy: 0.8475\n",
      "Epoch 174: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.8593 - accuracy: 0.8475 - val_loss: 1.6781 - val_accuracy: 0.7082 - lr: 0.1000\n",
      "Epoch 175/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8653 - accuracy: 0.8452\n",
      "Epoch 175: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8653 - accuracy: 0.8452 - val_loss: 1.6212 - val_accuracy: 0.7098 - lr: 0.1000\n",
      "Epoch 176/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8728 - accuracy: 0.8436\n",
      "Epoch 176: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8728 - accuracy: 0.8436 - val_loss: 1.6061 - val_accuracy: 0.7230 - lr: 0.1000\n",
      "Epoch 177/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8584 - accuracy: 0.8485\n",
      "Epoch 177: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8584 - accuracy: 0.8485 - val_loss: 1.6945 - val_accuracy: 0.7078 - lr: 0.1000\n",
      "Epoch 178/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8686 - accuracy: 0.8452\n",
      "Epoch 178: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.8686 - accuracy: 0.8452 - val_loss: 1.5987 - val_accuracy: 0.7142 - lr: 0.1000\n",
      "Epoch 179/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8459 - accuracy: 0.8510\n",
      "Epoch 179: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.8459 - accuracy: 0.8510 - val_loss: 1.6351 - val_accuracy: 0.7114 - lr: 0.1000\n",
      "Epoch 180/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8374 - accuracy: 0.8542\n",
      "Epoch 180: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8374 - accuracy: 0.8542 - val_loss: 1.6584 - val_accuracy: 0.7082 - lr: 0.1000\n",
      "Epoch 181/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8395 - accuracy: 0.8529\n",
      "Epoch 181: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 40s 113ms/step - loss: 0.8395 - accuracy: 0.8529 - val_loss: 1.6543 - val_accuracy: 0.7080 - lr: 0.1000\n",
      "Epoch 182/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8347 - accuracy: 0.8545\n",
      "Epoch 182: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8347 - accuracy: 0.8545 - val_loss: 1.6536 - val_accuracy: 0.7128 - lr: 0.1000\n",
      "Epoch 183/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8402 - accuracy: 0.8531\n",
      "Epoch 183: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8402 - accuracy: 0.8531 - val_loss: 1.6557 - val_accuracy: 0.7156 - lr: 0.1000\n",
      "Epoch 184/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8445 - accuracy: 0.8509\n",
      "Epoch 184: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8445 - accuracy: 0.8509 - val_loss: 1.6342 - val_accuracy: 0.7186 - lr: 0.1000\n",
      "Epoch 185/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8473 - accuracy: 0.8488\n",
      "Epoch 185: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.8473 - accuracy: 0.8488 - val_loss: 1.6283 - val_accuracy: 0.7130 - lr: 0.1000\n",
      "Epoch 186/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8562 - accuracy: 0.8478\n",
      "Epoch 186: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.8562 - accuracy: 0.8478 - val_loss: 1.6327 - val_accuracy: 0.7204 - lr: 0.1000\n",
      "Epoch 187/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8516 - accuracy: 0.8484\n",
      "Epoch 187: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8516 - accuracy: 0.8484 - val_loss: 1.6641 - val_accuracy: 0.7090 - lr: 0.1000\n",
      "Epoch 188/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8399 - accuracy: 0.8506\n",
      "Epoch 188: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.8399 - accuracy: 0.8506 - val_loss: 1.7337 - val_accuracy: 0.7010 - lr: 0.1000\n",
      "Epoch 189/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.8574\n",
      "Epoch 189: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.8223 - accuracy: 0.8574 - val_loss: 1.7363 - val_accuracy: 0.6990 - lr: 0.1000\n",
      "Epoch 190/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8425 - accuracy: 0.8525\n",
      "Epoch 190: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8425 - accuracy: 0.8525 - val_loss: 1.6202 - val_accuracy: 0.7156 - lr: 0.1000\n",
      "Epoch 191/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8227 - accuracy: 0.8550\n",
      "Epoch 191: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.8227 - accuracy: 0.8550 - val_loss: 1.6262 - val_accuracy: 0.7210 - lr: 0.1000\n",
      "Epoch 192/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8157 - accuracy: 0.8565\n",
      "Epoch 192: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 107ms/step - loss: 0.8157 - accuracy: 0.8565 - val_loss: 1.6219 - val_accuracy: 0.7156 - lr: 0.1000\n",
      "Epoch 193/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8178 - accuracy: 0.8581\n",
      "Epoch 193: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8178 - accuracy: 0.8581 - val_loss: 1.6114 - val_accuracy: 0.7176 - lr: 0.1000\n",
      "Epoch 194/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8241 - accuracy: 0.8526\n",
      "Epoch 194: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8241 - accuracy: 0.8526 - val_loss: 1.6949 - val_accuracy: 0.6982 - lr: 0.1000\n",
      "Epoch 195/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8212 - accuracy: 0.8563\n",
      "Epoch 195: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8212 - accuracy: 0.8563 - val_loss: 1.6178 - val_accuracy: 0.7172 - lr: 0.1000\n",
      "Epoch 196/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8175 - accuracy: 0.8576\n",
      "Epoch 196: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.8175 - accuracy: 0.8576 - val_loss: 1.6534 - val_accuracy: 0.7136 - lr: 0.1000\n",
      "Epoch 197/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8122 - accuracy: 0.8572\n",
      "Epoch 197: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.8122 - accuracy: 0.8572 - val_loss: 1.6427 - val_accuracy: 0.7100 - lr: 0.1000\n",
      "Epoch 198/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8134 - accuracy: 0.8587\n",
      "Epoch 198: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.8134 - accuracy: 0.8587 - val_loss: 1.6622 - val_accuracy: 0.7028 - lr: 0.1000\n",
      "Epoch 199/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8285 - accuracy: 0.8526\n",
      "Epoch 199: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8285 - accuracy: 0.8526 - val_loss: 1.6852 - val_accuracy: 0.7070 - lr: 0.1000\n",
      "Epoch 200/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8089 - accuracy: 0.8568\n",
      "Epoch 200: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8089 - accuracy: 0.8568 - val_loss: 1.5846 - val_accuracy: 0.7210 - lr: 0.1000\n",
      "Epoch 201/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8005 - accuracy: 0.8615\n",
      "Epoch 201: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.8005 - accuracy: 0.8615 - val_loss: 1.6754 - val_accuracy: 0.7160 - lr: 0.1000\n",
      "Epoch 202/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8040 - accuracy: 0.8600\n",
      "Epoch 202: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.8040 - accuracy: 0.8600 - val_loss: 1.6459 - val_accuracy: 0.7120 - lr: 0.1000\n",
      "Epoch 203/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8094 - accuracy: 0.8575\n",
      "Epoch 203: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8094 - accuracy: 0.8575 - val_loss: 1.6261 - val_accuracy: 0.7198 - lr: 0.1000\n",
      "Epoch 204/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7990 - accuracy: 0.8629\n",
      "Epoch 204: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.7990 - accuracy: 0.8629 - val_loss: 1.6485 - val_accuracy: 0.7132 - lr: 0.1000\n",
      "Epoch 205/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7994 - accuracy: 0.8594\n",
      "Epoch 205: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.7994 - accuracy: 0.8594 - val_loss: 1.6615 - val_accuracy: 0.7156 - lr: 0.1000\n",
      "Epoch 206/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8024 - accuracy: 0.8605\n",
      "Epoch 206: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8024 - accuracy: 0.8605 - val_loss: 1.6497 - val_accuracy: 0.7178 - lr: 0.1000\n",
      "Epoch 207/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7977 - accuracy: 0.8617\n",
      "Epoch 207: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.7977 - accuracy: 0.8617 - val_loss: 1.6641 - val_accuracy: 0.7182 - lr: 0.1000\n",
      "Epoch 208/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.8035 - accuracy: 0.8588\n",
      "Epoch 208: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.8035 - accuracy: 0.8588 - val_loss: 1.6216 - val_accuracy: 0.7210 - lr: 0.1000\n",
      "Epoch 209/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7725 - accuracy: 0.8689\n",
      "Epoch 209: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.7725 - accuracy: 0.8689 - val_loss: 1.6425 - val_accuracy: 0.7238 - lr: 0.1000\n",
      "Epoch 210/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7924 - accuracy: 0.8614\n",
      "Epoch 210: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.7924 - accuracy: 0.8614 - val_loss: 1.6643 - val_accuracy: 0.7226 - lr: 0.1000\n",
      "Epoch 211/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7843 - accuracy: 0.8659\n",
      "Epoch 211: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.7843 - accuracy: 0.8659 - val_loss: 1.7085 - val_accuracy: 0.7060 - lr: 0.1000\n",
      "Epoch 212/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7738 - accuracy: 0.8685\n",
      "Epoch 212: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.7738 - accuracy: 0.8685 - val_loss: 1.6347 - val_accuracy: 0.7168 - lr: 0.1000\n",
      "Epoch 213/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7881 - accuracy: 0.8626\n",
      "Epoch 213: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.7881 - accuracy: 0.8626 - val_loss: 1.6672 - val_accuracy: 0.7166 - lr: 0.1000\n",
      "Epoch 214/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7772 - accuracy: 0.8648\n",
      "Epoch 214: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.7772 - accuracy: 0.8648 - val_loss: 1.7312 - val_accuracy: 0.7094 - lr: 0.1000\n",
      "Epoch 215/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7704 - accuracy: 0.8676\n",
      "Epoch 215: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 36s 104ms/step - loss: 0.7704 - accuracy: 0.8676 - val_loss: 1.6583 - val_accuracy: 0.7126 - lr: 0.1000\n",
      "Epoch 216/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7688 - accuracy: 0.8668\n",
      "Epoch 216: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.7688 - accuracy: 0.8668 - val_loss: 1.6702 - val_accuracy: 0.7130 - lr: 0.1000\n",
      "Epoch 217/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7720 - accuracy: 0.8676\n",
      "Epoch 217: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.7720 - accuracy: 0.8676 - val_loss: 1.6414 - val_accuracy: 0.7198 - lr: 0.1000\n",
      "Epoch 218/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7871 - accuracy: 0.8629\n",
      "Epoch 218: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.7871 - accuracy: 0.8629 - val_loss: 1.6499 - val_accuracy: 0.7094 - lr: 0.1000\n",
      "Epoch 219/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7826 - accuracy: 0.8649\n",
      "Epoch 219: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.7826 - accuracy: 0.8649 - val_loss: 1.6399 - val_accuracy: 0.7178 - lr: 0.1000\n",
      "Epoch 220/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7764 - accuracy: 0.8653\n",
      "Epoch 220: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.7764 - accuracy: 0.8653 - val_loss: 1.7217 - val_accuracy: 0.7042 - lr: 0.1000\n",
      "Epoch 221/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7662 - accuracy: 0.8678\n",
      "Epoch 221: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.7662 - accuracy: 0.8678 - val_loss: 1.6407 - val_accuracy: 0.7236 - lr: 0.1000\n",
      "Epoch 222/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7770 - accuracy: 0.8649\n",
      "Epoch 222: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.7770 - accuracy: 0.8649 - val_loss: 1.6565 - val_accuracy: 0.7180 - lr: 0.1000\n",
      "Epoch 223/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7743 - accuracy: 0.8646\n",
      "Epoch 223: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.7743 - accuracy: 0.8646 - val_loss: 1.6198 - val_accuracy: 0.7220 - lr: 0.1000\n",
      "Epoch 224/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7701 - accuracy: 0.8681\n",
      "Epoch 224: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.7701 - accuracy: 0.8681 - val_loss: 1.6257 - val_accuracy: 0.7246 - lr: 0.1000\n",
      "Epoch 225/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7742 - accuracy: 0.8668\n",
      "Epoch 225: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 36s 104ms/step - loss: 0.7742 - accuracy: 0.8668 - val_loss: 1.6573 - val_accuracy: 0.7232 - lr: 0.1000\n",
      "Epoch 226/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7789 - accuracy: 0.8627\n",
      "Epoch 226: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.7789 - accuracy: 0.8627 - val_loss: 1.6216 - val_accuracy: 0.7294 - lr: 0.1000\n",
      "Epoch 227/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7566 - accuracy: 0.8703\n",
      "Epoch 227: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.7566 - accuracy: 0.8703 - val_loss: 1.7157 - val_accuracy: 0.7146 - lr: 0.1000\n",
      "Epoch 228/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7494 - accuracy: 0.8728\n",
      "Epoch 228: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.7494 - accuracy: 0.8728 - val_loss: 1.6545 - val_accuracy: 0.7188 - lr: 0.1000\n",
      "Epoch 229/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7523 - accuracy: 0.8706\n",
      "Epoch 229: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.7523 - accuracy: 0.8706 - val_loss: 1.6531 - val_accuracy: 0.7166 - lr: 0.1000\n",
      "Epoch 230/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7500 - accuracy: 0.8721\n",
      "Epoch 230: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.7500 - accuracy: 0.8721 - val_loss: 1.6440 - val_accuracy: 0.7184 - lr: 0.1000\n",
      "Epoch 231/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7549 - accuracy: 0.8693\n",
      "Epoch 231: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.7549 - accuracy: 0.8693 - val_loss: 1.6059 - val_accuracy: 0.7222 - lr: 0.1000\n",
      "Epoch 232/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7384 - accuracy: 0.8751\n",
      "Epoch 232: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.7384 - accuracy: 0.8751 - val_loss: 1.6211 - val_accuracy: 0.7092 - lr: 0.1000\n",
      "Epoch 233/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7566 - accuracy: 0.8683\n",
      "Epoch 233: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.7566 - accuracy: 0.8683 - val_loss: 1.6268 - val_accuracy: 0.7256 - lr: 0.1000\n",
      "Epoch 234/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7493 - accuracy: 0.8703\n",
      "Epoch 234: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.7493 - accuracy: 0.8703 - val_loss: 1.6559 - val_accuracy: 0.7126 - lr: 0.1000\n",
      "Epoch 235/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7328 - accuracy: 0.8761\n",
      "Epoch 235: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.7328 - accuracy: 0.8761 - val_loss: 1.6723 - val_accuracy: 0.7192 - lr: 0.1000\n",
      "Epoch 236/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7331 - accuracy: 0.8732\n",
      "Epoch 236: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.7331 - accuracy: 0.8732 - val_loss: 1.7020 - val_accuracy: 0.7068 - lr: 0.1000\n",
      "Epoch 237/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7471 - accuracy: 0.8726\n",
      "Epoch 237: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.7471 - accuracy: 0.8726 - val_loss: 1.6145 - val_accuracy: 0.7196 - lr: 0.1000\n",
      "Epoch 238/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7326 - accuracy: 0.8749\n",
      "Epoch 238: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.7326 - accuracy: 0.8749 - val_loss: 1.6810 - val_accuracy: 0.7164 - lr: 0.1000\n",
      "Epoch 239/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7373 - accuracy: 0.8727\n",
      "Epoch 239: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 110ms/step - loss: 0.7373 - accuracy: 0.8727 - val_loss: 1.5999 - val_accuracy: 0.7224 - lr: 0.1000\n",
      "Epoch 240/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7255 - accuracy: 0.8768\n",
      "Epoch 240: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.7255 - accuracy: 0.8768 - val_loss: 1.6057 - val_accuracy: 0.7216 - lr: 0.1000\n",
      "Epoch 241/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7320 - accuracy: 0.8752\n",
      "Epoch 241: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.7320 - accuracy: 0.8752 - val_loss: 1.6362 - val_accuracy: 0.7224 - lr: 0.1000\n",
      "Epoch 242/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7472 - accuracy: 0.8721\n",
      "Epoch 242: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 40s 112ms/step - loss: 0.7472 - accuracy: 0.8721 - val_loss: 1.7118 - val_accuracy: 0.7118 - lr: 0.1000\n",
      "Epoch 243/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7423 - accuracy: 0.8726\n",
      "Epoch 243: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.7423 - accuracy: 0.8726 - val_loss: 1.6371 - val_accuracy: 0.7186 - lr: 0.1000\n",
      "Epoch 244/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7304 - accuracy: 0.8744\n",
      "Epoch 244: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.7304 - accuracy: 0.8744 - val_loss: 1.6194 - val_accuracy: 0.7214 - lr: 0.1000\n",
      "Epoch 245/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7370 - accuracy: 0.8750\n",
      "Epoch 245: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 39s 112ms/step - loss: 0.7370 - accuracy: 0.8750 - val_loss: 1.6492 - val_accuracy: 0.7170 - lr: 0.1000\n",
      "Epoch 246/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7408 - accuracy: 0.8719\n",
      "Epoch 246: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.7408 - accuracy: 0.8719 - val_loss: 1.6274 - val_accuracy: 0.7156 - lr: 0.1000\n",
      "Epoch 247/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7383 - accuracy: 0.8728\n",
      "Epoch 247: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.7383 - accuracy: 0.8728 - val_loss: 1.6428 - val_accuracy: 0.7182 - lr: 0.1000\n",
      "Epoch 248/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7205 - accuracy: 0.8794\n",
      "Epoch 248: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.7205 - accuracy: 0.8794 - val_loss: 1.6693 - val_accuracy: 0.7146 - lr: 0.1000\n",
      "Epoch 249/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7136 - accuracy: 0.8803\n",
      "Epoch 249: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.7136 - accuracy: 0.8803 - val_loss: 1.6408 - val_accuracy: 0.7232 - lr: 0.1000\n",
      "Epoch 250/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.7232 - accuracy: 0.8769\n",
      "Epoch 250: val_loss did not improve from 1.56296\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.7232 - accuracy: 0.8769 - val_loss: 1.6038 - val_accuracy: 0.7262 - lr: 0.1000\n",
      "Epoch 251/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.8923\n",
      "Epoch 251: val_loss improved from 1.56296 to 1.51603, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 109ms/step - loss: 0.6771 - accuracy: 0.8923 - val_loss: 1.5160 - val_accuracy: 0.7408 - lr: 0.0100\n",
      "Epoch 252/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6277 - accuracy: 0.9065\n",
      "Epoch 252: val_loss did not improve from 1.51603\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.6277 - accuracy: 0.9065 - val_loss: 1.5236 - val_accuracy: 0.7372 - lr: 0.0100\n",
      "Epoch 253/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6268 - accuracy: 0.9067\n",
      "Epoch 253: val_loss did not improve from 1.51603\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.6268 - accuracy: 0.9067 - val_loss: 1.5172 - val_accuracy: 0.7390 - lr: 0.0100\n",
      "Epoch 254/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6080 - accuracy: 0.9130\n",
      "Epoch 254: val_loss improved from 1.51603 to 1.50739, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.6080 - accuracy: 0.9130 - val_loss: 1.5074 - val_accuracy: 0.7412 - lr: 0.0100\n",
      "Epoch 255/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6165 - accuracy: 0.9127\n",
      "Epoch 255: val_loss did not improve from 1.50739\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.6165 - accuracy: 0.9127 - val_loss: 1.5136 - val_accuracy: 0.7444 - lr: 0.0100\n",
      "Epoch 256/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.6001 - accuracy: 0.9181\n",
      "Epoch 256: val_loss did not improve from 1.50739\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.6001 - accuracy: 0.9181 - val_loss: 1.5241 - val_accuracy: 0.7430 - lr: 0.0100\n",
      "Epoch 257/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5897 - accuracy: 0.9212\n",
      "Epoch 257: val_loss improved from 1.50739 to 1.50360, saving model to model_resnet110SD_c100_best.hdf5\n",
      "351/351 [==============================] - 38s 108ms/step - loss: 0.5897 - accuracy: 0.9212 - val_loss: 1.5036 - val_accuracy: 0.7428 - lr: 0.0100\n",
      "Epoch 258/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5818 - accuracy: 0.9233\n",
      "Epoch 258: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5818 - accuracy: 0.9233 - val_loss: 1.5137 - val_accuracy: 0.7446 - lr: 0.0100\n",
      "Epoch 259/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.9219\n",
      "Epoch 259: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 104ms/step - loss: 0.5859 - accuracy: 0.9219 - val_loss: 1.5201 - val_accuracy: 0.7430 - lr: 0.0100\n",
      "Epoch 260/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5757 - accuracy: 0.9263\n",
      "Epoch 260: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.5757 - accuracy: 0.9263 - val_loss: 1.5075 - val_accuracy: 0.7464 - lr: 0.0100\n",
      "Epoch 261/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.9257\n",
      "Epoch 261: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.5701 - accuracy: 0.9257 - val_loss: 1.5091 - val_accuracy: 0.7452 - lr: 0.0100\n",
      "Epoch 262/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5690 - accuracy: 0.9259\n",
      "Epoch 262: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.5690 - accuracy: 0.9259 - val_loss: 1.5161 - val_accuracy: 0.7458 - lr: 0.0100\n",
      "Epoch 263/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5601 - accuracy: 0.9288\n",
      "Epoch 263: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.5601 - accuracy: 0.9288 - val_loss: 1.5128 - val_accuracy: 0.7478 - lr: 0.0100\n",
      "Epoch 264/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.9305\n",
      "Epoch 264: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.5588 - accuracy: 0.9305 - val_loss: 1.5058 - val_accuracy: 0.7450 - lr: 0.0100\n",
      "Epoch 265/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5585 - accuracy: 0.9302\n",
      "Epoch 265: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5585 - accuracy: 0.9302 - val_loss: 1.5203 - val_accuracy: 0.7464 - lr: 0.0100\n",
      "Epoch 266/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.9315\n",
      "Epoch 266: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.5558 - accuracy: 0.9315 - val_loss: 1.5075 - val_accuracy: 0.7456 - lr: 0.0100\n",
      "Epoch 267/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5516 - accuracy: 0.9312\n",
      "Epoch 267: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.5516 - accuracy: 0.9312 - val_loss: 1.5122 - val_accuracy: 0.7476 - lr: 0.0100\n",
      "Epoch 268/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5463 - accuracy: 0.9331\n",
      "Epoch 268: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5463 - accuracy: 0.9331 - val_loss: 1.5064 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 269/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5538 - accuracy: 0.9310\n",
      "Epoch 269: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5538 - accuracy: 0.9310 - val_loss: 1.5111 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 270/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5472 - accuracy: 0.9325\n",
      "Epoch 270: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.5472 - accuracy: 0.9325 - val_loss: 1.5145 - val_accuracy: 0.7478 - lr: 0.0100\n",
      "Epoch 271/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5449 - accuracy: 0.9338\n",
      "Epoch 271: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.5449 - accuracy: 0.9338 - val_loss: 1.5107 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 272/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.9337\n",
      "Epoch 272: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5437 - accuracy: 0.9337 - val_loss: 1.5105 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 273/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.9360\n",
      "Epoch 273: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.5329 - accuracy: 0.9360 - val_loss: 1.5042 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 274/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5312 - accuracy: 0.9374\n",
      "Epoch 274: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.5312 - accuracy: 0.9374 - val_loss: 1.5100 - val_accuracy: 0.7468 - lr: 0.0100\n",
      "Epoch 275/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.9367\n",
      "Epoch 275: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5343 - accuracy: 0.9367 - val_loss: 1.5152 - val_accuracy: 0.7490 - lr: 0.0100\n",
      "Epoch 276/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.9368\n",
      "Epoch 276: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5347 - accuracy: 0.9368 - val_loss: 1.5282 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 277/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.9382\n",
      "Epoch 277: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5255 - accuracy: 0.9382 - val_loss: 1.5223 - val_accuracy: 0.7470 - lr: 0.0100\n",
      "Epoch 278/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.9366\n",
      "Epoch 278: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5289 - accuracy: 0.9366 - val_loss: 1.5138 - val_accuracy: 0.7474 - lr: 0.0100\n",
      "Epoch 279/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.9340\n",
      "Epoch 279: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5427 - accuracy: 0.9340 - val_loss: 1.5202 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 280/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.9389\n",
      "Epoch 280: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5261 - accuracy: 0.9389 - val_loss: 1.5135 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 281/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.9394\n",
      "Epoch 281: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.5243 - accuracy: 0.9394 - val_loss: 1.5111 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 282/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.9389\n",
      "Epoch 282: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.5234 - accuracy: 0.9389 - val_loss: 1.5145 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 283/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5164 - accuracy: 0.9415\n",
      "Epoch 283: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.5164 - accuracy: 0.9415 - val_loss: 1.5141 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 284/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5233 - accuracy: 0.9399\n",
      "Epoch 284: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.5233 - accuracy: 0.9399 - val_loss: 1.5114 - val_accuracy: 0.7468 - lr: 0.0100\n",
      "Epoch 285/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.9416\n",
      "Epoch 285: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5155 - accuracy: 0.9416 - val_loss: 1.5206 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 286/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.9415\n",
      "Epoch 286: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5155 - accuracy: 0.9415 - val_loss: 1.5188 - val_accuracy: 0.7474 - lr: 0.0100\n",
      "Epoch 287/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.9402\n",
      "Epoch 287: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5165 - accuracy: 0.9402 - val_loss: 1.5226 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 288/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5249 - accuracy: 0.9376\n",
      "Epoch 288: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5249 - accuracy: 0.9376 - val_loss: 1.5304 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 289/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5281 - accuracy: 0.9374\n",
      "Epoch 289: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5281 - accuracy: 0.9374 - val_loss: 1.5249 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 290/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.9406\n",
      "Epoch 290: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5170 - accuracy: 0.9406 - val_loss: 1.5241 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 291/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.9384\n",
      "Epoch 291: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 38s 107ms/step - loss: 0.5251 - accuracy: 0.9384 - val_loss: 1.5286 - val_accuracy: 0.7476 - lr: 0.0100\n",
      "Epoch 292/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.9425\n",
      "Epoch 292: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.5091 - accuracy: 0.9425 - val_loss: 1.5280 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 293/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.9406\n",
      "Epoch 293: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5138 - accuracy: 0.9406 - val_loss: 1.5282 - val_accuracy: 0.7480 - lr: 0.0100\n",
      "Epoch 294/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.9429\n",
      "Epoch 294: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5084 - accuracy: 0.9429 - val_loss: 1.5187 - val_accuracy: 0.7450 - lr: 0.0100\n",
      "Epoch 295/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5058 - accuracy: 0.9441\n",
      "Epoch 295: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.5058 - accuracy: 0.9441 - val_loss: 1.5205 - val_accuracy: 0.7522 - lr: 0.0100\n",
      "Epoch 296/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.9390\n",
      "Epoch 296: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 105ms/step - loss: 0.5170 - accuracy: 0.9390 - val_loss: 1.5290 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 297/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.9456\n",
      "Epoch 297: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 103ms/step - loss: 0.4969 - accuracy: 0.9456 - val_loss: 1.5121 - val_accuracy: 0.7472 - lr: 0.0100\n",
      "Epoch 298/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.9427\n",
      "Epoch 298: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 106ms/step - loss: 0.5080 - accuracy: 0.9427 - val_loss: 1.5283 - val_accuracy: 0.7490 - lr: 0.0100\n",
      "Epoch 299/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4984 - accuracy: 0.9456\n",
      "Epoch 299: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 37s 104ms/step - loss: 0.4984 - accuracy: 0.9456 - val_loss: 1.5214 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 300/300\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.9447\n",
      "Epoch 300: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 39s 111ms/step - loss: 0.5005 - accuracy: 0.9447 - val_loss: 1.5239 - val_accuracy: 0.7492 - lr: 0.0100\n",
      "Epoch 1/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5506 - accuracy: 0.9287\n",
      "Epoch 1: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 46s 102ms/step - loss: 0.5506 - accuracy: 0.9287 - val_loss: 1.5239 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.9327\n",
      "Epoch 2: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5364 - accuracy: 0.9327 - val_loss: 1.5246 - val_accuracy: 0.7490 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5482 - accuracy: 0.9287\n",
      "Epoch 3: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5482 - accuracy: 0.9287 - val_loss: 1.5299 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5539 - accuracy: 0.9273\n",
      "Epoch 4: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5539 - accuracy: 0.9273 - val_loss: 1.5271 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5339 - accuracy: 0.9340\n",
      "Epoch 5: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5339 - accuracy: 0.9340 - val_loss: 1.5255 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5373 - accuracy: 0.9329\n",
      "Epoch 6: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5373 - accuracy: 0.9329 - val_loss: 1.5284 - val_accuracy: 0.7494 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.9345\n",
      "Epoch 7: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.5242 - accuracy: 0.9345 - val_loss: 1.5265 - val_accuracy: 0.7482 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5479 - accuracy: 0.9285\n",
      "Epoch 8: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5479 - accuracy: 0.9285 - val_loss: 1.5342 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5286 - accuracy: 0.9346\n",
      "Epoch 9: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 96ms/step - loss: 0.5286 - accuracy: 0.9346 - val_loss: 1.5279 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.9310\n",
      "Epoch 10: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 96ms/step - loss: 0.5427 - accuracy: 0.9310 - val_loss: 1.5316 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.9364\n",
      "Epoch 11: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5258 - accuracy: 0.9364 - val_loss: 1.5278 - val_accuracy: 0.7494 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.9337\n",
      "Epoch 12: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5342 - accuracy: 0.9337 - val_loss: 1.5334 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5507 - accuracy: 0.9292\n",
      "Epoch 13: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5507 - accuracy: 0.9292 - val_loss: 1.5312 - val_accuracy: 0.7504 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5478 - accuracy: 0.9303\n",
      "Epoch 14: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5478 - accuracy: 0.9303 - val_loss: 1.5372 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5235 - accuracy: 0.9370\n",
      "Epoch 15: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5235 - accuracy: 0.9370 - val_loss: 1.5344 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.9311\n",
      "Epoch 16: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5427 - accuracy: 0.9311 - val_loss: 1.5417 - val_accuracy: 0.7492 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5418 - accuracy: 0.9297\n",
      "Epoch 17: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 35s 97ms/step - loss: 0.5418 - accuracy: 0.9297 - val_loss: 1.5486 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.9384\n",
      "Epoch 18: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5188 - accuracy: 0.9384 - val_loss: 1.5392 - val_accuracy: 0.7492 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.9348\n",
      "Epoch 19: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 98ms/step - loss: 0.5292 - accuracy: 0.9348 - val_loss: 1.5344 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.9331\n",
      "Epoch 20: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5313 - accuracy: 0.9331 - val_loss: 1.5484 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5260 - accuracy: 0.9355\n",
      "Epoch 21: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.5260 - accuracy: 0.9355 - val_loss: 1.5417 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5162 - accuracy: 0.9381\n",
      "Epoch 22: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5162 - accuracy: 0.9381 - val_loss: 1.5443 - val_accuracy: 0.7478 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.9304\n",
      "Epoch 23: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5416 - accuracy: 0.9304 - val_loss: 1.5387 - val_accuracy: 0.7468 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.9336\n",
      "Epoch 24: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5372 - accuracy: 0.9336 - val_loss: 1.5391 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5124 - accuracy: 0.9405\n",
      "Epoch 25: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 97ms/step - loss: 0.5124 - accuracy: 0.9405 - val_loss: 1.5423 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.9318\n",
      "Epoch 26: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5350 - accuracy: 0.9318 - val_loss: 1.5474 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.9304\n",
      "Epoch 27: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 35s 99ms/step - loss: 0.5342 - accuracy: 0.9304 - val_loss: 1.5449 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 28/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.9321\n",
      "Epoch 28: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 35s 98ms/step - loss: 0.5300 - accuracy: 0.9321 - val_loss: 1.5518 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 29/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5254 - accuracy: 0.9340\n",
      "Epoch 29: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 35s 99ms/step - loss: 0.5254 - accuracy: 0.9340 - val_loss: 1.5499 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 30/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5196 - accuracy: 0.9371\n",
      "Epoch 30: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5196 - accuracy: 0.9371 - val_loss: 1.5417 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 31/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5124 - accuracy: 0.9389\n",
      "Epoch 31: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.5124 - accuracy: 0.9389 - val_loss: 1.5462 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 32/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.9334\n",
      "Epoch 32: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5274 - accuracy: 0.9334 - val_loss: 1.5530 - val_accuracy: 0.7482 - lr: 0.0100\n",
      "Epoch 33/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.9375\n",
      "Epoch 33: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5163 - accuracy: 0.9375 - val_loss: 1.5471 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 34/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.9343\n",
      "Epoch 34: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5242 - accuracy: 0.9343 - val_loss: 1.5560 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 35/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.9378\n",
      "Epoch 35: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5198 - accuracy: 0.9378 - val_loss: 1.5421 - val_accuracy: 0.7474 - lr: 0.0100\n",
      "Epoch 36/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.9367\n",
      "Epoch 36: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 35s 98ms/step - loss: 0.5232 - accuracy: 0.9367 - val_loss: 1.5480 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 37/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5341 - accuracy: 0.9346\n",
      "Epoch 37: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 92ms/step - loss: 0.5341 - accuracy: 0.9346 - val_loss: 1.5539 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 38/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.9379\n",
      "Epoch 38: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5132 - accuracy: 0.9379 - val_loss: 1.5462 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 39/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.9388\n",
      "Epoch 39: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5127 - accuracy: 0.9388 - val_loss: 1.5442 - val_accuracy: 0.7480 - lr: 0.0100\n",
      "Epoch 40/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.9348\n",
      "Epoch 40: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5271 - accuracy: 0.9348 - val_loss: 1.5515 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 41/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.9388\n",
      "Epoch 41: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 35s 100ms/step - loss: 0.5105 - accuracy: 0.9388 - val_loss: 1.5522 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 42/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5196 - accuracy: 0.9357\n",
      "Epoch 42: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5196 - accuracy: 0.9357 - val_loss: 1.5505 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 43/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5148 - accuracy: 0.9390\n",
      "Epoch 43: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5148 - accuracy: 0.9390 - val_loss: 1.5526 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 44/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.9402\n",
      "Epoch 44: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5048 - accuracy: 0.9402 - val_loss: 1.5502 - val_accuracy: 0.7494 - lr: 0.0100\n",
      "Epoch 45/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.9339\n",
      "Epoch 45: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5271 - accuracy: 0.9339 - val_loss: 1.5624 - val_accuracy: 0.7506 - lr: 0.0100\n",
      "Epoch 46/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.9397\n",
      "Epoch 46: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5108 - accuracy: 0.9397 - val_loss: 1.5547 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 47/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.9359\n",
      "Epoch 47: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5221 - accuracy: 0.9359 - val_loss: 1.5541 - val_accuracy: 0.7508 - lr: 0.0100\n",
      "Epoch 48/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.9344\n",
      "Epoch 48: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5255 - accuracy: 0.9344 - val_loss: 1.5568 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 49/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.9368\n",
      "Epoch 49: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5172 - accuracy: 0.9368 - val_loss: 1.5579 - val_accuracy: 0.7512 - lr: 0.0100\n",
      "Epoch 50/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.9397\n",
      "Epoch 50: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 97ms/step - loss: 0.5105 - accuracy: 0.9397 - val_loss: 1.5508 - val_accuracy: 0.7492 - lr: 0.0100\n",
      "Epoch 51/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.9333\n",
      "Epoch 51: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.5261 - accuracy: 0.9333 - val_loss: 1.5597 - val_accuracy: 0.7478 - lr: 0.0100\n",
      "Epoch 52/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.9380\n",
      "Epoch 52: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5127 - accuracy: 0.9380 - val_loss: 1.5535 - val_accuracy: 0.7490 - lr: 0.0100\n",
      "Epoch 53/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.9347\n",
      "Epoch 53: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 35s 99ms/step - loss: 0.5258 - accuracy: 0.9347 - val_loss: 1.5547 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 54/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5287 - accuracy: 0.9324\n",
      "Epoch 54: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5287 - accuracy: 0.9324 - val_loss: 1.5581 - val_accuracy: 0.7484 - lr: 0.0100\n",
      "Epoch 55/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5215 - accuracy: 0.9363\n",
      "Epoch 55: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 98ms/step - loss: 0.5215 - accuracy: 0.9363 - val_loss: 1.5522 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 56/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5359 - accuracy: 0.9318\n",
      "Epoch 56: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5359 - accuracy: 0.9318 - val_loss: 1.5647 - val_accuracy: 0.7520 - lr: 0.0100\n",
      "Epoch 57/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.9357\n",
      "Epoch 57: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5270 - accuracy: 0.9357 - val_loss: 1.5530 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 58/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5183 - accuracy: 0.9365\n",
      "Epoch 58: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 97ms/step - loss: 0.5183 - accuracy: 0.9365 - val_loss: 1.5626 - val_accuracy: 0.7490 - lr: 0.0100\n",
      "Epoch 59/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.9385\n",
      "Epoch 59: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5079 - accuracy: 0.9385 - val_loss: 1.5534 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 60/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.9364\n",
      "Epoch 60: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 97ms/step - loss: 0.5170 - accuracy: 0.9364 - val_loss: 1.5583 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 61/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.9411\n",
      "Epoch 61: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 101ms/step - loss: 0.5081 - accuracy: 0.9411 - val_loss: 1.5574 - val_accuracy: 0.7498 - lr: 0.0100\n",
      "Epoch 62/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.9366\n",
      "Epoch 62: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 35s 99ms/step - loss: 0.5188 - accuracy: 0.9366 - val_loss: 1.5503 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 63/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5182 - accuracy: 0.9341\n",
      "Epoch 63: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 98ms/step - loss: 0.5182 - accuracy: 0.9341 - val_loss: 1.5575 - val_accuracy: 0.7486 - lr: 0.0100\n",
      "Epoch 64/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.9406\n",
      "Epoch 64: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5046 - accuracy: 0.9406 - val_loss: 1.5634 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 65/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.9421\n",
      "Epoch 65: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5042 - accuracy: 0.9421 - val_loss: 1.5564 - val_accuracy: 0.7496 - lr: 0.0100\n",
      "Epoch 66/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5301 - accuracy: 0.9324\n",
      "Epoch 66: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5301 - accuracy: 0.9324 - val_loss: 1.5645 - val_accuracy: 0.7512 - lr: 0.0100\n",
      "Epoch 67/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.9390\n",
      "Epoch 67: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5094 - accuracy: 0.9390 - val_loss: 1.5562 - val_accuracy: 0.7510 - lr: 0.0100\n",
      "Epoch 68/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4958 - accuracy: 0.9430\n",
      "Epoch 68: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.4958 - accuracy: 0.9430 - val_loss: 1.5585 - val_accuracy: 0.7514 - lr: 0.0100\n",
      "Epoch 69/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.9329\n",
      "Epoch 69: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5255 - accuracy: 0.9329 - val_loss: 1.5647 - val_accuracy: 0.7500 - lr: 0.0100\n",
      "Epoch 70/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4881 - accuracy: 0.9458\n",
      "Epoch 70: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 98ms/step - loss: 0.4881 - accuracy: 0.9458 - val_loss: 1.5597 - val_accuracy: 0.7492 - lr: 0.0100\n",
      "Epoch 71/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.9372\n",
      "Epoch 71: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.5114 - accuracy: 0.9372 - val_loss: 1.5552 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 72/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5215 - accuracy: 0.9346\n",
      "Epoch 72: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5215 - accuracy: 0.9346 - val_loss: 1.5663 - val_accuracy: 0.7488 - lr: 0.0100\n",
      "Epoch 73/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.9366\n",
      "Epoch 73: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5163 - accuracy: 0.9366 - val_loss: 1.5581 - val_accuracy: 0.7502 - lr: 0.0100\n",
      "Epoch 74/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5068 - accuracy: 0.9390\n",
      "Epoch 74: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5068 - accuracy: 0.9390 - val_loss: 1.5555 - val_accuracy: 0.7490 - lr: 0.0100\n",
      "Epoch 75/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.9325\n",
      "Epoch 75: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5300 - accuracy: 0.9325 - val_loss: 1.5663 - val_accuracy: 0.7514 - lr: 0.0100\n",
      "Epoch 76/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.9394\n",
      "Epoch 76: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5072 - accuracy: 0.9394 - val_loss: 1.5616 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5019 - accuracy: 0.9420\n",
      "Epoch 77: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5019 - accuracy: 0.9420 - val_loss: 1.5532 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5135 - accuracy: 0.9362\n",
      "Epoch 78: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5135 - accuracy: 0.9362 - val_loss: 1.5630 - val_accuracy: 0.7498 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.9382\n",
      "Epoch 79: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5064 - accuracy: 0.9382 - val_loss: 1.5573 - val_accuracy: 0.7498 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.9394\n",
      "Epoch 80: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5080 - accuracy: 0.9394 - val_loss: 1.5616 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4973 - accuracy: 0.9425\n",
      "Epoch 81: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.4973 - accuracy: 0.9425 - val_loss: 1.5557 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.9326\n",
      "Epoch 82: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5289 - accuracy: 0.9326 - val_loss: 1.5672 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5226 - accuracy: 0.9374\n",
      "Epoch 83: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5226 - accuracy: 0.9374 - val_loss: 1.5634 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.9374\n",
      "Epoch 84: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5132 - accuracy: 0.9374 - val_loss: 1.5657 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.9408\n",
      "Epoch 85: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5047 - accuracy: 0.9408 - val_loss: 1.5598 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.9364\n",
      "Epoch 86: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5179 - accuracy: 0.9364 - val_loss: 1.5630 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.9401\n",
      "Epoch 87: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5064 - accuracy: 0.9401 - val_loss: 1.5605 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.9392\n",
      "Epoch 88: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5071 - accuracy: 0.9392 - val_loss: 1.5613 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5154 - accuracy: 0.9362\n",
      "Epoch 89: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5154 - accuracy: 0.9362 - val_loss: 1.5654 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5124 - accuracy: 0.9372\n",
      "Epoch 90: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5124 - accuracy: 0.9372 - val_loss: 1.5636 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5097 - accuracy: 0.9382\n",
      "Epoch 91: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 92ms/step - loss: 0.5097 - accuracy: 0.9382 - val_loss: 1.5642 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.9406\n",
      "Epoch 92: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5028 - accuracy: 0.9406 - val_loss: 1.5593 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.9404\n",
      "Epoch 93: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5040 - accuracy: 0.9404 - val_loss: 1.5559 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5015 - accuracy: 0.9426\n",
      "Epoch 94: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5015 - accuracy: 0.9426 - val_loss: 1.5617 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.9378\n",
      "Epoch 95: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5138 - accuracy: 0.9378 - val_loss: 1.5676 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.9370\n",
      "Epoch 96: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5141 - accuracy: 0.9370 - val_loss: 1.5592 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.9398\n",
      "Epoch 97: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5071 - accuracy: 0.9398 - val_loss: 1.5631 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.9370\n",
      "Epoch 98: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5125 - accuracy: 0.9370 - val_loss: 1.5632 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5146 - accuracy: 0.9382\n",
      "Epoch 99: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5146 - accuracy: 0.9382 - val_loss: 1.5645 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4966 - accuracy: 0.9429\n",
      "Epoch 100: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.4966 - accuracy: 0.9429 - val_loss: 1.5574 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.9400\n",
      "Epoch 101: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5070 - accuracy: 0.9400 - val_loss: 1.5606 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4886 - accuracy: 0.9456\n",
      "Epoch 102: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.4886 - accuracy: 0.9456 - val_loss: 1.5557 - val_accuracy: 0.7498 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4944 - accuracy: 0.9446\n",
      "Epoch 103: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.4944 - accuracy: 0.9446 - val_loss: 1.5485 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5152 - accuracy: 0.9385\n",
      "Epoch 104: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5152 - accuracy: 0.9385 - val_loss: 1.5660 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.9417\n",
      "Epoch 105: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5032 - accuracy: 0.9417 - val_loss: 1.5612 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.9397\n",
      "Epoch 106: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5040 - accuracy: 0.9397 - val_loss: 1.5677 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5158 - accuracy: 0.9368\n",
      "Epoch 107: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5158 - accuracy: 0.9368 - val_loss: 1.5572 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4956 - accuracy: 0.9418\n",
      "Epoch 108: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.4956 - accuracy: 0.9418 - val_loss: 1.5669 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5017 - accuracy: 0.9412\n",
      "Epoch 109: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 96ms/step - loss: 0.5017 - accuracy: 0.9412 - val_loss: 1.5639 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5017 - accuracy: 0.9407\n",
      "Epoch 110: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5017 - accuracy: 0.9407 - val_loss: 1.5546 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.9379\n",
      "Epoch 111: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5150 - accuracy: 0.9379 - val_loss: 1.5633 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5018 - accuracy: 0.9414\n",
      "Epoch 112: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5018 - accuracy: 0.9414 - val_loss: 1.5616 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.9370\n",
      "Epoch 113: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5120 - accuracy: 0.9370 - val_loss: 1.5694 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5133 - accuracy: 0.9374\n",
      "Epoch 114: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5133 - accuracy: 0.9374 - val_loss: 1.5685 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.9385\n",
      "Epoch 115: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5125 - accuracy: 0.9385 - val_loss: 1.5666 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.9401\n",
      "Epoch 116: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5041 - accuracy: 0.9401 - val_loss: 1.5654 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5115 - accuracy: 0.9390\n",
      "Epoch 117: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5115 - accuracy: 0.9390 - val_loss: 1.5634 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.9403\n",
      "Epoch 118: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.5042 - accuracy: 0.9403 - val_loss: 1.5626 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.9427\n",
      "Epoch 119: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 97ms/step - loss: 0.4976 - accuracy: 0.9427 - val_loss: 1.5657 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4981 - accuracy: 0.9425\n",
      "Epoch 120: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.4981 - accuracy: 0.9425 - val_loss: 1.5565 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4967 - accuracy: 0.9438\n",
      "Epoch 121: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.4967 - accuracy: 0.9438 - val_loss: 1.5586 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.9402\n",
      "Epoch 122: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 87ms/step - loss: 0.5050 - accuracy: 0.9402 - val_loss: 1.5621 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.9375\n",
      "Epoch 123: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5123 - accuracy: 0.9375 - val_loss: 1.5624 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5110 - accuracy: 0.9387\n",
      "Epoch 124: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.5110 - accuracy: 0.9387 - val_loss: 1.5644 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.9403\n",
      "Epoch 125: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5072 - accuracy: 0.9403 - val_loss: 1.5641 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.9402\n",
      "Epoch 126: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5023 - accuracy: 0.9402 - val_loss: 1.5676 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5011 - accuracy: 0.9423\n",
      "Epoch 127: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5011 - accuracy: 0.9423 - val_loss: 1.5576 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.9419\n",
      "Epoch 128: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5013 - accuracy: 0.9419 - val_loss: 1.5623 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.9332\n",
      "Epoch 129: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5279 - accuracy: 0.9332 - val_loss: 1.5667 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5010 - accuracy: 0.9415\n",
      "Epoch 130: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5010 - accuracy: 0.9415 - val_loss: 1.5622 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5029 - accuracy: 0.9394\n",
      "Epoch 131: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5029 - accuracy: 0.9394 - val_loss: 1.5658 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4938 - accuracy: 0.9434\n",
      "Epoch 132: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.4938 - accuracy: 0.9434 - val_loss: 1.5649 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.9427\n",
      "Epoch 133: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.4993 - accuracy: 0.9427 - val_loss: 1.5666 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.9392\n",
      "Epoch 134: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5067 - accuracy: 0.9392 - val_loss: 1.5651 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.9393\n",
      "Epoch 135: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5043 - accuracy: 0.9393 - val_loss: 1.5610 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.9402\n",
      "Epoch 136: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5070 - accuracy: 0.9402 - val_loss: 1.5665 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.9358\n",
      "Epoch 137: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5176 - accuracy: 0.9358 - val_loss: 1.5672 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.9418\n",
      "Epoch 138: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5037 - accuracy: 0.9418 - val_loss: 1.5634 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4955 - accuracy: 0.9430\n",
      "Epoch 139: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.4955 - accuracy: 0.9430 - val_loss: 1.5667 - val_accuracy: 0.7518 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.9374\n",
      "Epoch 140: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5132 - accuracy: 0.9374 - val_loss: 1.5723 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5330 - accuracy: 0.9308\n",
      "Epoch 141: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5330 - accuracy: 0.9308 - val_loss: 1.5699 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.9368\n",
      "Epoch 142: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5179 - accuracy: 0.9368 - val_loss: 1.5678 - val_accuracy: 0.7518 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.9328\n",
      "Epoch 143: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5256 - accuracy: 0.9328 - val_loss: 1.5685 - val_accuracy: 0.7516 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.9413\n",
      "Epoch 144: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5005 - accuracy: 0.9413 - val_loss: 1.5698 - val_accuracy: 0.7516 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5010 - accuracy: 0.9427\n",
      "Epoch 145: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5010 - accuracy: 0.9427 - val_loss: 1.5606 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.9388\n",
      "Epoch 146: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5130 - accuracy: 0.9388 - val_loss: 1.5664 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4958 - accuracy: 0.9424\n",
      "Epoch 147: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.4958 - accuracy: 0.9424 - val_loss: 1.5609 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5086 - accuracy: 0.9409\n",
      "Epoch 148: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 98ms/step - loss: 0.5086 - accuracy: 0.9409 - val_loss: 1.5622 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4972 - accuracy: 0.9413\n",
      "Epoch 149: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 97ms/step - loss: 0.4972 - accuracy: 0.9413 - val_loss: 1.5585 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.9415\n",
      "Epoch 150: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5059 - accuracy: 0.9415 - val_loss: 1.5580 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.9399\n",
      "Epoch 151: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 36s 102ms/step - loss: 0.5071 - accuracy: 0.9399 - val_loss: 1.5711 - val_accuracy: 0.7516 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.9396\n",
      "Epoch 152: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5045 - accuracy: 0.9396 - val_loss: 1.5636 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.9416\n",
      "Epoch 153: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.4996 - accuracy: 0.9416 - val_loss: 1.5661 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5097 - accuracy: 0.9379\n",
      "Epoch 154: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 85ms/step - loss: 0.5097 - accuracy: 0.9379 - val_loss: 1.5681 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5014 - accuracy: 0.9425\n",
      "Epoch 155: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5014 - accuracy: 0.9425 - val_loss: 1.5636 - val_accuracy: 0.7518 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5087 - accuracy: 0.9394\n",
      "Epoch 156: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5087 - accuracy: 0.9394 - val_loss: 1.5639 - val_accuracy: 0.7516 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.9407\n",
      "Epoch 157: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5052 - accuracy: 0.9407 - val_loss: 1.5642 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.9419\n",
      "Epoch 158: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 87ms/step - loss: 0.5023 - accuracy: 0.9419 - val_loss: 1.5582 - val_accuracy: 0.7496 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.9394\n",
      "Epoch 159: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5100 - accuracy: 0.9394 - val_loss: 1.5647 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.9388\n",
      "Epoch 160: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5065 - accuracy: 0.9388 - val_loss: 1.5666 - val_accuracy: 0.7516 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.9399\n",
      "Epoch 161: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5028 - accuracy: 0.9399 - val_loss: 1.5640 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.9396\n",
      "Epoch 162: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.5063 - accuracy: 0.9396 - val_loss: 1.5637 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.9411\n",
      "Epoch 163: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 30s 86ms/step - loss: 0.4996 - accuracy: 0.9411 - val_loss: 1.5655 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.9401\n",
      "Epoch 164: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5094 - accuracy: 0.9401 - val_loss: 1.5651 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4983 - accuracy: 0.9407\n",
      "Epoch 165: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.4983 - accuracy: 0.9407 - val_loss: 1.5582 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5055 - accuracy: 0.9398\n",
      "Epoch 166: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5055 - accuracy: 0.9398 - val_loss: 1.5618 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.9435\n",
      "Epoch 167: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.4952 - accuracy: 0.9435 - val_loss: 1.5591 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5061 - accuracy: 0.9392\n",
      "Epoch 168: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5061 - accuracy: 0.9392 - val_loss: 1.5640 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.9398\n",
      "Epoch 169: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5084 - accuracy: 0.9398 - val_loss: 1.5669 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.9416\n",
      "Epoch 170: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 94ms/step - loss: 0.4994 - accuracy: 0.9416 - val_loss: 1.5557 - val_accuracy: 0.7498 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4977 - accuracy: 0.9421\n",
      "Epoch 171: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 93ms/step - loss: 0.4977 - accuracy: 0.9421 - val_loss: 1.5616 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.9405\n",
      "Epoch 172: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5077 - accuracy: 0.9405 - val_loss: 1.5694 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4979 - accuracy: 0.9435\n",
      "Epoch 173: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.4979 - accuracy: 0.9435 - val_loss: 1.5605 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4864 - accuracy: 0.9457\n",
      "Epoch 174: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 97ms/step - loss: 0.4864 - accuracy: 0.9457 - val_loss: 1.5672 - val_accuracy: 0.7516 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.9364\n",
      "Epoch 175: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5172 - accuracy: 0.9364 - val_loss: 1.5702 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5139 - accuracy: 0.9378\n",
      "Epoch 176: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5139 - accuracy: 0.9378 - val_loss: 1.5650 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.9417\n",
      "Epoch 177: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.4976 - accuracy: 0.9417 - val_loss: 1.5625 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5017 - accuracy: 0.9419\n",
      "Epoch 178: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5017 - accuracy: 0.9419 - val_loss: 1.5594 - val_accuracy: 0.7498 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.9361\n",
      "Epoch 179: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 33s 95ms/step - loss: 0.5155 - accuracy: 0.9361 - val_loss: 1.5719 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.9393\n",
      "Epoch 180: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5075 - accuracy: 0.9393 - val_loss: 1.5657 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.9404\n",
      "Epoch 181: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5028 - accuracy: 0.9404 - val_loss: 1.5654 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5088 - accuracy: 0.9382\n",
      "Epoch 182: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5088 - accuracy: 0.9382 - val_loss: 1.5710 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.9409\n",
      "Epoch 183: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 97ms/step - loss: 0.5047 - accuracy: 0.9409 - val_loss: 1.5640 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5061 - accuracy: 0.9401\n",
      "Epoch 184: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5061 - accuracy: 0.9401 - val_loss: 1.5660 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.9407\n",
      "Epoch 185: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5076 - accuracy: 0.9407 - val_loss: 1.5673 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.9344\n",
      "Epoch 186: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5239 - accuracy: 0.9344 - val_loss: 1.5633 - val_accuracy: 0.7502 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.9384\n",
      "Epoch 187: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 89ms/step - loss: 0.5098 - accuracy: 0.9384 - val_loss: 1.5656 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.9383\n",
      "Epoch 188: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5100 - accuracy: 0.9383 - val_loss: 1.5728 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4951 - accuracy: 0.9434\n",
      "Epoch 189: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.4951 - accuracy: 0.9434 - val_loss: 1.5581 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.9397\n",
      "Epoch 190: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 87ms/step - loss: 0.5064 - accuracy: 0.9397 - val_loss: 1.5660 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.9395\n",
      "Epoch 191: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5057 - accuracy: 0.9395 - val_loss: 1.5663 - val_accuracy: 0.7506 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4940 - accuracy: 0.9442\n",
      "Epoch 192: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 98ms/step - loss: 0.4940 - accuracy: 0.9442 - val_loss: 1.5569 - val_accuracy: 0.7498 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5099 - accuracy: 0.9391\n",
      "Epoch 193: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.5099 - accuracy: 0.9391 - val_loss: 1.5623 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.9379\n",
      "Epoch 194: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.5112 - accuracy: 0.9379 - val_loss: 1.5693 - val_accuracy: 0.7516 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.9417\n",
      "Epoch 195: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 90ms/step - loss: 0.4985 - accuracy: 0.9417 - val_loss: 1.5648 - val_accuracy: 0.7504 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.9401\n",
      "Epoch 196: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5026 - accuracy: 0.9401 - val_loss: 1.5727 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5111 - accuracy: 0.9374\n",
      "Epoch 197: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 34s 96ms/step - loss: 0.5111 - accuracy: 0.9374 - val_loss: 1.5674 - val_accuracy: 0.7514 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4924 - accuracy: 0.9438\n",
      "Epoch 198: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 32s 91ms/step - loss: 0.4924 - accuracy: 0.9438 - val_loss: 1.5616 - val_accuracy: 0.7508 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.4988 - accuracy: 0.9411\n",
      "Epoch 199: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.4988 - accuracy: 0.9411 - val_loss: 1.5700 - val_accuracy: 0.7516 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "351/351 [==============================] - ETA: 0s - loss: 0.5107 - accuracy: 0.9385\n",
      "Epoch 200: val_loss did not improve from 1.50360\n",
      "351/351 [==============================] - 31s 88ms/step - loss: 0.5107 - accuracy: 0.9385 - val_loss: 1.5609 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Current:  311\n",
      "313/313 [==============================] - 7s 15ms/step\n",
      "Accuracy: 75.11\n",
      "Error: 24.89\n",
      "ECE: 0.13913255594223736\n",
      "MCE: 0.2953859572101412\n",
      "Loss: 1.2317225282402715\n",
      "brier: 0.22313591422790868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[24.89,\n",
       " 0.13913255594223736,\n",
       " 0.2953859572101412,\n",
       " 1.2317225282402715,\n",
       " 0.22313591422790868]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, img_gen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list, batch_size=128,lr_schedule = [[0, 0.1],[nb_epochs*0.5,0.01],[nb_epochs*0.75,0.001]],cbks=[checkpointer], name='resnet_sd_cifar100')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
