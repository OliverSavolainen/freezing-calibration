{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87015b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T09:56:09.385191Z",
     "iopub.status.busy": "2023-04-24T09:56:09.384369Z",
     "iopub.status.idle": "2023-04-24T09:56:16.932798Z",
     "shell.execute_reply": "2023-04-24T09:56:16.931673Z"
    },
    "papermill": {
     "duration": 7.557217,
     "end_time": "2023-04-24T09:56:16.935457",
     "exception": false,
     "start_time": "2023-04-24T09:56:09.378240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import cifar100\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d730219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import freezing\n",
    "import densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e509f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T09:56:24.057971Z",
     "iopub.status.busy": "2023-04-24T09:56:24.057600Z",
     "iopub.status.idle": "2023-04-24T09:56:24.066079Z",
     "shell.execute_reply": "2023-04-24T09:56:24.064971Z"
    },
    "papermill": {
     "duration": 0.058493,
     "end_time": "2023-04-24T09:56:24.068655",
     "exception": false,
     "start_time": "2023-04-24T09:56:24.010162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 100\n",
    "nb_epoch = 300\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "img_channels = 3\n",
    "\n",
    "img_dim = (img_channels, img_rows, img_cols) if K.image_data_format() == \"channels_first\" else (img_rows, img_cols, img_channels)\n",
    "depth = 40\n",
    "nb_dense_block = 3\n",
    "growth_rate = 12\n",
    "bottle_neck = False\n",
    "nb_filter = 12\n",
    "reduction = 0.0\n",
    "dropout_rate = 0.0 # 0.0 for data augmentation\n",
    "seed = 333\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Preprocessing for DenseNet https://arxiv.org/pdf/1608.06993v3.pdf\n",
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101126fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T09:56:24.162098Z",
     "iopub.status.busy": "2023-04-24T09:56:24.161737Z",
     "iopub.status.idle": "2023-04-24T09:56:32.607937Z",
     "shell.execute_reply": "2023-04-24T09:56:32.605921Z"
    },
    "papermill": {
     "duration": 8.497222,
     "end_time": "2023-04-24T09:56:32.611676",
     "exception": false,
     "start_time": "2023-04-24T09:56:24.114454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = densenet.DenseNet(img_dim, classes=nb_classes, depth=depth, nb_dense_block=nb_dense_block,\n",
    "                          growth_rate=growth_rate, nb_filter=nb_filter, dropout_rate=dropout_rate, weights=None, weight_decay=1e-4)\n",
    "print(\"Model created\")\n",
    "\n",
    "model.summary()\n",
    "sgd = SGD(learning_rate=0.1, momentum=0.9, nesterov=True)  # dampening = 0.9? Should be zero?\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "print(\"Finished compiling\")\n",
    "print(\"Building model...\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "#For data preprocessing, we normalize the data using the channel means and standard deviations (https://arxiv.org/pdf/1608.06993v3.pdf)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)\n",
    "\n",
    "x_train45, x_val, y_train45, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)  # random_state = seed\n",
    "\n",
    "\n",
    "img_gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.125,  # 0.125*32 = 4 so max padding of 4 pixels, as described in paper.\n",
    "    height_shift_range=0.125,  # first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32Ã—32 images\n",
    "    fill_mode = \"constant\",\n",
    "    cval = 0\n",
    ")\n",
    "\n",
    "y_train45 = to_categorical(y_train45, nb_classes)  # 1-hot vector\n",
    "y_val = to_categorical(y_val, nb_classes)\n",
    "y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "img_gen.fit(x_train45, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2640b52b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T09:56:32.845553Z",
     "iopub.status.busy": "2023-04-24T09:56:32.844657Z",
     "iopub.status.idle": "2023-04-24T09:56:32.878928Z",
     "shell.execute_reply": "2023-04-24T09:56:32.877956Z"
    },
    "papermill": {
     "duration": 0.108223,
     "end_time": "2023-04-24T09:56:32.881130",
     "exception": false,
     "start_time": "2023-04-24T09:56:32.772907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freezing_list = []\n",
    "for i in range(len(model.layers)):\n",
    "  if i < len(model.layers) * 0.8:\n",
    "    freezing_list.append(int(nb_epoch*0.6))\n",
    "freezing_list.append(nb_epoch)\n",
    "checkpointer = ModelCheckpoint('model_cont_dense_c100_best.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec53127a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T09:56:33.023627Z",
     "iopub.status.busy": "2023-04-24T09:56:33.023267Z",
     "iopub.status.idle": "2023-04-24T14:20:05.677344Z",
     "shell.execute_reply": "2023-04-24T14:20:05.676222Z"
    },
    "papermill": {
     "duration": 15812.728353,
     "end_time": "2023-04-24T14:20:05.679501",
     "exception": false,
     "start_time": "2023-04-24T09:56:32.951148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-24 09:56:33        68745\n",
      "variables.h5                                   2023-04-24 09:56:33      4622744\n",
      "metadata.json                                  2023-04-24 09:56:33           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-24 09:56:32        68745\n",
      "variables.h5                                   2023-04-24 09:56:32      4622744\n",
      "metadata.json                                  2023-04-24 09:56:32           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......activation\n",
      ".........vars\n",
      "......activation_1\n",
      ".........vars\n",
      "......activation_10\n",
      ".........vars\n",
      "......activation_11\n",
      ".........vars\n",
      "......activation_12\n",
      ".........vars\n",
      "......activation_13\n",
      ".........vars\n",
      "......activation_14\n",
      ".........vars\n",
      "......activation_15\n",
      ".........vars\n",
      "......activation_16\n",
      ".........vars\n",
      "......activation_17\n",
      ".........vars\n",
      "......activation_18\n",
      ".........vars\n",
      "......activation_19\n",
      ".........vars\n",
      "......activation_2\n",
      ".........vars\n",
      "......activation_20\n",
      ".........vars\n",
      "......activation_21\n",
      ".........vars\n",
      "......activation_22\n",
      ".........vars\n",
      "......activation_23\n",
      ".........vars\n",
      "......activation_24\n",
      ".........vars\n",
      "......activation_25\n",
      ".........vars\n",
      "......activation_26\n",
      ".........vars\n",
      "......activation_27\n",
      ".........vars\n",
      "......activation_28\n",
      ".........vars\n",
      "......activation_29\n",
      ".........vars\n",
      "......activation_3\n",
      ".........vars\n",
      "......activation_30\n",
      ".........vars\n",
      "......activation_31\n",
      ".........vars\n",
      "......activation_32\n",
      ".........vars\n",
      "......activation_33\n",
      ".........vars\n",
      "......activation_34\n",
      ".........vars\n",
      "......activation_35\n",
      ".........vars\n",
      "......activation_36\n",
      ".........vars\n",
      "......activation_37\n",
      ".........vars\n",
      "......activation_38\n",
      ".........vars\n",
      "......activation_4\n",
      ".........vars\n",
      "......activation_5\n",
      ".........vars\n",
      "......activation_6\n",
      ".........vars\n",
      "......activation_7\n",
      ".........vars\n",
      "......activation_8\n",
      ".........vars\n",
      "......activation_9\n",
      ".........vars\n",
      "......average_pooling2d\n",
      ".........vars\n",
      "......average_pooling2d_1\n",
      ".........vars\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_13\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_14\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_15\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_16\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_17\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_18\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_19\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_20\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_21\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_22\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_23\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_24\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_25\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_26\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_27\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_28\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_29\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_30\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_31\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_32\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_33\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_34\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_35\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_36\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_37\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_38\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......concatenate\n",
      ".........vars\n",
      "......concatenate_1\n",
      ".........vars\n",
      "......concatenate_10\n",
      ".........vars\n",
      "......concatenate_11\n",
      ".........vars\n",
      "......concatenate_12\n",
      ".........vars\n",
      "......concatenate_13\n",
      ".........vars\n",
      "......concatenate_14\n",
      ".........vars\n",
      "......concatenate_15\n",
      ".........vars\n",
      "......concatenate_16\n",
      ".........vars\n",
      "......concatenate_17\n",
      ".........vars\n",
      "......concatenate_18\n",
      ".........vars\n",
      "......concatenate_19\n",
      ".........vars\n",
      "......concatenate_2\n",
      ".........vars\n",
      "......concatenate_20\n",
      ".........vars\n",
      "......concatenate_21\n",
      ".........vars\n",
      "......concatenate_22\n",
      ".........vars\n",
      "......concatenate_23\n",
      ".........vars\n",
      "......concatenate_24\n",
      ".........vars\n",
      "......concatenate_25\n",
      ".........vars\n",
      "......concatenate_26\n",
      ".........vars\n",
      "......concatenate_27\n",
      ".........vars\n",
      "......concatenate_28\n",
      ".........vars\n",
      "......concatenate_29\n",
      ".........vars\n",
      "......concatenate_3\n",
      ".........vars\n",
      "......concatenate_30\n",
      ".........vars\n",
      "......concatenate_31\n",
      ".........vars\n",
      "......concatenate_32\n",
      ".........vars\n",
      "......concatenate_33\n",
      ".........vars\n",
      "......concatenate_34\n",
      ".........vars\n",
      "......concatenate_35\n",
      ".........vars\n",
      "......concatenate_4\n",
      ".........vars\n",
      "......concatenate_5\n",
      ".........vars\n",
      "......concatenate_6\n",
      ".........vars\n",
      "......concatenate_7\n",
      ".........vars\n",
      "......concatenate_8\n",
      ".........vars\n",
      "......concatenate_9\n",
      ".........vars\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_13\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_14\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_15\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_16\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_17\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_18\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_19\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_20\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_21\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_22\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_23\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_24\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_25\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_26\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_27\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_28\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_29\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_30\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_31\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_32\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_33\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_34\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_35\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_36\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_37\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_38\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "...vars\n",
      "Epoch 1/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 3.9321 - accuracy: 0.1120\n",
      "Epoch 1: val_loss improved from inf to 3.93197, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 61s 72ms/step - loss: 3.9321 - accuracy: 0.1120 - val_loss: 3.9320 - val_accuracy: 0.1460 - lr: 0.1000\n",
      "Epoch 2/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 3.2170 - accuracy: 0.2278\n",
      "Epoch 2: val_loss improved from 3.93197 to 3.61623, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 3.2170 - accuracy: 0.2278 - val_loss: 3.6162 - val_accuracy: 0.2054 - lr: 0.1000\n",
      "Epoch 3/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 2.7487 - accuracy: 0.3139\n",
      "Epoch 3: val_loss improved from 3.61623 to 3.24349, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 2.7487 - accuracy: 0.3139 - val_loss: 3.2435 - val_accuracy: 0.2520 - lr: 0.1000\n",
      "Epoch 4/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 2.4283 - accuracy: 0.3802\n",
      "Epoch 4: val_loss improved from 3.24349 to 3.02025, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 2.4283 - accuracy: 0.3802 - val_loss: 3.0203 - val_accuracy: 0.3232 - lr: 0.1000\n",
      "Epoch 5/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 2.2102 - accuracy: 0.4298\n",
      "Epoch 5: val_loss improved from 3.02025 to 2.50899, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 2.2102 - accuracy: 0.4298 - val_loss: 2.5090 - val_accuracy: 0.3736 - lr: 0.1000\n",
      "Epoch 6/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 2.0352 - accuracy: 0.4725\n",
      "Epoch 6: val_loss improved from 2.50899 to 2.48138, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 50s 70ms/step - loss: 2.0352 - accuracy: 0.4725 - val_loss: 2.4814 - val_accuracy: 0.4034 - lr: 0.1000\n",
      "Epoch 7/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.9025 - accuracy: 0.5018\n",
      "Epoch 7: val_loss improved from 2.48138 to 2.37448, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.9025 - accuracy: 0.5018 - val_loss: 2.3745 - val_accuracy: 0.4398 - lr: 0.1000\n",
      "Epoch 8/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.7846 - accuracy: 0.5332\n",
      "Epoch 8: val_loss did not improve from 2.37448\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.7846 - accuracy: 0.5332 - val_loss: 2.4668 - val_accuracy: 0.4208 - lr: 0.1000\n",
      "Epoch 9/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.6808 - accuracy: 0.5583\n",
      "Epoch 9: val_loss improved from 2.37448 to 2.29826, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.6808 - accuracy: 0.5583 - val_loss: 2.2983 - val_accuracy: 0.4780 - lr: 0.1000\n",
      "Epoch 10/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.5942 - accuracy: 0.5816\n",
      "Epoch 10: val_loss did not improve from 2.29826\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.5942 - accuracy: 0.5816 - val_loss: 2.3556 - val_accuracy: 0.4622 - lr: 0.1000\n",
      "Epoch 11/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.5192 - accuracy: 0.5986\n",
      "Epoch 11: val_loss improved from 2.29826 to 2.08333, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 1.5192 - accuracy: 0.5986 - val_loss: 2.0833 - val_accuracy: 0.5020 - lr: 0.1000\n",
      "Epoch 12/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.4614 - accuracy: 0.6134\n",
      "Epoch 12: val_loss improved from 2.08333 to 1.95941, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 1.4614 - accuracy: 0.6134 - val_loss: 1.9594 - val_accuracy: 0.5298 - lr: 0.1000\n",
      "Epoch 13/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.4109 - accuracy: 0.6262\n",
      "Epoch 13: val_loss improved from 1.95941 to 1.91527, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.4109 - accuracy: 0.6262 - val_loss: 1.9153 - val_accuracy: 0.5414 - lr: 0.1000\n",
      "Epoch 14/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.3518 - accuracy: 0.6455\n",
      "Epoch 14: val_loss did not improve from 1.91527\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.3518 - accuracy: 0.6455 - val_loss: 2.0233 - val_accuracy: 0.5294 - lr: 0.1000\n",
      "Epoch 15/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.3126 - accuracy: 0.6544\n",
      "Epoch 15: val_loss did not improve from 1.91527\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.3126 - accuracy: 0.6544 - val_loss: 2.0382 - val_accuracy: 0.5158 - lr: 0.1000\n",
      "Epoch 16/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.2740 - accuracy: 0.6643\n",
      "Epoch 16: val_loss did not improve from 1.91527\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.2740 - accuracy: 0.6643 - val_loss: 1.9576 - val_accuracy: 0.5498 - lr: 0.1000\n",
      "Epoch 17/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.2413 - accuracy: 0.6755\n",
      "Epoch 17: val_loss improved from 1.91527 to 1.83368, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 1.2413 - accuracy: 0.6755 - val_loss: 1.8337 - val_accuracy: 0.5650 - lr: 0.1000\n",
      "Epoch 18/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.2072 - accuracy: 0.6829\n",
      "Epoch 18: val_loss did not improve from 1.83368\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 1.2072 - accuracy: 0.6829 - val_loss: 1.9113 - val_accuracy: 0.5638 - lr: 0.1000\n",
      "Epoch 19/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.1780 - accuracy: 0.6949\n",
      "Epoch 19: val_loss did not improve from 1.83368\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 1.1780 - accuracy: 0.6949 - val_loss: 1.8652 - val_accuracy: 0.5702 - lr: 0.1000\n",
      "Epoch 20/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.1490 - accuracy: 0.7002\n",
      "Epoch 20: val_loss did not improve from 1.83368\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 1.1490 - accuracy: 0.7002 - val_loss: 1.8497 - val_accuracy: 0.5706 - lr: 0.1000\n",
      "Epoch 21/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.1217 - accuracy: 0.7081\n",
      "Epoch 21: val_loss did not improve from 1.83368\n",
      "703/703 [==============================] - 50s 70ms/step - loss: 1.1217 - accuracy: 0.7081 - val_loss: 1.8813 - val_accuracy: 0.5714 - lr: 0.1000\n",
      "Epoch 22/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0939 - accuracy: 0.7175\n",
      "Epoch 22: val_loss improved from 1.83368 to 1.70474, saving model to model_cont_dense_c100_best.hdf5\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 1.0939 - accuracy: 0.7175 - val_loss: 1.7047 - val_accuracy: 0.6090 - lr: 0.1000\n",
      "Epoch 23/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0624 - accuracy: 0.7253\n",
      "Epoch 23: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.0624 - accuracy: 0.7253 - val_loss: 1.8578 - val_accuracy: 0.5910 - lr: 0.1000\n",
      "Epoch 24/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0584 - accuracy: 0.7285\n",
      "Epoch 24: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 1.0584 - accuracy: 0.7285 - val_loss: 1.9451 - val_accuracy: 0.5908 - lr: 0.1000\n",
      "Epoch 25/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0256 - accuracy: 0.7364\n",
      "Epoch 25: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 1.0256 - accuracy: 0.7364 - val_loss: 1.7115 - val_accuracy: 0.6012 - lr: 0.1000\n",
      "Epoch 26/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 1.0104 - accuracy: 0.7423\n",
      "Epoch 26: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 1.0104 - accuracy: 0.7423 - val_loss: 1.8928 - val_accuracy: 0.5874 - lr: 0.1000\n",
      "Epoch 27/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9859 - accuracy: 0.7496\n",
      "Epoch 27: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.9859 - accuracy: 0.7496 - val_loss: 1.7695 - val_accuracy: 0.6048 - lr: 0.1000\n",
      "Epoch 28/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9777 - accuracy: 0.7500\n",
      "Epoch 28: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.9777 - accuracy: 0.7500 - val_loss: 1.8239 - val_accuracy: 0.5992 - lr: 0.1000\n",
      "Epoch 29/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9515 - accuracy: 0.7584\n",
      "Epoch 29: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.9515 - accuracy: 0.7584 - val_loss: 1.8537 - val_accuracy: 0.6070 - lr: 0.1000\n",
      "Epoch 30/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9399 - accuracy: 0.7636\n",
      "Epoch 30: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.9399 - accuracy: 0.7636 - val_loss: 1.7351 - val_accuracy: 0.6086 - lr: 0.1000\n",
      "Epoch 31/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.7694\n",
      "Epoch 31: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.9172 - accuracy: 0.7694 - val_loss: 1.8530 - val_accuracy: 0.6102 - lr: 0.1000\n",
      "Epoch 32/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.9048 - accuracy: 0.7709\n",
      "Epoch 32: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.9048 - accuracy: 0.7709 - val_loss: 1.9388 - val_accuracy: 0.5952 - lr: 0.1000\n",
      "Epoch 33/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8896 - accuracy: 0.7774\n",
      "Epoch 33: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.8896 - accuracy: 0.7774 - val_loss: 1.9315 - val_accuracy: 0.6110 - lr: 0.1000\n",
      "Epoch 34/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8752 - accuracy: 0.7839\n",
      "Epoch 34: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.8752 - accuracy: 0.7839 - val_loss: 1.9504 - val_accuracy: 0.6122 - lr: 0.1000\n",
      "Epoch 35/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8698 - accuracy: 0.7824\n",
      "Epoch 35: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.8698 - accuracy: 0.7824 - val_loss: 1.7782 - val_accuracy: 0.6232 - lr: 0.1000\n",
      "Epoch 36/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8487 - accuracy: 0.7895\n",
      "Epoch 36: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.8487 - accuracy: 0.7895 - val_loss: 1.9162 - val_accuracy: 0.6192 - lr: 0.1000\n",
      "Epoch 37/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8431 - accuracy: 0.7907\n",
      "Epoch 37: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 70ms/step - loss: 0.8431 - accuracy: 0.7907 - val_loss: 2.0255 - val_accuracy: 0.5986 - lr: 0.1000\n",
      "Epoch 38/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8306 - accuracy: 0.7929\n",
      "Epoch 38: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.8306 - accuracy: 0.7929 - val_loss: 1.8270 - val_accuracy: 0.6104 - lr: 0.1000\n",
      "Epoch 39/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.8058 - accuracy: 0.8009\n",
      "Epoch 39: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.8058 - accuracy: 0.8009 - val_loss: 2.0223 - val_accuracy: 0.6126 - lr: 0.1000\n",
      "Epoch 40/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7943 - accuracy: 0.8051\n",
      "Epoch 40: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.7943 - accuracy: 0.8051 - val_loss: 1.9299 - val_accuracy: 0.6020 - lr: 0.1000\n",
      "Epoch 41/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7887 - accuracy: 0.8098\n",
      "Epoch 41: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 70ms/step - loss: 0.7887 - accuracy: 0.8098 - val_loss: 1.8866 - val_accuracy: 0.6072 - lr: 0.1000\n",
      "Epoch 42/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7864 - accuracy: 0.8088\n",
      "Epoch 42: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.7864 - accuracy: 0.8088 - val_loss: 2.0305 - val_accuracy: 0.6080 - lr: 0.1000\n",
      "Epoch 43/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7806 - accuracy: 0.8101\n",
      "Epoch 43: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.7806 - accuracy: 0.8101 - val_loss: 1.8046 - val_accuracy: 0.6292 - lr: 0.1000\n",
      "Epoch 44/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7594 - accuracy: 0.8152\n",
      "Epoch 44: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.7594 - accuracy: 0.8152 - val_loss: 1.8675 - val_accuracy: 0.6340 - lr: 0.1000\n",
      "Epoch 45/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7488 - accuracy: 0.8201\n",
      "Epoch 45: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.7488 - accuracy: 0.8201 - val_loss: 1.8534 - val_accuracy: 0.6242 - lr: 0.1000\n",
      "Epoch 46/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7432 - accuracy: 0.8218\n",
      "Epoch 46: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.7432 - accuracy: 0.8218 - val_loss: 2.2028 - val_accuracy: 0.6042 - lr: 0.1000\n",
      "Epoch 47/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7399 - accuracy: 0.8225\n",
      "Epoch 47: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.7399 - accuracy: 0.8225 - val_loss: 2.1980 - val_accuracy: 0.6006 - lr: 0.1000\n",
      "Epoch 48/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7269 - accuracy: 0.8260\n",
      "Epoch 48: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.7269 - accuracy: 0.8260 - val_loss: 2.0219 - val_accuracy: 0.6150 - lr: 0.1000\n",
      "Epoch 49/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7176 - accuracy: 0.8299\n",
      "Epoch 49: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.7176 - accuracy: 0.8299 - val_loss: 1.8242 - val_accuracy: 0.6352 - lr: 0.1000\n",
      "Epoch 50/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7046 - accuracy: 0.8333\n",
      "Epoch 50: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.7046 - accuracy: 0.8333 - val_loss: 2.0241 - val_accuracy: 0.6114 - lr: 0.1000\n",
      "Epoch 51/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.7115 - accuracy: 0.8324\n",
      "Epoch 51: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.7115 - accuracy: 0.8324 - val_loss: 1.9710 - val_accuracy: 0.6270 - lr: 0.1000\n",
      "Epoch 52/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.8383\n",
      "Epoch 52: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.6937 - accuracy: 0.8383 - val_loss: 2.1358 - val_accuracy: 0.6114 - lr: 0.1000\n",
      "Epoch 53/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6851 - accuracy: 0.8380\n",
      "Epoch 53: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.6851 - accuracy: 0.8380 - val_loss: 2.1343 - val_accuracy: 0.6090 - lr: 0.1000\n",
      "Epoch 54/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.8422\n",
      "Epoch 54: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.6803 - accuracy: 0.8422 - val_loss: 2.1644 - val_accuracy: 0.6036 - lr: 0.1000\n",
      "Epoch 55/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6710 - accuracy: 0.8436\n",
      "Epoch 55: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.6710 - accuracy: 0.8436 - val_loss: 2.0029 - val_accuracy: 0.6240 - lr: 0.1000\n",
      "Epoch 56/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6683 - accuracy: 0.8451\n",
      "Epoch 56: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.6683 - accuracy: 0.8451 - val_loss: 2.0403 - val_accuracy: 0.6220 - lr: 0.1000\n",
      "Epoch 57/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.8491\n",
      "Epoch 57: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.6571 - accuracy: 0.8491 - val_loss: 2.3139 - val_accuracy: 0.5812 - lr: 0.1000\n",
      "Epoch 58/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6728 - accuracy: 0.8459\n",
      "Epoch 58: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.6728 - accuracy: 0.8459 - val_loss: 2.0856 - val_accuracy: 0.6112 - lr: 0.1000\n",
      "Epoch 59/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6426 - accuracy: 0.8523\n",
      "Epoch 59: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.6426 - accuracy: 0.8523 - val_loss: 2.0758 - val_accuracy: 0.6150 - lr: 0.1000\n",
      "Epoch 60/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6441 - accuracy: 0.8519\n",
      "Epoch 60: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.6441 - accuracy: 0.8519 - val_loss: 2.2826 - val_accuracy: 0.6112 - lr: 0.1000\n",
      "Epoch 61/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.8528\n",
      "Epoch 61: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.6392 - accuracy: 0.8528 - val_loss: 2.0573 - val_accuracy: 0.6188 - lr: 0.1000\n",
      "Epoch 62/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.8608\n",
      "Epoch 62: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.6191 - accuracy: 0.8608 - val_loss: 2.2129 - val_accuracy: 0.6122 - lr: 0.1000\n",
      "Epoch 63/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6363 - accuracy: 0.8535\n",
      "Epoch 63: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.6363 - accuracy: 0.8535 - val_loss: 2.1309 - val_accuracy: 0.6336 - lr: 0.1000\n",
      "Epoch 64/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6118 - accuracy: 0.8635\n",
      "Epoch 64: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.6118 - accuracy: 0.8635 - val_loss: 2.3146 - val_accuracy: 0.5978 - lr: 0.1000\n",
      "Epoch 65/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6144 - accuracy: 0.8618\n",
      "Epoch 65: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.6144 - accuracy: 0.8618 - val_loss: 2.2295 - val_accuracy: 0.6166 - lr: 0.1000\n",
      "Epoch 66/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6101 - accuracy: 0.8654\n",
      "Epoch 66: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.6101 - accuracy: 0.8654 - val_loss: 2.1793 - val_accuracy: 0.6220 - lr: 0.1000\n",
      "Epoch 67/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.8658\n",
      "Epoch 67: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5984 - accuracy: 0.8658 - val_loss: 2.2654 - val_accuracy: 0.6138 - lr: 0.1000\n",
      "Epoch 68/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6112 - accuracy: 0.8642\n",
      "Epoch 68: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.6112 - accuracy: 0.8642 - val_loss: 2.0798 - val_accuracy: 0.6372 - lr: 0.1000\n",
      "Epoch 69/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5975 - accuracy: 0.8687\n",
      "Epoch 69: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.5975 - accuracy: 0.8687 - val_loss: 2.4072 - val_accuracy: 0.6106 - lr: 0.1000\n",
      "Epoch 70/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.6029 - accuracy: 0.8660\n",
      "Epoch 70: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.6029 - accuracy: 0.8660 - val_loss: 2.2226 - val_accuracy: 0.6142 - lr: 0.1000\n",
      "Epoch 71/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.8709\n",
      "Epoch 71: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5858 - accuracy: 0.8709 - val_loss: 2.1143 - val_accuracy: 0.6346 - lr: 0.1000\n",
      "Epoch 72/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5916 - accuracy: 0.8699\n",
      "Epoch 72: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5916 - accuracy: 0.8699 - val_loss: 2.1620 - val_accuracy: 0.6332 - lr: 0.1000\n",
      "Epoch 73/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5931 - accuracy: 0.8697\n",
      "Epoch 73: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5931 - accuracy: 0.8697 - val_loss: 2.2577 - val_accuracy: 0.6228 - lr: 0.1000\n",
      "Epoch 74/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.8743\n",
      "Epoch 74: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 70ms/step - loss: 0.5755 - accuracy: 0.8743 - val_loss: 2.0922 - val_accuracy: 0.6368 - lr: 0.1000\n",
      "Epoch 75/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.8762\n",
      "Epoch 75: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5696 - accuracy: 0.8762 - val_loss: 2.1838 - val_accuracy: 0.6154 - lr: 0.1000\n",
      "Epoch 76/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.8763\n",
      "Epoch 76: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.5698 - accuracy: 0.8763 - val_loss: 2.1060 - val_accuracy: 0.6348 - lr: 0.1000\n",
      "Epoch 77/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.8722\n",
      "Epoch 77: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5859 - accuracy: 0.8722 - val_loss: 2.1948 - val_accuracy: 0.6286 - lr: 0.1000\n",
      "Epoch 78/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5590 - accuracy: 0.8809\n",
      "Epoch 78: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.5590 - accuracy: 0.8809 - val_loss: 2.1237 - val_accuracy: 0.6348 - lr: 0.1000\n",
      "Epoch 79/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5514 - accuracy: 0.8812\n",
      "Epoch 79: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5514 - accuracy: 0.8812 - val_loss: 2.1489 - val_accuracy: 0.6392 - lr: 0.1000\n",
      "Epoch 80/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5340 - accuracy: 0.8850\n",
      "Epoch 80: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.5340 - accuracy: 0.8850 - val_loss: 2.1425 - val_accuracy: 0.6256 - lr: 0.1000\n",
      "Epoch 81/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5508 - accuracy: 0.8801\n",
      "Epoch 81: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.5508 - accuracy: 0.8801 - val_loss: 2.0812 - val_accuracy: 0.6406 - lr: 0.1000\n",
      "Epoch 82/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5566 - accuracy: 0.8801\n",
      "Epoch 82: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5566 - accuracy: 0.8801 - val_loss: 2.2634 - val_accuracy: 0.6172 - lr: 0.1000\n",
      "Epoch 83/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5442 - accuracy: 0.8859\n",
      "Epoch 83: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5442 - accuracy: 0.8859 - val_loss: 2.3114 - val_accuracy: 0.6184 - lr: 0.1000\n",
      "Epoch 84/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.8908\n",
      "Epoch 84: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5212 - accuracy: 0.8908 - val_loss: 2.5702 - val_accuracy: 0.6018 - lr: 0.1000\n",
      "Epoch 85/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5471 - accuracy: 0.8837\n",
      "Epoch 85: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.5471 - accuracy: 0.8837 - val_loss: 2.1854 - val_accuracy: 0.6230 - lr: 0.1000\n",
      "Epoch 86/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.8898\n",
      "Epoch 86: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5259 - accuracy: 0.8898 - val_loss: 2.2695 - val_accuracy: 0.6274 - lr: 0.1000\n",
      "Epoch 87/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.8918\n",
      "Epoch 87: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5191 - accuracy: 0.8918 - val_loss: 2.1463 - val_accuracy: 0.6350 - lr: 0.1000\n",
      "Epoch 88/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5175 - accuracy: 0.8918\n",
      "Epoch 88: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.5175 - accuracy: 0.8918 - val_loss: 2.2755 - val_accuracy: 0.6186 - lr: 0.1000\n",
      "Epoch 89/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.8910\n",
      "Epoch 89: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5198 - accuracy: 0.8910 - val_loss: 2.1034 - val_accuracy: 0.6436 - lr: 0.1000\n",
      "Epoch 90/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.8886\n",
      "Epoch 90: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5294 - accuracy: 0.8886 - val_loss: 2.2516 - val_accuracy: 0.6364 - lr: 0.1000\n",
      "Epoch 91/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.8929\n",
      "Epoch 91: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.5206 - accuracy: 0.8929 - val_loss: 2.2624 - val_accuracy: 0.6308 - lr: 0.1000\n",
      "Epoch 92/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.8930\n",
      "Epoch 92: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.5163 - accuracy: 0.8930 - val_loss: 2.2526 - val_accuracy: 0.6320 - lr: 0.1000\n",
      "Epoch 93/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.8947\n",
      "Epoch 93: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 46s 66ms/step - loss: 0.5163 - accuracy: 0.8947 - val_loss: 2.2328 - val_accuracy: 0.6408 - lr: 0.1000\n",
      "Epoch 94/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4973 - accuracy: 0.8999\n",
      "Epoch 94: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4973 - accuracy: 0.8999 - val_loss: 2.4126 - val_accuracy: 0.6174 - lr: 0.1000\n",
      "Epoch 95/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4910 - accuracy: 0.9009\n",
      "Epoch 95: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4910 - accuracy: 0.9009 - val_loss: 2.2327 - val_accuracy: 0.6380 - lr: 0.1000\n",
      "Epoch 96/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5033 - accuracy: 0.8967\n",
      "Epoch 96: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.5033 - accuracy: 0.8967 - val_loss: 2.2345 - val_accuracy: 0.6390 - lr: 0.1000\n",
      "Epoch 97/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.8985\n",
      "Epoch 97: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4982 - accuracy: 0.8985 - val_loss: 2.3726 - val_accuracy: 0.6216 - lr: 0.1000\n",
      "Epoch 98/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.8952\n",
      "Epoch 98: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.5093 - accuracy: 0.8952 - val_loss: 2.3361 - val_accuracy: 0.6176 - lr: 0.1000\n",
      "Epoch 99/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.8997\n",
      "Epoch 99: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4982 - accuracy: 0.8997 - val_loss: 2.1280 - val_accuracy: 0.6350 - lr: 0.1000\n",
      "Epoch 100/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.9009\n",
      "Epoch 100: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.4913 - accuracy: 0.9009 - val_loss: 2.2767 - val_accuracy: 0.6244 - lr: 0.1000\n",
      "Epoch 101/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4848 - accuracy: 0.9029\n",
      "Epoch 101: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4848 - accuracy: 0.9029 - val_loss: 2.2511 - val_accuracy: 0.6382 - lr: 0.1000\n",
      "Epoch 102/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4759 - accuracy: 0.9052\n",
      "Epoch 102: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.4759 - accuracy: 0.9052 - val_loss: 2.2273 - val_accuracy: 0.6314 - lr: 0.1000\n",
      "Epoch 103/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4710 - accuracy: 0.9054\n",
      "Epoch 103: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4710 - accuracy: 0.9054 - val_loss: 2.3133 - val_accuracy: 0.6342 - lr: 0.1000\n",
      "Epoch 104/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4903 - accuracy: 0.9014\n",
      "Epoch 104: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4903 - accuracy: 0.9014 - val_loss: 2.1242 - val_accuracy: 0.6512 - lr: 0.1000\n",
      "Epoch 105/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.9040\n",
      "Epoch 105: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.4856 - accuracy: 0.9040 - val_loss: 2.0432 - val_accuracy: 0.6666 - lr: 0.1000\n",
      "Epoch 106/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.9057\n",
      "Epoch 106: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4715 - accuracy: 0.9057 - val_loss: 2.5686 - val_accuracy: 0.6228 - lr: 0.1000\n",
      "Epoch 107/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4798 - accuracy: 0.9047\n",
      "Epoch 107: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4798 - accuracy: 0.9047 - val_loss: 2.4310 - val_accuracy: 0.6436 - lr: 0.1000\n",
      "Epoch 108/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.9038\n",
      "Epoch 108: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4785 - accuracy: 0.9038 - val_loss: 2.1595 - val_accuracy: 0.6476 - lr: 0.1000\n",
      "Epoch 109/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4904 - accuracy: 0.9027\n",
      "Epoch 109: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4904 - accuracy: 0.9027 - val_loss: 2.3908 - val_accuracy: 0.6244 - lr: 0.1000\n",
      "Epoch 110/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4726 - accuracy: 0.9078\n",
      "Epoch 110: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4726 - accuracy: 0.9078 - val_loss: 2.2898 - val_accuracy: 0.6370 - lr: 0.1000\n",
      "Epoch 111/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.9067\n",
      "Epoch 111: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4719 - accuracy: 0.9067 - val_loss: 2.3961 - val_accuracy: 0.6196 - lr: 0.1000\n",
      "Epoch 112/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.9116\n",
      "Epoch 112: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4585 - accuracy: 0.9116 - val_loss: 2.2815 - val_accuracy: 0.6306 - lr: 0.1000\n",
      "Epoch 113/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.9124\n",
      "Epoch 113: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4510 - accuracy: 0.9124 - val_loss: 2.3567 - val_accuracy: 0.6302 - lr: 0.1000\n",
      "Epoch 114/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.9085\n",
      "Epoch 114: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4626 - accuracy: 0.9085 - val_loss: 2.5724 - val_accuracy: 0.6272 - lr: 0.1000\n",
      "Epoch 115/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.9106\n",
      "Epoch 115: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4596 - accuracy: 0.9106 - val_loss: 2.1606 - val_accuracy: 0.6522 - lr: 0.1000\n",
      "Epoch 116/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.9112\n",
      "Epoch 116: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4540 - accuracy: 0.9112 - val_loss: 2.7158 - val_accuracy: 0.6102 - lr: 0.1000\n",
      "Epoch 117/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4573 - accuracy: 0.9100\n",
      "Epoch 117: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4573 - accuracy: 0.9100 - val_loss: 2.4380 - val_accuracy: 0.6432 - lr: 0.1000\n",
      "Epoch 118/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.9145\n",
      "Epoch 118: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.4461 - accuracy: 0.9145 - val_loss: 2.2027 - val_accuracy: 0.6444 - lr: 0.1000\n",
      "Epoch 119/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.9140\n",
      "Epoch 119: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 46s 66ms/step - loss: 0.4470 - accuracy: 0.9140 - val_loss: 2.2837 - val_accuracy: 0.6282 - lr: 0.1000\n",
      "Epoch 120/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.9118\n",
      "Epoch 120: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4523 - accuracy: 0.9118 - val_loss: 2.4942 - val_accuracy: 0.6290 - lr: 0.1000\n",
      "Epoch 121/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4570 - accuracy: 0.9131\n",
      "Epoch 121: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4570 - accuracy: 0.9131 - val_loss: 2.2378 - val_accuracy: 0.6544 - lr: 0.1000\n",
      "Epoch 122/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.9157\n",
      "Epoch 122: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4440 - accuracy: 0.9157 - val_loss: 2.3410 - val_accuracy: 0.6374 - lr: 0.1000\n",
      "Epoch 123/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4368 - accuracy: 0.9193\n",
      "Epoch 123: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4368 - accuracy: 0.9193 - val_loss: 2.2651 - val_accuracy: 0.6476 - lr: 0.1000\n",
      "Epoch 124/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4332 - accuracy: 0.9178\n",
      "Epoch 124: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4332 - accuracy: 0.9178 - val_loss: 2.2941 - val_accuracy: 0.6432 - lr: 0.1000\n",
      "Epoch 125/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4393 - accuracy: 0.9168\n",
      "Epoch 125: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4393 - accuracy: 0.9168 - val_loss: 2.2028 - val_accuracy: 0.6538 - lr: 0.1000\n",
      "Epoch 126/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.9164\n",
      "Epoch 126: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4394 - accuracy: 0.9164 - val_loss: 2.6154 - val_accuracy: 0.6280 - lr: 0.1000\n",
      "Epoch 127/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4349 - accuracy: 0.9198\n",
      "Epoch 127: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4349 - accuracy: 0.9198 - val_loss: 2.2484 - val_accuracy: 0.6456 - lr: 0.1000\n",
      "Epoch 128/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.9175\n",
      "Epoch 128: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4352 - accuracy: 0.9175 - val_loss: 2.5616 - val_accuracy: 0.6280 - lr: 0.1000\n",
      "Epoch 129/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4402 - accuracy: 0.9158\n",
      "Epoch 129: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4402 - accuracy: 0.9158 - val_loss: 2.3719 - val_accuracy: 0.6328 - lr: 0.1000\n",
      "Epoch 130/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.9216\n",
      "Epoch 130: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4264 - accuracy: 0.9216 - val_loss: 2.2953 - val_accuracy: 0.6468 - lr: 0.1000\n",
      "Epoch 131/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.9203\n",
      "Epoch 131: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4252 - accuracy: 0.9203 - val_loss: 2.3558 - val_accuracy: 0.6486 - lr: 0.1000\n",
      "Epoch 132/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.9211\n",
      "Epoch 132: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 46s 66ms/step - loss: 0.4243 - accuracy: 0.9211 - val_loss: 2.5826 - val_accuracy: 0.6026 - lr: 0.1000\n",
      "Epoch 133/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.9158\n",
      "Epoch 133: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4475 - accuracy: 0.9158 - val_loss: 2.3234 - val_accuracy: 0.6302 - lr: 0.1000\n",
      "Epoch 134/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4365 - accuracy: 0.9185\n",
      "Epoch 134: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4365 - accuracy: 0.9185 - val_loss: 2.3790 - val_accuracy: 0.6458 - lr: 0.1000\n",
      "Epoch 135/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4191 - accuracy: 0.9231\n",
      "Epoch 135: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 46s 66ms/step - loss: 0.4191 - accuracy: 0.9231 - val_loss: 2.4591 - val_accuracy: 0.6394 - lr: 0.1000\n",
      "Epoch 136/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4237 - accuracy: 0.9204\n",
      "Epoch 136: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4237 - accuracy: 0.9204 - val_loss: 2.4492 - val_accuracy: 0.6412 - lr: 0.1000\n",
      "Epoch 137/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4188 - accuracy: 0.9225\n",
      "Epoch 137: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4188 - accuracy: 0.9225 - val_loss: 2.3092 - val_accuracy: 0.6370 - lr: 0.1000\n",
      "Epoch 138/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4203 - accuracy: 0.9231\n",
      "Epoch 138: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4203 - accuracy: 0.9231 - val_loss: 2.6227 - val_accuracy: 0.6124 - lr: 0.1000\n",
      "Epoch 139/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.9196\n",
      "Epoch 139: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.4318 - accuracy: 0.9196 - val_loss: 2.2654 - val_accuracy: 0.6536 - lr: 0.1000\n",
      "Epoch 140/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.9244\n",
      "Epoch 140: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4117 - accuracy: 0.9244 - val_loss: 2.3296 - val_accuracy: 0.6354 - lr: 0.1000\n",
      "Epoch 141/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4106 - accuracy: 0.9246\n",
      "Epoch 141: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4106 - accuracy: 0.9246 - val_loss: 2.3162 - val_accuracy: 0.6414 - lr: 0.1000\n",
      "Epoch 142/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.9235\n",
      "Epoch 142: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4204 - accuracy: 0.9235 - val_loss: 2.5071 - val_accuracy: 0.6394 - lr: 0.1000\n",
      "Epoch 143/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.9272\n",
      "Epoch 143: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4065 - accuracy: 0.9272 - val_loss: 2.3769 - val_accuracy: 0.6460 - lr: 0.1000\n",
      "Epoch 144/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4053 - accuracy: 0.9273\n",
      "Epoch 144: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4053 - accuracy: 0.9273 - val_loss: 2.3420 - val_accuracy: 0.6476 - lr: 0.1000\n",
      "Epoch 145/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4168 - accuracy: 0.9224\n",
      "Epoch 145: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 70ms/step - loss: 0.4168 - accuracy: 0.9224 - val_loss: 2.2922 - val_accuracy: 0.6440 - lr: 0.1000\n",
      "Epoch 146/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4141 - accuracy: 0.9239\n",
      "Epoch 146: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.4141 - accuracy: 0.9239 - val_loss: 2.3791 - val_accuracy: 0.6386 - lr: 0.1000\n",
      "Epoch 147/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4248 - accuracy: 0.9207\n",
      "Epoch 147: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4248 - accuracy: 0.9207 - val_loss: 2.2700 - val_accuracy: 0.6430 - lr: 0.1000\n",
      "Epoch 148/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.9298\n",
      "Epoch 148: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.3974 - accuracy: 0.9298 - val_loss: 2.3578 - val_accuracy: 0.6382 - lr: 0.1000\n",
      "Epoch 149/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4225 - accuracy: 0.9230\n",
      "Epoch 149: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.4225 - accuracy: 0.9230 - val_loss: 2.6830 - val_accuracy: 0.6168 - lr: 0.1000\n",
      "Epoch 150/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.4157 - accuracy: 0.9255\n",
      "Epoch 150: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.4157 - accuracy: 0.9255 - val_loss: 2.6876 - val_accuracy: 0.6124 - lr: 0.1000\n",
      "Epoch 151/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9621\n",
      "Epoch 151: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.3068 - accuracy: 0.9621 - val_loss: 1.9413 - val_accuracy: 0.6982 - lr: 0.0100\n",
      "Epoch 152/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9782\n",
      "Epoch 152: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.2580 - accuracy: 0.9782 - val_loss: 1.8886 - val_accuracy: 0.7056 - lr: 0.0100\n",
      "Epoch 153/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9841\n",
      "Epoch 153: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.2394 - accuracy: 0.9841 - val_loss: 1.8851 - val_accuracy: 0.7066 - lr: 0.0100\n",
      "Epoch 154/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.9849\n",
      "Epoch 154: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.2294 - accuracy: 0.9849 - val_loss: 1.9010 - val_accuracy: 0.7046 - lr: 0.0100\n",
      "Epoch 155/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2212 - accuracy: 0.9859\n",
      "Epoch 155: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.2212 - accuracy: 0.9859 - val_loss: 1.8862 - val_accuracy: 0.7036 - lr: 0.0100\n",
      "Epoch 156/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9870\n",
      "Epoch 156: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.2137 - accuracy: 0.9870 - val_loss: 1.8968 - val_accuracy: 0.7060 - lr: 0.0100\n",
      "Epoch 157/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9887\n",
      "Epoch 157: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.2053 - accuracy: 0.9887 - val_loss: 1.8812 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 158/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.9897\n",
      "Epoch 158: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1982 - accuracy: 0.9897 - val_loss: 1.8920 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 159/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9907\n",
      "Epoch 159: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 70ms/step - loss: 0.1909 - accuracy: 0.9907 - val_loss: 1.8904 - val_accuracy: 0.7094 - lr: 0.0100\n",
      "Epoch 160/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9902\n",
      "Epoch 160: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.1868 - accuracy: 0.9902 - val_loss: 1.8980 - val_accuracy: 0.7098 - lr: 0.0100\n",
      "Epoch 161/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9917\n",
      "Epoch 161: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 66ms/step - loss: 0.1800 - accuracy: 0.9917 - val_loss: 1.8930 - val_accuracy: 0.7092 - lr: 0.0100\n",
      "Epoch 162/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9914\n",
      "Epoch 162: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1759 - accuracy: 0.9914 - val_loss: 1.9012 - val_accuracy: 0.7110 - lr: 0.0100\n",
      "Epoch 163/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9921\n",
      "Epoch 163: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.1691 - accuracy: 0.9921 - val_loss: 1.9048 - val_accuracy: 0.7094 - lr: 0.0100\n",
      "Epoch 164/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9912\n",
      "Epoch 164: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1683 - accuracy: 0.9912 - val_loss: 1.8828 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "Epoch 165/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9927\n",
      "Epoch 165: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1611 - accuracy: 0.9927 - val_loss: 1.8906 - val_accuracy: 0.7090 - lr: 0.0100\n",
      "Epoch 166/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9931\n",
      "Epoch 166: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.1572 - accuracy: 0.9931 - val_loss: 1.9004 - val_accuracy: 0.7084 - lr: 0.0100\n",
      "Epoch 167/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9931\n",
      "Epoch 167: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.1527 - accuracy: 0.9931 - val_loss: 1.8993 - val_accuracy: 0.7098 - lr: 0.0100\n",
      "Epoch 168/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1487 - accuracy: 0.9926\n",
      "Epoch 168: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.1487 - accuracy: 0.9926 - val_loss: 1.9000 - val_accuracy: 0.7082 - lr: 0.0100\n",
      "Epoch 169/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9938\n",
      "Epoch 169: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.1437 - accuracy: 0.9938 - val_loss: 1.9089 - val_accuracy: 0.7082 - lr: 0.0100\n",
      "Epoch 170/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9941\n",
      "Epoch 170: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.1405 - accuracy: 0.9941 - val_loss: 1.9149 - val_accuracy: 0.7080 - lr: 0.0100\n",
      "Epoch 171/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9943\n",
      "Epoch 171: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.1365 - accuracy: 0.9943 - val_loss: 1.9145 - val_accuracy: 0.7096 - lr: 0.0100\n",
      "Epoch 172/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9941\n",
      "Epoch 172: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 47s 67ms/step - loss: 0.1340 - accuracy: 0.9941 - val_loss: 1.9163 - val_accuracy: 0.7098 - lr: 0.0100\n",
      "Epoch 173/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9944\n",
      "Epoch 173: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 69ms/step - loss: 0.1304 - accuracy: 0.9944 - val_loss: 1.9082 - val_accuracy: 0.7084 - lr: 0.0100\n",
      "Epoch 174/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9941\n",
      "Epoch 174: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.1276 - accuracy: 0.9941 - val_loss: 1.9157 - val_accuracy: 0.7092 - lr: 0.0100\n",
      "Epoch 175/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9945\n",
      "Epoch 175: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1239 - accuracy: 0.9945 - val_loss: 1.9149 - val_accuracy: 0.7080 - lr: 0.0100\n",
      "Epoch 176/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9950\n",
      "Epoch 176: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 69ms/step - loss: 0.1204 - accuracy: 0.9950 - val_loss: 1.9325 - val_accuracy: 0.7060 - lr: 0.0100\n",
      "Epoch 177/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9939\n",
      "Epoch 177: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 50s 71ms/step - loss: 0.1202 - accuracy: 0.9939 - val_loss: 1.9382 - val_accuracy: 0.7088 - lr: 0.0100\n",
      "Epoch 178/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9948\n",
      "Epoch 178: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 46s 66ms/step - loss: 0.1145 - accuracy: 0.9948 - val_loss: 1.9527 - val_accuracy: 0.7060 - lr: 0.0100\n",
      "Epoch 179/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9953\n",
      "Epoch 179: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 49s 70ms/step - loss: 0.1125 - accuracy: 0.9953 - val_loss: 1.9236 - val_accuracy: 0.7086 - lr: 0.0100\n",
      "Epoch 180/180\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9947\n",
      "Epoch 180: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 48s 68ms/step - loss: 0.1100 - accuracy: 0.9947 - val_loss: 1.9344 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 1/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9965\n",
      "Epoch 1: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 36s 47ms/step - loss: 0.1031 - accuracy: 0.9965 - val_loss: 1.9391 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "Epoch 2/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1030 - accuracy: 0.9967\n",
      "Epoch 2: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1030 - accuracy: 0.9968 - val_loss: 1.9408 - val_accuracy: 0.7110 - lr: 0.0100\n",
      "Epoch 3/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9971\n",
      "Epoch 3: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1020 - accuracy: 0.9971 - val_loss: 1.9467 - val_accuracy: 0.7098 - lr: 0.0100\n",
      "Epoch 4/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9967\n",
      "Epoch 4: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1020 - accuracy: 0.9968 - val_loss: 1.9557 - val_accuracy: 0.7100 - lr: 0.0100\n",
      "Epoch 5/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1027 - accuracy: 0.9963\n",
      "Epoch 5: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.1027 - accuracy: 0.9963 - val_loss: 1.9581 - val_accuracy: 0.7110 - lr: 0.0100\n",
      "Epoch 6/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9972\n",
      "Epoch 6: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1012 - accuracy: 0.9972 - val_loss: 1.9523 - val_accuracy: 0.7124 - lr: 0.0100\n",
      "Epoch 7/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1021 - accuracy: 0.9969\n",
      "Epoch 7: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1021 - accuracy: 0.9969 - val_loss: 1.9600 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "Epoch 8/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9966\n",
      "Epoch 8: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1020 - accuracy: 0.9966 - val_loss: 1.9653 - val_accuracy: 0.7120 - lr: 0.0100\n",
      "Epoch 9/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9972\n",
      "Epoch 9: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.1015 - accuracy: 0.9972 - val_loss: 1.9697 - val_accuracy: 0.7104 - lr: 0.0100\n",
      "Epoch 10/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9973\n",
      "Epoch 10: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1011 - accuracy: 0.9973 - val_loss: 1.9728 - val_accuracy: 0.7116 - lr: 0.0100\n",
      "Epoch 11/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1005 - accuracy: 0.9971\n",
      "Epoch 11: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1005 - accuracy: 0.9971 - val_loss: 1.9751 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "Epoch 12/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9970\n",
      "Epoch 12: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1009 - accuracy: 0.9970 - val_loss: 1.9769 - val_accuracy: 0.7122 - lr: 0.0100\n",
      "Epoch 13/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9969\n",
      "Epoch 13: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1012 - accuracy: 0.9969 - val_loss: 1.9785 - val_accuracy: 0.7134 - lr: 0.0100\n",
      "Epoch 14/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1014 - accuracy: 0.9970\n",
      "Epoch 14: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.1014 - accuracy: 0.9970 - val_loss: 1.9853 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 15/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1004 - accuracy: 0.9972\n",
      "Epoch 15: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1004 - accuracy: 0.9972 - val_loss: 1.9856 - val_accuracy: 0.7114 - lr: 0.0100\n",
      "Epoch 16/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9969\n",
      "Epoch 16: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1019 - accuracy: 0.9969 - val_loss: 1.9819 - val_accuracy: 0.7110 - lr: 0.0100\n",
      "Epoch 17/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9971\n",
      "Epoch 17: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.1008 - accuracy: 0.9971 - val_loss: 1.9898 - val_accuracy: 0.7128 - lr: 0.0100\n",
      "Epoch 18/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9969\n",
      "Epoch 18: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1014 - accuracy: 0.9969 - val_loss: 1.9907 - val_accuracy: 0.7122 - lr: 0.0100\n",
      "Epoch 19/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9971\n",
      "Epoch 19: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1003 - accuracy: 0.9972 - val_loss: 1.9969 - val_accuracy: 0.7110 - lr: 0.0100\n",
      "Epoch 20/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9970\n",
      "Epoch 20: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1010 - accuracy: 0.9970 - val_loss: 2.0027 - val_accuracy: 0.7108 - lr: 0.0100\n",
      "Epoch 21/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9966\n",
      "Epoch 21: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.1007 - accuracy: 0.9966 - val_loss: 2.0019 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 22/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9971\n",
      "Epoch 22: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1006 - accuracy: 0.9971 - val_loss: 2.0063 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 23/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9973\n",
      "Epoch 23: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0994 - accuracy: 0.9973 - val_loss: 2.0008 - val_accuracy: 0.7114 - lr: 0.0100\n",
      "Epoch 24/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9972\n",
      "Epoch 24: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1005 - accuracy: 0.9972 - val_loss: 2.0144 - val_accuracy: 0.7108 - lr: 0.0100\n",
      "Epoch 25/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0996 - accuracy: 0.9975\n",
      "Epoch 25: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0996 - accuracy: 0.9975 - val_loss: 2.0112 - val_accuracy: 0.7120 - lr: 0.0100\n",
      "Epoch 26/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9972\n",
      "Epoch 26: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1006 - accuracy: 0.9972 - val_loss: 2.0148 - val_accuracy: 0.7110 - lr: 0.0100\n",
      "Epoch 27/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9967\n",
      "Epoch 27: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.1006 - accuracy: 0.9967 - val_loss: 2.0156 - val_accuracy: 0.7090 - lr: 0.0100\n",
      "Epoch 28/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9971\n",
      "Epoch 28: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.1001 - accuracy: 0.9971 - val_loss: 2.0227 - val_accuracy: 0.7094 - lr: 0.0100\n",
      "Epoch 29/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9971\n",
      "Epoch 29: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1001 - accuracy: 0.9971 - val_loss: 2.0231 - val_accuracy: 0.7104 - lr: 0.0100\n",
      "Epoch 30/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9973\n",
      "Epoch 30: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.1000 - accuracy: 0.9973 - val_loss: 2.0288 - val_accuracy: 0.7108 - lr: 0.0100\n",
      "Epoch 31/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9971\n",
      "Epoch 31: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.1000 - accuracy: 0.9972 - val_loss: 2.0250 - val_accuracy: 0.7108 - lr: 0.0100\n",
      "Epoch 32/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9976\n",
      "Epoch 32: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0991 - accuracy: 0.9976 - val_loss: 2.0317 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 33/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9972\n",
      "Epoch 33: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 32s 46ms/step - loss: 0.0997 - accuracy: 0.9972 - val_loss: 2.0345 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 34/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9973\n",
      "Epoch 34: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0997 - accuracy: 0.9973 - val_loss: 2.0330 - val_accuracy: 0.7112 - lr: 0.0100\n",
      "Epoch 35/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9972\n",
      "Epoch 35: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0992 - accuracy: 0.9972 - val_loss: 2.0353 - val_accuracy: 0.7086 - lr: 0.0100\n",
      "Epoch 36/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 0.9970\n",
      "Epoch 36: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0999 - accuracy: 0.9970 - val_loss: 2.0323 - val_accuracy: 0.7104 - lr: 0.0100\n",
      "Epoch 37/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9973\n",
      "Epoch 37: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0998 - accuracy: 0.9973 - val_loss: 2.0380 - val_accuracy: 0.7102 - lr: 0.0100\n",
      "Epoch 38/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.9974\n",
      "Epoch 38: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0995 - accuracy: 0.9974 - val_loss: 2.0361 - val_accuracy: 0.7100 - lr: 0.0100\n",
      "Epoch 39/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0997 - accuracy: 0.9972\n",
      "Epoch 39: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0997 - accuracy: 0.9972 - val_loss: 2.0427 - val_accuracy: 0.7106 - lr: 0.0100\n",
      "Epoch 40/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9977\n",
      "Epoch 40: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 32s 45ms/step - loss: 0.0984 - accuracy: 0.9977 - val_loss: 2.0440 - val_accuracy: 0.7106 - lr: 0.0100\n",
      "Epoch 41/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9975\n",
      "Epoch 41: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0990 - accuracy: 0.9975 - val_loss: 2.0489 - val_accuracy: 0.7086 - lr: 0.0100\n",
      "Epoch 42/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9974\n",
      "Epoch 42: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0991 - accuracy: 0.9974 - val_loss: 2.0490 - val_accuracy: 0.7104 - lr: 0.0100\n",
      "Epoch 43/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9974\n",
      "Epoch 43: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.0993 - accuracy: 0.9974 - val_loss: 2.0575 - val_accuracy: 0.7108 - lr: 0.0100\n",
      "Epoch 44/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9974\n",
      "Epoch 44: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0990 - accuracy: 0.9974 - val_loss: 2.0519 - val_accuracy: 0.7096 - lr: 0.0100\n",
      "Epoch 45/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9975\n",
      "Epoch 45: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0987 - accuracy: 0.9975 - val_loss: 2.0565 - val_accuracy: 0.7122 - lr: 0.0100\n",
      "Epoch 46/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9973\n",
      "Epoch 46: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0988 - accuracy: 0.9973 - val_loss: 2.0568 - val_accuracy: 0.7104 - lr: 0.0010\n",
      "Epoch 47/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9974\n",
      "Epoch 47: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0987 - accuracy: 0.9974 - val_loss: 2.0482 - val_accuracy: 0.7114 - lr: 0.0010\n",
      "Epoch 48/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9970\n",
      "Epoch 48: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0990 - accuracy: 0.9970 - val_loss: 2.0512 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 49/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9970\n",
      "Epoch 49: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0991 - accuracy: 0.9970 - val_loss: 2.0525 - val_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 50/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9970\n",
      "Epoch 50: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0997 - accuracy: 0.9970 - val_loss: 2.0541 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 51/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9977\n",
      "Epoch 51: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0986 - accuracy: 0.9977 - val_loss: 2.0573 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 52/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0993 - accuracy: 0.9972\n",
      "Epoch 52: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0993 - accuracy: 0.9972 - val_loss: 2.0555 - val_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 53/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.9975\n",
      "Epoch 53: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.0986 - accuracy: 0.9975 - val_loss: 2.0616 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 54/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9977\n",
      "Epoch 54: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0987 - accuracy: 0.9978 - val_loss: 2.0593 - val_accuracy: 0.7104 - lr: 0.0010\n",
      "Epoch 55/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9975\n",
      "Epoch 55: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0988 - accuracy: 0.9975 - val_loss: 2.0592 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 56/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9977\n",
      "Epoch 56: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.0982 - accuracy: 0.9977 - val_loss: 2.0550 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 57/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9971\n",
      "Epoch 57: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0988 - accuracy: 0.9971 - val_loss: 2.0548 - val_accuracy: 0.7108 - lr: 0.0010\n",
      "Epoch 58/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9974\n",
      "Epoch 58: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0990 - accuracy: 0.9974 - val_loss: 2.0575 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 59/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9978\n",
      "Epoch 59: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0985 - accuracy: 0.9978 - val_loss: 2.0563 - val_accuracy: 0.7096 - lr: 0.0010\n",
      "Epoch 60/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9980\n",
      "Epoch 60: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0984 - accuracy: 0.9980 - val_loss: 2.0553 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 61/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9973\n",
      "Epoch 61: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0987 - accuracy: 0.9973 - val_loss: 2.0575 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 62/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9975\n",
      "Epoch 62: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0985 - accuracy: 0.9975 - val_loss: 2.0580 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 63/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9975\n",
      "Epoch 63: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0987 - accuracy: 0.9975 - val_loss: 2.0607 - val_accuracy: 0.7096 - lr: 0.0010\n",
      "Epoch 64/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9978\n",
      "Epoch 64: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0984 - accuracy: 0.9978 - val_loss: 2.0567 - val_accuracy: 0.7116 - lr: 0.0010\n",
      "Epoch 65/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9973\n",
      "Epoch 65: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0989 - accuracy: 0.9973 - val_loss: 2.0560 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 66/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9974\n",
      "Epoch 66: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0987 - accuracy: 0.9974 - val_loss: 2.0568 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 67/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0979 - accuracy: 0.9976\n",
      "Epoch 67: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0979 - accuracy: 0.9976 - val_loss: 2.0525 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 68/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9976\n",
      "Epoch 68: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0984 - accuracy: 0.9976 - val_loss: 2.0578 - val_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 69/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9973\n",
      "Epoch 69: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0989 - accuracy: 0.9973 - val_loss: 2.0604 - val_accuracy: 0.7090 - lr: 0.0010\n",
      "Epoch 70/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9973\n",
      "Epoch 70: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0985 - accuracy: 0.9973 - val_loss: 2.0561 - val_accuracy: 0.7094 - lr: 0.0010\n",
      "Epoch 71/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9975\n",
      "Epoch 71: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0984 - accuracy: 0.9975 - val_loss: 2.0569 - val_accuracy: 0.7096 - lr: 0.0010\n",
      "Epoch 72/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9975\n",
      "Epoch 72: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0982 - accuracy: 0.9975 - val_loss: 2.0600 - val_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 73/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9976\n",
      "Epoch 73: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0981 - accuracy: 0.9976 - val_loss: 2.0610 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 74/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9974\n",
      "Epoch 74: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0988 - accuracy: 0.9974 - val_loss: 2.0642 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 75/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9971\n",
      "Epoch 75: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0987 - accuracy: 0.9971 - val_loss: 2.0640 - val_accuracy: 0.7104 - lr: 0.0010\n",
      "Epoch 76/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9974\n",
      "Epoch 76: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.0990 - accuracy: 0.9974 - val_loss: 2.0554 - val_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 77/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9976\n",
      "Epoch 77: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0986 - accuracy: 0.9976 - val_loss: 2.0577 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 78/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9971\n",
      "Epoch 78: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0990 - accuracy: 0.9971 - val_loss: 2.0550 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 79/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0977 - accuracy: 0.9979\n",
      "Epoch 79: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0978 - accuracy: 0.9979 - val_loss: 2.0602 - val_accuracy: 0.7104 - lr: 0.0010\n",
      "Epoch 80/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9971\n",
      "Epoch 80: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0991 - accuracy: 0.9971 - val_loss: 2.0609 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 81/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9977\n",
      "Epoch 81: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0991 - accuracy: 0.9976 - val_loss: 2.0602 - val_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 82/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9977\n",
      "Epoch 82: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0987 - accuracy: 0.9977 - val_loss: 2.0594 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 83/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9976\n",
      "Epoch 83: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0982 - accuracy: 0.9976 - val_loss: 2.0584 - val_accuracy: 0.7104 - lr: 0.0010\n",
      "Epoch 84/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0993 - accuracy: 0.9970\n",
      "Epoch 84: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 31s 44ms/step - loss: 0.0993 - accuracy: 0.9970 - val_loss: 2.0566 - val_accuracy: 0.7108 - lr: 0.0010\n",
      "Epoch 85/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9977\n",
      "Epoch 85: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0985 - accuracy: 0.9977 - val_loss: 2.0607 - val_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 86/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9975\n",
      "Epoch 86: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0986 - accuracy: 0.9975 - val_loss: 2.0657 - val_accuracy: 0.7108 - lr: 0.0010\n",
      "Epoch 87/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9971\n",
      "Epoch 87: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0991 - accuracy: 0.9971 - val_loss: 2.0597 - val_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 88/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.9975\n",
      "Epoch 88: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0986 - accuracy: 0.9975 - val_loss: 2.0587 - val_accuracy: 0.7116 - lr: 0.0010\n",
      "Epoch 89/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9974\n",
      "Epoch 89: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.0986 - accuracy: 0.9974 - val_loss: 2.0658 - val_accuracy: 0.7116 - lr: 0.0010\n",
      "Epoch 90/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9974\n",
      "Epoch 90: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0987 - accuracy: 0.9974 - val_loss: 2.0594 - val_accuracy: 0.7110 - lr: 0.0010\n",
      "Epoch 91/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9978\n",
      "Epoch 91: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0985 - accuracy: 0.9978 - val_loss: 2.0622 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 92/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9977\n",
      "Epoch 92: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0986 - accuracy: 0.9977 - val_loss: 2.0621 - val_accuracy: 0.7096 - lr: 0.0010\n",
      "Epoch 93/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9973\n",
      "Epoch 93: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.0989 - accuracy: 0.9973 - val_loss: 2.0675 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 94/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9972\n",
      "Epoch 94: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0987 - accuracy: 0.9972 - val_loss: 2.0617 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 95/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9975\n",
      "Epoch 95: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0985 - accuracy: 0.9975 - val_loss: 2.0593 - val_accuracy: 0.7114 - lr: 0.0010\n",
      "Epoch 96/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9974\n",
      "Epoch 96: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0984 - accuracy: 0.9974 - val_loss: 2.0638 - val_accuracy: 0.7108 - lr: 0.0010\n",
      "Epoch 97/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9975\n",
      "Epoch 97: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 30s 43ms/step - loss: 0.0979 - accuracy: 0.9975 - val_loss: 2.0621 - val_accuracy: 0.7110 - lr: 0.0010\n",
      "Epoch 98/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9972\n",
      "Epoch 98: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0987 - accuracy: 0.9972 - val_loss: 2.0592 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 99/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9975\n",
      "Epoch 99: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0980 - accuracy: 0.9975 - val_loss: 2.0596 - val_accuracy: 0.7116 - lr: 0.0010\n",
      "Epoch 100/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9974\n",
      "Epoch 100: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0984 - accuracy: 0.9974 - val_loss: 2.0630 - val_accuracy: 0.7110 - lr: 0.0010\n",
      "Epoch 101/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9974\n",
      "Epoch 101: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 46ms/step - loss: 0.0990 - accuracy: 0.9974 - val_loss: 2.0613 - val_accuracy: 0.7116 - lr: 0.0010\n",
      "Epoch 102/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9972\n",
      "Epoch 102: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0988 - accuracy: 0.9972 - val_loss: 2.0626 - val_accuracy: 0.7104 - lr: 0.0010\n",
      "Epoch 103/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9972\n",
      "Epoch 103: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.0990 - accuracy: 0.9972 - val_loss: 2.0675 - val_accuracy: 0.7108 - lr: 0.0010\n",
      "Epoch 104/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9977\n",
      "Epoch 104: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0981 - accuracy: 0.9977 - val_loss: 2.0654 - val_accuracy: 0.7104 - lr: 0.0010\n",
      "Epoch 105/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9978\n",
      "Epoch 105: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.0984 - accuracy: 0.9978 - val_loss: 2.0660 - val_accuracy: 0.7118 - lr: 0.0010\n",
      "Epoch 106/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9974\n",
      "Epoch 106: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0985 - accuracy: 0.9974 - val_loss: 2.0614 - val_accuracy: 0.7114 - lr: 0.0010\n",
      "Epoch 107/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0993 - accuracy: 0.9970\n",
      "Epoch 107: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0993 - accuracy: 0.9970 - val_loss: 2.0673 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 108/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9978\n",
      "Epoch 108: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 38ms/step - loss: 0.0983 - accuracy: 0.9978 - val_loss: 2.0651 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 109/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9975\n",
      "Epoch 109: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 34s 48ms/step - loss: 0.0984 - accuracy: 0.9975 - val_loss: 2.0673 - val_accuracy: 0.7114 - lr: 0.0010\n",
      "Epoch 110/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9975\n",
      "Epoch 110: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0983 - accuracy: 0.9975 - val_loss: 2.0665 - val_accuracy: 0.7108 - lr: 0.0010\n",
      "Epoch 111/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9974\n",
      "Epoch 111: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0980 - accuracy: 0.9974 - val_loss: 2.0681 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 112/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9972\n",
      "Epoch 112: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 39ms/step - loss: 0.0986 - accuracy: 0.9972 - val_loss: 2.0701 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 113/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9972\n",
      "Epoch 113: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0985 - accuracy: 0.9972 - val_loss: 2.0656 - val_accuracy: 0.7114 - lr: 0.0010\n",
      "Epoch 114/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9975\n",
      "Epoch 114: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0983 - accuracy: 0.9975 - val_loss: 2.0681 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 115/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9974\n",
      "Epoch 115: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0984 - accuracy: 0.9974 - val_loss: 2.0700 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 116/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9974\n",
      "Epoch 116: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 28s 40ms/step - loss: 0.0986 - accuracy: 0.9974 - val_loss: 2.0672 - val_accuracy: 0.7094 - lr: 0.0010\n",
      "Epoch 117/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9975\n",
      "Epoch 117: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 33s 47ms/step - loss: 0.0983 - accuracy: 0.9976 - val_loss: 2.0721 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 118/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9976\n",
      "Epoch 118: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0980 - accuracy: 0.9976 - val_loss: 2.0696 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 119/120\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9978\n",
      "Epoch 119: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0979 - accuracy: 0.9978 - val_loss: 2.0700 - val_accuracy: 0.7094 - lr: 0.0010\n",
      "Epoch 120/120\n",
      "702/703 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9973\n",
      "Epoch 120: val_loss did not improve from 1.70474\n",
      "703/703 [==============================] - 27s 39ms/step - loss: 0.0980 - accuracy: 0.9973 - val_loss: 2.0677 - val_accuracy: 0.7108 - lr: 0.0010\n",
      "Current:  126\n",
      "313/313 [==============================] - 4s 10ms/step\n",
      "Accuracy: 69.98\n",
      "Error: 30.019999999999996\n",
      "ECE: 0.20820267888903615\n",
      "MCE: 0.4635832021871118\n",
      "Loss: 2.0276901432766756\n",
      "brier: 0.2740956561416483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30.019999999999996,\n",
       " 0.20820267888903615,\n",
       " 0.4635832021871118,\n",
       " 2.0276901432766756,\n",
       " 0.2740956561416483]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezing.training_with_freezing(model, img_gen, sgd, x_train45, y_train45, x_val, y_val, x_test, y_test,freezing_list,batch_size=batch_size,lr_schedule = [[0, 0.1],[nb_epoch*0.5,0.01],[nb_epoch*0.75,0.001]], cbks=[checkpointer],name='dense_cifar100')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15858.830182,
   "end_time": "2023-04-24T14:20:18.957948",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-24T09:56:00.127766",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
